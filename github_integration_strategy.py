#!/usr/bin/env python3
"""
üöÄ GitHub Integration Strategy for Sophia AI MCP Enhancement
===========================================================

This script analyzes the best ways to integrate top MCP repositories into the
Sophia AI GitHub organization using the provided PAT with broad privileges.

Author: Sophia AI Platform Team
Date: 2025-06-29
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 1225 lines

Recommended decomposition:
- github_integration_strategy_core.py - Core functionality
- github_integration_strategy_utils.py - Utility functions
- github_integration_strategy_models.py - Data models
- github_integration_strategy_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import json
import logging
import subprocess
import sys
from datetime import datetime
from pathlib import Path

import requests

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(
            f'github_integration_strategy_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        ),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)


class GitHubIntegrationStrategy:
    """Analyzes and implements GitHub integration strategy for MCP repositories."""

    def __init__(self, pat_token: str = None):
        """Initialize with GitHub PAT token."""
        # Extract PAT from git remote if not provided
        if not pat_token:
            self.pat_token = self.extract_pat_from_git_remote()
        else:
            self.pat_token = pat_token

        self.headers = {
            "Authorization": f"token {self.pat_token}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "Sophia-AI-Integration-Strategy",
        }

        self.base_url = "https://api.github.com"
        self.org_name = "ai-cherry"
        self.repo_name = "sophia-main"

        # Top MCP repositories to integrate
        self.target_repositories = [
            {
                "name": "anthropic-mcp-servers",
                "url": "https://github.com/modelcontextprotocol/servers",
                "priority": "critical",
                "integration_type": "submodule",
                "purpose": "Reference implementations and community servers",
            },
            {
                "name": "anthropic-mcp-python-sdk",
                "url": "https://github.com/modelcontextprotocol/python-sdk",
                "priority": "critical",
                "integration_type": "dependency",
                "purpose": "Official Python SDK for MCP protocol",
            },
            {
                "name": "anthropic-mcp-inspector",
                "url": "https://github.com/modelcontextprotocol/inspector",
                "priority": "critical",
                "integration_type": "submodule",
                "purpose": "Visual testing and debugging tool",
            },
            {
                "name": "notion-mcp-server",
                "url": "https://github.com/makenotion/notion-mcp-server",
                "priority": "high",
                "integration_type": "fork",
                "purpose": "Official Notion MCP server implementation",
            },
            {
                "name": "slack-mcp-server",
                "url": "https://github.com/korotovsky/slack-mcp-server",
                "priority": "high",
                "integration_type": "fork",
                "purpose": "Advanced Slack integration with SSE support",
            },
            {
                "name": "brightdata-mcp-server",
                "url": "https://github.com/brightdata/mcp-server",
                "priority": "medium",
                "integration_type": "fork",
                "purpose": "Web scraping and data collection capabilities",
            },
        ]

        self.integration_results = {
            "analysis_timestamp": datetime.now().isoformat(),
            "repositories_analyzed": [],
            "integration_recommendations": [],
            "github_configurations": [],
            "security_considerations": [],
            "implementation_plan": [],
        }

    def extract_pat_from_git_remote(self) -> str:
        """Extract PAT token from git remote URL."""
        try:
            result = subprocess.run(
                ["git", "remote", "get-url", "origin"],
                capture_output=True,
                text=True,
                cwd="/home/ubuntu/sophia-main",
            )

            if result.returncode == 0:
                remote_url = result.stdout.strip()
                # Extract PAT from URL format: https://github_pat_TOKEN@github.com/...
                if "github_pat_" in remote_url:
                    start = remote_url.find("github_pat_")
                    end = remote_url.find("@github.com")
                    if start != -1 and end != -1:
                        return remote_url[start:end]

            logger.error("Could not extract PAT from git remote")
            return None

        except Exception as e:
            logger.error(f"Error extracting PAT: {e}")
            return None

    def analyze_github_organization(self) -> dict:
        """Analyze current GitHub organization structure and permissions."""
        logger.info(f"üîç Analyzing GitHub organization: {self.org_name}")

        try:
            # Get organization info
            org_response = requests.get(
                f"{self.base_url}/orgs/{self.org_name}", headers=self.headers
            )

            if org_response.status_code == 200:
                org_data = org_response.json()
                logger.info(f"‚úÖ Organization access confirmed: {org_data['name']}")

                # Get repositories
                repos_response = requests.get(
                    f"{self.base_url}/orgs/{self.org_name}/repos",
                    headers=self.headers,
                    params={"per_page": 100},
                )

                repos_data = (
                    repos_response.json() if repos_response.status_code == 200 else []
                )

                # Get organization permissions
                permissions = self.analyze_organization_permissions()

                analysis = {
                    "organization": {
                        "name": org_data["name"],
                        "description": org_data.get("description", ""),
                        "public_repos": org_data["public_repos"],
                        "private_repos": org_data.get("total_private_repos", 0),
                        "plan": org_data.get("plan", {}).get("name", "unknown"),
                    },
                    "repositories": [
                        {
                            "name": repo["name"],
                            "private": repo["private"],
                            "size": repo["size"],
                            "language": repo["language"],
                            "updated_at": repo["updated_at"],
                        }
                        for repo in repos_data
                    ],
                    "permissions": permissions,
                    "integration_capabilities": self.assess_integration_capabilities(
                        org_data, permissions
                    ),
                }

                self.integration_results["repositories_analyzed"] = analysis
                return analysis

            else:
                logger.error(
                    f"‚ùå Failed to access organization: {org_response.status_code}"
                )
                return {}

        except Exception as e:
            logger.error(f"‚ùå Error analyzing organization: {e}")
            return {}

    def analyze_organization_permissions(self) -> dict:
        """Analyze PAT permissions and organization access levels."""
        logger.info("üîê Analyzing PAT permissions and access levels")

        permissions = {
            "repo_access": False,
            "admin_access": False,
            "actions_access": False,
            "packages_access": False,
            "security_access": False,
            "webhooks_access": False,
        }

        try:
            # Test repository access
            repo_response = requests.get(
                f"{self.base_url}/repos/{self.org_name}/{self.repo_name}",
                headers=self.headers,
            )
            permissions["repo_access"] = repo_response.status_code == 200

            # Test admin access (try to get organization settings)
            admin_response = requests.get(
                f"{self.base_url}/orgs/{self.org_name}/actions/permissions",
                headers=self.headers,
            )
            permissions["admin_access"] = admin_response.status_code == 200

            # Test actions access
            actions_response = requests.get(
                f"{self.base_url}/repos/{self.org_name}/{self.repo_name}/actions/workflows",
                headers=self.headers,
            )
            permissions["actions_access"] = actions_response.status_code == 200

            # Test packages access
            packages_response = requests.get(
                f"{self.base_url}/orgs/{self.org_name}/packages", headers=self.headers
            )
            permissions["packages_access"] = packages_response.status_code == 200

            # Test security access
            security_response = requests.get(
                f"{self.base_url}/repos/{self.org_name}/{self.repo_name}/vulnerability-alerts",
                headers=self.headers,
            )
            permissions["security_access"] = security_response.status_code == 200

            # Test webhooks access
            webhooks_response = requests.get(
                f"{self.base_url}/repos/{self.org_name}/{self.repo_name}/hooks",
                headers=self.headers,
            )
            permissions["webhooks_access"] = webhooks_response.status_code == 200

            logger.info(
                f"‚úÖ Permissions analysis complete: {sum(permissions.values())}/6 capabilities available"
            )
            return permissions

        except Exception as e:
            logger.error(f"‚ùå Error analyzing permissions: {e}")
            return permissions

    def assess_integration_capabilities(
        self, org_data: dict, permissions: dict
    ) -> dict:
        """Assess what integration strategies are possible with current permissions."""
        capabilities = {
            "can_create_repos": permissions["admin_access"],
            "can_fork_repos": permissions["repo_access"],
            "can_add_submodules": permissions["repo_access"],
            "can_setup_actions": permissions["actions_access"],
            "can_manage_packages": permissions["packages_access"],
            "can_setup_security": permissions["security_access"],
            "can_setup_webhooks": permissions["webhooks_access"],
            "recommended_strategies": [],
        }

        # Determine recommended integration strategies
        if capabilities["can_create_repos"]:
            capabilities["recommended_strategies"].append("create_dedicated_mcp_repos")

        if capabilities["can_fork_repos"]:
            capabilities["recommended_strategies"].append("fork_external_repos")

        if capabilities["can_add_submodules"]:
            capabilities["recommended_strategies"].append("add_as_submodules")

        if capabilities["can_setup_actions"]:
            capabilities["recommended_strategies"].append("automated_sync_workflows")

        return capabilities

    def analyze_target_repositories(self) -> list[dict]:
        """Analyze each target repository for integration feasibility."""
        logger.info("üìä Analyzing target repositories for integration")

        analyzed_repos = []

        for repo_config in self.target_repositories:
            logger.info(f"üîç Analyzing: {repo_config['name']}")

            try:
                # Extract owner and repo name from URL
                url_parts = (
                    repo_config["url"].replace("https://github.com/", "").split("/")
                )
                owner, repo_name = url_parts[0], url_parts[1]

                # Get repository information
                repo_response = requests.get(
                    f"{self.base_url}/repos/{owner}/{repo_name}", headers=self.headers
                )

                if repo_response.status_code == 200:
                    repo_data = repo_response.json()

                    analysis = {
                        "config": repo_config,
                        "github_data": {
                            "full_name": repo_data["full_name"],
                            "description": repo_data["description"],
                            "language": repo_data["language"],
                            "size": repo_data["size"],
                            "stars": repo_data["stargazers_count"],
                            "forks": repo_data["forks_count"],
                            "license": repo_data.get("license", {}).get(
                                "name", "Unknown"
                            ),
                            "updated_at": repo_data["updated_at"],
                            "default_branch": repo_data["default_branch"],
                            "topics": repo_data.get("topics", []),
                        },
                        "integration_assessment": self.assess_repository_integration(
                            repo_data, repo_config
                        ),
                        "security_analysis": self.analyze_repository_security(
                            repo_data
                        ),
                        "recommended_approach": self.recommend_integration_approach(
                            repo_data, repo_config
                        ),
                    }

                    analyzed_repos.append(analysis)
                    logger.info(f"‚úÖ Analysis complete for {repo_config['name']}")

                else:
                    logger.warning(
                        f"‚ö†Ô∏è Could not access repository: {repo_config['name']}"
                    )
                    analyzed_repos.append(
                        {
                            "config": repo_config,
                            "error": f"HTTP {repo_response.status_code}",
                            "recommended_approach": "manual_download",
                        }
                    )

            except Exception as e:
                logger.error(f"‚ùå Error analyzing {repo_config['name']}: {e}")
                analyzed_repos.append(
                    {
                        "config": repo_config,
                        "error": str(e),
                        "recommended_approach": "manual_download",
                    }
                )

        return analyzed_repos

    def assess_repository_integration(self, repo_data: dict, config: dict) -> dict:
        """Assess how well a repository can be integrated."""
        assessment = {
            "compatibility_score": 0,
            "integration_complexity": "unknown",
            "benefits": [],
            "challenges": [],
            "requirements": [],
        }

        # Assess compatibility based on language
        if repo_data["language"] == "Python":
            assessment["compatibility_score"] += 30
            assessment["benefits"].append("Native Python compatibility")
        elif repo_data["language"] == "TypeScript":
            assessment["compatibility_score"] += 20
            assessment["benefits"].append("Can be used as reference or via Node.js")
            assessment["challenges"].append("Requires Node.js runtime")

        # Assess based on size
        if repo_data["size"] < 10000:  # Less than 10MB
            assessment["compatibility_score"] += 20
            assessment["benefits"].append("Lightweight integration")
        elif repo_data["size"] > 100000:  # More than 100MB
            assessment["challenges"].append("Large repository size")

        # Assess based on activity
        from datetime import datetime, timedelta

        updated_date = datetime.fromisoformat(
            repo_data["updated_at"].replace("Z", "+00:00")
        )
        if updated_date > datetime.now().replace(
            tzinfo=updated_date.tzinfo
        ) - timedelta(days=30):
            assessment["compatibility_score"] += 25
            assessment["benefits"].append("Recently updated (active development)")

        # Assess based on popularity
        if repo_data["stargazers_count"] > 1000:
            assessment["compatibility_score"] += 15
            assessment["benefits"].append("High community adoption")

        # Assess based on license
        license_name = repo_data.get("license", {}).get("name", "Unknown")
        if license_name in [
            "MIT License",
            "Apache License 2.0",
            'BSD 3-Clause "New" or "Revised" License',
        ]:
            assessment["compatibility_score"] += 10
            assessment["benefits"].append("Permissive license")
        elif license_name == "Unknown":
            assessment["challenges"].append("Unknown license terms")

        # Determine integration complexity
        if assessment["compatibility_score"] >= 80:
            assessment["integration_complexity"] = "low"
        elif assessment["compatibility_score"] >= 60:
            assessment["integration_complexity"] = "medium"
        else:
            assessment["integration_complexity"] = "high"

        return assessment

    def analyze_repository_security(self, repo_data: dict) -> dict:
        """Analyze security aspects of the repository."""
        security = {
            "security_score": 0,
            "security_features": [],
            "security_concerns": [],
            "recommendations": [],
        }

        # Check for security features
        if repo_data.get("has_issues"):
            security["security_features"].append("Issue tracking enabled")
            security["security_score"] += 10

        if repo_data.get("has_wiki"):
            security["security_features"].append("Documentation available")
            security["security_score"] += 5

        # Check for potential concerns
        if not repo_data.get("license"):
            security["security_concerns"].append("No license specified")
            security["recommendations"].append(
                "Verify license terms before integration"
            )

        if repo_data["size"] > 50000:  # Large repositories
            security["security_concerns"].append(
                "Large codebase requires thorough review"
            )
            security["recommendations"].append(
                "Conduct security audit before integration"
            )

        return security

    def recommend_integration_approach(self, repo_data: dict, config: dict) -> dict:
        """Recommend the best integration approach for each repository."""
        approach = {
            "primary_method": config["integration_type"],
            "alternative_methods": [],
            "implementation_steps": [],
            "considerations": [],
        }

        # Refine recommendation based on analysis
        if config["integration_type"] == "submodule":
            approach["implementation_steps"] = [
                f"git submodule add {config['url']} external/{config['name']}",
                "git submodule update --init --recursive",
                "Add submodule to .gitmodules configuration",
                "Update documentation with submodule usage",
            ]
            approach["considerations"] = [
                "Submodules require careful version management",
                "Team members need to understand submodule workflows",
                "Consider pinning to specific commits for stability",
            ]

        elif config["integration_type"] == "fork":
            approach["implementation_steps"] = [
                f"Fork repository to {self.org_name} organization",
                "Clone forked repository locally",
                "Add upstream remote for updates",
                "Customize for Sophia AI integration",
                "Set up automated sync workflows",
            ]
            approach["considerations"] = [
                "Maintain sync with upstream repository",
                "Document customizations clearly",
                "Consider contributing improvements back upstream",
            ]

        elif config["integration_type"] == "dependency":
            approach["implementation_steps"] = [
                "Add to pyproject.toml dependencies",
                "Update uv.lock file",
                "Import and integrate in codebase",
                "Add to requirements documentation",
            ]
            approach["considerations"] = [
                "Monitor for security updates",
                "Pin to specific versions for stability",
                "Test compatibility with existing dependencies",
            ]

        # Add alternative methods
        if config["integration_type"] != "submodule":
            approach["alternative_methods"].append("submodule")
        if config["integration_type"] != "fork":
            approach["alternative_methods"].append("fork")
        if config["integration_type"] != "dependency":
            approach["alternative_methods"].append("dependency")

        return approach

    def create_github_configurations(self, analyzed_repos: list[dict]) -> list[dict]:
        """Create GitHub-specific configurations for optimal integration."""
        logger.info("‚öôÔ∏è Creating GitHub configurations for integration")

        configurations = []

        # 1. Repository Structure Configuration
        repo_structure = {
            "type": "repository_structure",
            "name": "MCP Integration Structure",
            "description": "Organize MCP repositories within Sophia AI",
            "structure": {
                "external/": {
                    "description": "External MCP repositories as submodules",
                    "submodules": [
                        repo["config"]["name"]
                        for repo in analyzed_repos
                        if isinstance(repo.get("recommended_approach"), dict)
                        and repo.get("recommended_approach", {}).get("primary_method")
                        == "submodule"
                    ],
                },
                "mcp-integrations/": {
                    "description": "Custom integrations based on external repos",
                    "purpose": "Sophia-specific adaptations of external MCP servers",
                },
                "mcp-forks/": {
                    "description": "Links to forked repositories in organization",
                    "repos": [
                        repo["config"]["name"]
                        for repo in analyzed_repos
                        if isinstance(repo.get("recommended_approach"), dict)
                        and repo.get("recommended_approach", {}).get("primary_method")
                        == "fork"
                    ],
                },
            },
        }
        configurations.append(repo_structure)

        # 2. GitHub Actions Workflows
        workflows_config = {
            "type": "github_actions",
            "name": "MCP Integration Workflows",
            "description": "Automated workflows for MCP repository management",
            "workflows": [
                {
                    "name": "sync-mcp-submodules.yml",
                    "purpose": "Automatically sync submodules with upstream",
                    "trigger": "schedule (weekly)",
                    "actions": [
                        "Check for upstream updates",
                        "Update submodules if changes detected",
                        "Run integration tests",
                        "Create PR if tests pass",
                    ],
                },
                {
                    "name": "test-mcp-integrations.yml",
                    "purpose": "Test all MCP server integrations",
                    "trigger": "push, pull_request",
                    "actions": [
                        "Setup Python environment",
                        "Install dependencies",
                        "Run MCP server tests",
                        "Validate protocol compliance",
                    ],
                },
                {
                    "name": "security-audit-mcp.yml",
                    "purpose": "Security audit of MCP dependencies",
                    "trigger": "schedule (daily)",
                    "actions": [
                        "Scan dependencies for vulnerabilities",
                        "Check for license compliance",
                        "Generate security report",
                        "Create issues for findings",
                    ],
                },
            ],
        }
        configurations.append(workflows_config)

        # 3. Branch Protection Rules
        branch_protection = {
            "type": "branch_protection",
            "name": "MCP Integration Branch Protection",
            "description": "Protect main branch from unsafe MCP integrations",
            "rules": {
                "main": {
                    "required_status_checks": [
                        "test-mcp-integrations",
                        "security-audit-mcp",
                    ],
                    "enforce_admins": False,
                    "required_pull_request_reviews": {
                        "required_approving_review_count": 1,
                        "dismiss_stale_reviews": True,
                        "require_code_owner_reviews": True,
                    },
                    "restrictions": None,
                }
            },
        }
        configurations.append(branch_protection)

        # 4. Repository Secrets Configuration
        secrets_config = {
            "type": "repository_secrets",
            "name": "MCP Integration Secrets",
            "description": "Required secrets for MCP server integrations",
            "secrets": [
                {
                    "name": "NOTION_API_KEY",
                    "description": "API key for Notion MCP server",
                    "required_for": ["notion-mcp-server"],
                },
                {
                    "name": "SLACK_BOT_TOKEN",
                    "description": "Bot token for Slack MCP server",
                    "required_for": ["slack-mcp-server"],
                },
                {
                    "name": "BRIGHTDATA_API_KEY",
                    "description": "API key for BrightData web scraping",
                    "required_for": ["brightdata-mcp-server"],
                },
                {
                    "name": "PINECONE_API_KEY",
                    "description": "API key for Pinecone vector database",
                    "required_for": ["vector-memory-server"],
                },
            ],
        }
        configurations.append(secrets_config)

        # 5. Issue Templates
        issue_templates = {
            "type": "issue_templates",
            "name": "MCP Integration Issue Templates",
            "description": "Standardized templates for MCP-related issues",
            "templates": [
                {
                    "name": "mcp-server-bug.md",
                    "title": "MCP Server Bug Report",
                    "labels": ["bug", "mcp-server"],
                    "assignees": ["mcp-team"],
                    "body": """
## MCP Server Bug Report

**Server Name:**
**Version:**
**Environment:**

### Description
A clear description of the bug.

### Steps to Reproduce
1.
2.
3.

### Expected Behavior
What should happen.

### Actual Behavior
What actually happens.

### Logs
```
Paste relevant logs here
```

### Additional Context
Any other context about the problem.
""",
                },
                {
                    "name": "mcp-integration-request.md",
                    "title": "New MCP Integration Request",
                    "labels": ["enhancement", "mcp-integration"],
                    "assignees": ["mcp-team"],
                    "body": """
## MCP Integration Request

**Service/Tool:**
**Repository URL:**
**Priority:** High/Medium/Low

### Business Justification
Why this integration is needed.

### Technical Requirements
- [ ] API access available
- [ ] Documentation reviewed
- [ ] Security assessment completed
- [ ] Integration approach defined

### Success Criteria
What defines a successful integration.

### Additional Notes
Any other relevant information.
""",
                },
            ],
        }
        configurations.append(issue_templates)

        return configurations

    def create_implementation_plan(
        self, analyzed_repos: list[dict], configurations: list[dict]
    ) -> dict:
        """Create detailed implementation plan for GitHub integration."""
        logger.info("üìã Creating comprehensive implementation plan")

        plan = {
            "overview": {
                "total_repositories": len(analyzed_repos),
                "integration_methods": {
                    "submodules": len(
                        [
                            r
                            for r in analyzed_repos
                            if isinstance(r.get("recommended_approach"), dict)
                            and r.get("recommended_approach", {}).get("primary_method")
                            == "submodule"
                        ]
                    ),
                    "forks": len(
                        [
                            r
                            for r in analyzed_repos
                            if isinstance(r.get("recommended_approach"), dict)
                            and r.get("recommended_approach", {}).get("primary_method")
                            == "fork"
                        ]
                    ),
                    "dependencies": len(
                        [
                            r
                            for r in analyzed_repos
                            if isinstance(r.get("recommended_approach"), dict)
                            and r.get("recommended_approach", {}).get("primary_method")
                            == "dependency"
                        ]
                    ),
                },
                "estimated_timeline": "4 weeks",
                "risk_level": "medium",
            },
            "phases": [
                {
                    "phase": 1,
                    "name": "Foundation Setup",
                    "duration": "1 week",
                    "tasks": [
                        "Setup repository structure",
                        "Configure GitHub Actions workflows",
                        "Implement branch protection rules",
                        "Add critical dependencies (MCP SDK)",
                    ],
                    "deliverables": [
                        "Repository structure established",
                        "CI/CD pipelines operational",
                        "Security measures in place",
                    ],
                },
                {
                    "phase": 2,
                    "name": "Core Integrations",
                    "duration": "2 weeks",
                    "tasks": [
                        "Add Anthropic MCP servers as submodules",
                        "Fork and customize Notion MCP server",
                        "Fork and enhance Slack MCP server",
                        "Integrate MCP Inspector for testing",
                    ],
                    "deliverables": [
                        "Core MCP framework operational",
                        "Enterprise integrations functional",
                        "Testing framework established",
                    ],
                },
                {
                    "phase": 3,
                    "name": "Advanced Capabilities",
                    "duration": "1 week",
                    "tasks": [
                        "Integrate BrightData web capabilities",
                        "Add vector database support",
                        "Implement security monitoring",
                        "Optimize performance",
                    ],
                    "deliverables": [
                        "Advanced features operational",
                        "Security monitoring active",
                        "Performance optimized",
                    ],
                },
            ],
            "risk_mitigation": [
                {
                    "risk": "Dependency conflicts",
                    "probability": "medium",
                    "impact": "high",
                    "mitigation": "Thorough testing in isolated environments",
                },
                {
                    "risk": "License compatibility issues",
                    "probability": "low",
                    "impact": "high",
                    "mitigation": "Legal review of all licenses before integration",
                },
                {
                    "risk": "Security vulnerabilities",
                    "probability": "medium",
                    "impact": "high",
                    "mitigation": "Automated security scanning and regular audits",
                },
            ],
            "success_metrics": [
                "All target repositories successfully integrated",
                "99.9% production readiness achieved",
                "Zero security vulnerabilities introduced",
                "CI/CD pipeline success rate > 95%",
            ],
        }

        return plan

    def generate_implementation_scripts(self) -> dict[str, str]:
        """Generate scripts for implementing the GitHub integration."""
        logger.info("üîß Generating implementation scripts")

        scripts = {}

        # 1. Repository setup script
        scripts[
            "setup_repository_structure.sh"
        ] = """#!/bin/bash
# Setup MCP Integration Repository Structure

echo "üöÄ Setting up Sophia AI MCP integration structure..."

# Create directory structure
mkdir -p external
mkdir -p mcp-integrations
mkdir -p .github/workflows
mkdir -p .github/ISSUE_TEMPLATE

# Create documentation
cat > external/README.md << 'EOF'
# External MCP Repositories

This directory contains external MCP repositories integrated as submodules.

## Submodules

- `anthropic-mcp-servers/` - Official Anthropic MCP server implementations
- `anthropic-mcp-inspector/` - Visual testing and debugging tool

## Usage

To update all submodules:
```bash
git submodule update --remote --recursive
```

To initialize submodules after cloning:
```bash
git submodule update --init --recursive
```
EOF

echo "‚úÖ Repository structure created"
"""

        # 2. Submodule integration script
        scripts[
            "integrate_submodules.sh"
        ] = """#!/bin/bash
# Integrate MCP repositories as submodules

echo "üîó Integrating MCP repositories as submodules..."

# Add Anthropic MCP servers
git submodule add https://github.com/modelcontextprotocol/servers.git external/anthropic-mcp-servers

# Add MCP Inspector
git submodule add https://github.com/modelcontextprotocol/inspector.git external/anthropic-mcp-inspector

# Initialize and update submodules
git submodule update --init --recursive

# Commit submodule additions
git add .gitmodules external/
git commit -m "Add MCP repositories as submodules

- Added Anthropic MCP servers for reference implementations
- Added MCP Inspector for testing and debugging
- Configured submodules for automatic updates"

echo "‚úÖ Submodules integrated successfully"
"""

        # 3. Fork management script
        scripts[
            "setup_forks.py"
        ] = '''#!/usr/bin/env python3
"""Setup and manage forked MCP repositories."""

import requests
import json
import os

def fork_repository(owner, repo, org):
    """Fork a repository to the organization."""
    headers = {
        'Authorization': f'token {os.environ.get("GITHUB_TOKEN")}',
        'Accept': 'application/vnd.github.v3+json'
    }

    fork_data = {
        'organization': org
    }

    response = requests.post(
        f'https://api.github.com/repos/{owner}/{repo}/forks',
        headers=headers,
        json=fork_data
    )

    if response.status_code == 202:
        print(f"‚úÖ Successfully forked {owner}/{repo} to {org}")
        return response.json()
    else:
        print(f"‚ùå Failed to fork {owner}/{repo}: {response.status_code}")
        return None

def main():
    """Main fork setup function."""
    forks_to_create = [
        ('makenotion', 'notion-mcp-server'),
        ('korotovsky', 'slack-mcp-server'),
        ('brightdata', 'mcp-server')
    ]

    org = 'ai-cherry'

    for owner, repo in forks_to_create:
        fork_repository(owner, repo, org)

if __name__ == '__main__':
    main()
'''

        # 4. GitHub Actions workflow
        scripts[
            "test-mcp-integrations.yml"
        ] = """name: Test MCP Integrations

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test-mcp-servers:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]

    steps:
    - uses: actions/checkout@v4
      with:
        submodules: recursive

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH

    - name: Install dependencies
      run: |
        uv sync

    - name: Test MCP servers
      run: |
        uv run python scripts/test_mcp_servers.py

    - name: Validate MCP protocol compliance
      run: |
        uv run python scripts/validate_mcp_compliance.py

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: test-results/
"""

        return scripts

    def execute_analysis(self) -> dict:
        """Execute the complete GitHub integration analysis."""
        logger.info("üöÄ Starting comprehensive GitHub integration analysis")

        try:
            # 1. Analyze GitHub organization
            org_analysis = self.analyze_github_organization()

            # 2. Analyze target repositories
            repo_analysis = self.analyze_target_repositories()

            # 3. Create GitHub configurations
            github_configs = self.create_github_configurations(repo_analysis)

            # 4. Create implementation plan
            implementation_plan = self.create_implementation_plan(
                repo_analysis, github_configs
            )

            # 5. Generate implementation scripts
            implementation_scripts = self.generate_implementation_scripts()

            # Compile results
            self.integration_results.update(
                {
                    "organization_analysis": org_analysis,
                    "repository_analysis": repo_analysis,
                    "github_configurations": github_configs,
                    "implementation_plan": implementation_plan,
                    "implementation_scripts": implementation_scripts,
                    "recommendations": self.generate_final_recommendations(
                        repo_analysis, github_configs
                    ),
                }
            )

            # Save results
            self.save_results()

            logger.info("üéâ GitHub integration analysis completed successfully")
            return self.integration_results

        except Exception as e:
            logger.error(f"‚ùå Analysis failed: {e}")
            self.integration_results["error"] = str(e)
            return self.integration_results

    def generate_final_recommendations(
        self, repo_analysis: list[dict], github_configs: list[dict]
    ) -> list[dict]:
        """Generate final recommendations for GitHub integration."""
        recommendations = []

        # High-priority recommendations
        recommendations.append(
            {
                "priority": "critical",
                "category": "security",
                "title": "Implement comprehensive security scanning",
                "description": "Set up automated security scanning for all integrated repositories",
                "action_items": [
                    "Configure Dependabot for dependency updates",
                    "Enable CodeQL analysis for security vulnerabilities",
                    "Set up license compliance checking",
                    "Implement secrets scanning",
                ],
            }
        )

        recommendations.append(
            {
                "priority": "critical",
                "category": "integration",
                "title": "Establish MCP protocol compliance testing",
                "description": "Ensure all integrated servers comply with MCP protocol standards",
                "action_items": [
                    "Integrate MCP Inspector into CI/CD pipeline",
                    "Create automated protocol compliance tests",
                    "Set up regression testing for protocol changes",
                    "Document protocol compliance requirements",
                ],
            }
        )

        recommendations.append(
            {
                "priority": "high",
                "category": "maintenance",
                "title": "Automate repository synchronization",
                "description": "Keep integrated repositories synchronized with upstream changes",
                "action_items": [
                    "Set up automated submodule updates",
                    "Configure upstream sync for forked repositories",
                    "Implement change notification system",
                    "Create rollback procedures for failed updates",
                ],
            }
        )

        return recommendations

    def save_results(self):
        """Save analysis results to files."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Save comprehensive results
        results_file = f"github_integration_analysis_{timestamp}.json"
        with open(results_file, "w") as f:
            json.dump(self.integration_results, f, indent=2, default=str)

        logger.info(f"üìÑ Results saved to: {results_file}")

        # Save implementation scripts
        scripts_dir = Path("implementation_scripts")
        scripts_dir.mkdir(exist_ok=True)

        for script_name, script_content in self.integration_results.get(
            "implementation_scripts", {}
        ).items():
            script_path = scripts_dir / script_name
            script_path.write_text(script_content)
            if script_name.endswith(".sh"):
                script_path.chmod(0o755)

        logger.info(f"üîß Implementation scripts saved to: {scripts_dir}")


def main():
    """Main execution function."""
    print("üöÄ GitHub Integration Strategy Analysis")
    print("=" * 60)

    try:
        # Initialize strategy analyzer
        strategy = GitHubIntegrationStrategy()

        # Execute comprehensive analysis
        results = strategy.execute_analysis()

        # Print summary
        print("\n" + "=" * 60)
        print("üìä ANALYSIS SUMMARY")
        print("=" * 60)

        if "error" not in results:
            org_analysis = results.get("organization_analysis", {})
            repo_analysis = results.get("repository_analysis", [])

            print(
                f"‚úÖ Organization: {org_analysis.get('organization', {}).get('name', 'Unknown')}"
            )
            print(f"üì¶ Repositories analyzed: {len(repo_analysis)}")
            print(
                f"üîß GitHub configurations: {len(results.get('github_configurations', []))}"
            )
            print(
                f"üìã Implementation phases: {len(results.get('implementation_plan', {}).get('phases', []))}"
            )
            print(f"üéØ Recommendations: {len(results.get('recommendations', []))}")

            print("\nüéâ Analysis completed successfully!")
            print(
                "üìÑ Check the generated files for detailed results and implementation scripts."
            )
        else:
            print(f"‚ùå Analysis failed: {results['error']}")

        print("=" * 60)

    except Exception as e:
        print(f"‚ùå Execution failed: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
