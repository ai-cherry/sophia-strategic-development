#!/usr/bin/env python3
"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 638 lines

Recommended decomposition:
- deploy_snowflake_stability_standalone_core.py - Core functionality
- deploy_snowflake_stability_standalone_utils.py - Utility functions
- deploy_snowflake_stability_standalone_models.py - Data models
- deploy_snowflake_stability_standalone_handlers.py - Request handlers

TODO: Implement file decomposition
"""

from backend.core.auto_esc_config import get_config_value

"""
Standalone Snowflake Stability Enhancement Deployment Script
Implements comprehensive database-level stability features for Sophia AI production deployment.
No complex import dependencies - uses direct Snowflake connector.
"""

import json
import logging
import sys
from datetime import datetime

import snowflake.connector

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("snowflake_stability_deployment.log"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)

# Snowflake Configuration
SNOWFLAKE_CONFIG = {
    "account": "ZNB04675",
    "user": "SCOOBYJAVA15",
    "password": get_config_value("snowflake_password"),
    "role": "ACCOUNTADMIN",
    "database": "SOPHIA_AI_PROD",
    "warehouse": "SOPHIA_AI_WH",
}


class SnowflakeStabilityDeployer:
    def __init__(self):
        self.conn = None
        self.deployment_status = {
            "resource_monitors": {"status": "pending", "details": []},
            "warehouses": {"status": "pending", "details": []},
            "security_roles": {"status": "pending", "details": []},
            "performance_optimization": {"status": "pending", "details": []},
            "backup_recovery": {"status": "pending", "details": []},
            "monitoring_schemas": {"status": "pending", "details": []},
        }

    def connect(self) -> bool:
        """Initialize Snowflake connection with error handling."""
        try:
            self.conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)
            logger.info("‚úÖ Snowflake connection established successfully")

            # Test connection
            cursor = self.conn.cursor()
            cursor.execute(
                "SELECT CURRENT_TIMESTAMP(), CURRENT_USER(), CURRENT_WAREHOUSE()"
            )
            result = cursor.fetchone()
            logger.info(
                f"Connected as {result[1]} using warehouse {result[2]} at {result[0]}"
            )
            cursor.close()
            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Snowflake connection: {e}")
            return False

    def execute_query(self, query: str, description: str = "") -> bool:
        """Execute a single query with error handling."""
        try:
            cursor = self.conn.cursor()
            cursor.execute(query)
            cursor.close()
            if description:
                logger.info(f"‚úÖ {description}")
            return True
        except Exception as e:
            logger.error(
                f"‚ùå Failed to execute query{' - ' + description if description else ''}: {e}"
            )
            logger.error(f"Query: {query}")
            return False

    def deploy_resource_monitors(self) -> bool:
        """Deploy resource monitors for cost control and performance management."""
        logger.info("üîß Deploying resource monitors...")

        resource_monitor_queries = [
            (
                """
            CREATE OR REPLACE RESOURCE MONITOR SOPHIA_AI_PROD_MONITOR
            WITH CREDIT_QUOTA = 1000
            FREQUENCY = MONTHLY
            TRIGGERS
                ON 75 PERCENT DO NOTIFY
                ON 90 PERCENT DO SUSPEND_IMMEDIATE
                ON 95 PERCENT DO SUSPEND_IMMEDIATE
            """,
                "Production resource monitor (1000 credits/month)",
            ),
            (
                """
            CREATE OR REPLACE RESOURCE MONITOR SOPHIA_AI_DEV_MONITOR
            WITH CREDIT_QUOTA = 200
            FREQUENCY = MONTHLY
            TRIGGERS
                ON 80 PERCENT DO NOTIFY
                ON 95 PERCENT DO SUSPEND_IMMEDIATE
            """,
                "Development resource monitor (200 credits/month)",
            ),
            (
                """
            CREATE OR REPLACE RESOURCE MONITOR SOPHIA_AI_ANALYTICS_MONITOR
            WITH CREDIT_QUOTA = 500
            FREQUENCY = MONTHLY
            TRIGGERS
                ON 85 PERCENT DO NOTIFY
                ON 95 PERCENT DO SUSPEND_IMMEDIATE
            """,
                "Analytics resource monitor (500 credits/month)",
            ),
        ]

        success_count = 0
        for query, description in resource_monitor_queries:
            if self.execute_query(query, description):
                success_count += 1
                self.deployment_status["resource_monitors"]["details"].append(
                    description
                )

        if success_count == len(resource_monitor_queries):
            self.deployment_status["resource_monitors"]["status"] = "completed"
            return True
        else:
            self.deployment_status["resource_monitors"]["status"] = "failed"
            return False

    def deploy_specialized_warehouses(self) -> bool:
        """Deploy specialized warehouses for different workload types."""
        logger.info("üè≠ Deploying specialized warehouses...")

        warehouse_queries = [
            (
                """
            CREATE OR REPLACE WAREHOUSE SOPHIA_AI_CHAT_WH
            WITH WAREHOUSE_SIZE = 'SMALL'
                AUTO_SUSPEND = 30
                AUTO_RESUME = TRUE
                INITIALLY_SUSPENDED = FALSE
                SCALING_POLICY = 'ECONOMY'
                MAX_CLUSTER_COUNT = 3
                MIN_CLUSTER_COUNT = 1
                COMMENT = 'Optimized for chat queries - fast response, low cost'
            """,
                "Chat warehouse (SMALL, 30s suspend)",
            ),
            (
                """
            CREATE OR REPLACE WAREHOUSE SOPHIA_AI_ANALYTICS_WH
            WITH WAREHOUSE_SIZE = 'MEDIUM'
                AUTO_SUSPEND = 300
                AUTO_RESUME = TRUE
                INITIALLY_SUSPENDED = TRUE
                SCALING_POLICY = 'STANDARD'
                MAX_CLUSTER_COUNT = 5
                MIN_CLUSTER_COUNT = 1
                COMMENT = 'Optimized for analytics and reporting workloads'
            """,
                "Analytics warehouse (MEDIUM, 300s suspend)",
            ),
            (
                """
            CREATE OR REPLACE WAREHOUSE SOPHIA_AI_ETL_WH
            WITH WAREHOUSE_SIZE = 'LARGE'
                AUTO_SUSPEND = 60
                AUTO_RESUME = TRUE
                INITIALLY_SUSPENDED = TRUE
                SCALING_POLICY = 'ECONOMY'
                MAX_CLUSTER_COUNT = 2
                MIN_CLUSTER_COUNT = 1
                COMMENT = 'Optimized for ETL and batch processing'
            """,
                "ETL warehouse (LARGE, 60s suspend)",
            ),
            (
                """
            CREATE OR REPLACE WAREHOUSE SOPHIA_AI_ML_WH
            WITH WAREHOUSE_SIZE = 'X-LARGE'
                AUTO_SUSPEND = 180
                AUTO_RESUME = TRUE
                INITIALLY_SUSPENDED = TRUE
                SCALING_POLICY = 'ECONOMY'
                MAX_CLUSTER_COUNT = 3
                MIN_CLUSTER_COUNT = 1
                COMMENT = 'Optimized for AI/ML processing and embeddings'
            """,
                "ML warehouse (X-LARGE, 180s suspend)",
            ),
        ]

        # Assign resource monitors to warehouses
        monitor_assignments = [
            (
                "ALTER WAREHOUSE SOPHIA_AI_CHAT_WH SET RESOURCE_MONITOR = SOPHIA_AI_PROD_MONITOR",
                "Chat warehouse monitor assignment",
            ),
            (
                "ALTER WAREHOUSE SOPHIA_AI_ANALYTICS_WH SET RESOURCE_MONITOR = SOPHIA_AI_ANALYTICS_MONITOR",
                "Analytics warehouse monitor assignment",
            ),
            (
                "ALTER WAREHOUSE SOPHIA_AI_ETL_WH SET RESOURCE_MONITOR = SOPHIA_AI_PROD_MONITOR",
                "ETL warehouse monitor assignment",
            ),
            (
                "ALTER WAREHOUSE SOPHIA_AI_ML_WH SET RESOURCE_MONITOR = SOPHIA_AI_PROD_MONITOR",
                "ML warehouse monitor assignment",
            ),
        ]

        success_count = 0
        total_operations = len(warehouse_queries) + len(monitor_assignments)

        # Create warehouses
        for query, description in warehouse_queries:
            if self.execute_query(query, description):
                success_count += 1
                self.deployment_status["warehouses"]["details"].append(description)

        # Assign resource monitors
        for query, description in monitor_assignments:
            if self.execute_query(query, description):
                success_count += 1

        if success_count == total_operations:
            self.deployment_status["warehouses"]["status"] = "completed"
            return True
        else:
            self.deployment_status["warehouses"]["status"] = "failed"
            return False

    def deploy_security_roles(self) -> bool:
        """Deploy security roles and access control."""
        logger.info("üîê Deploying security roles and access control...")

        security_queries = [
            # Service roles
            ("CREATE ROLE IF NOT EXISTS SOPHIA_AI_CHAT_SERVICE", "Chat service role"),
            (
                "CREATE ROLE IF NOT EXISTS SOPHIA_AI_ANALYTICS_SERVICE",
                "Analytics service role",
            ),
            ("CREATE ROLE IF NOT EXISTS SOPHIA_AI_ETL_SERVICE", "ETL service role"),
            ("CREATE ROLE IF NOT EXISTS SOPHIA_AI_ADMIN_SERVICE", "Admin service role"),
            # Grant database access
            (
                "GRANT USAGE ON DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_CHAT_SERVICE",
                "Chat service database access",
            ),
            (
                "GRANT USAGE ON DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ANALYTICS_SERVICE",
                "Analytics service database access",
            ),
            (
                "GRANT USAGE ON DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ETL_SERVICE",
                "ETL service database access",
            ),
            (
                "GRANT ALL ON DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ADMIN_SERVICE",
                "Admin service database access",
            ),
            # Grant warehouse access
            (
                "GRANT USAGE ON WAREHOUSE SOPHIA_AI_CHAT_WH TO ROLE SOPHIA_AI_CHAT_SERVICE",
                "Chat service warehouse access",
            ),
            (
                "GRANT USAGE ON WAREHOUSE SOPHIA_AI_ANALYTICS_WH TO ROLE SOPHIA_AI_ANALYTICS_SERVICE",
                "Analytics service warehouse access",
            ),
            (
                "GRANT USAGE ON WAREHOUSE SOPHIA_AI_ETL_WH TO ROLE SOPHIA_AI_ETL_SERVICE",
                "ETL service warehouse access",
            ),
            (
                "GRANT USAGE ON WAREHOUSE SOPHIA_AI_ML_WH TO ROLE SOPHIA_AI_ADMIN_SERVICE",
                "Admin service ML warehouse access",
            ),
            # Schema-level permissions
            (
                "GRANT USAGE ON SCHEMA SOPHIA_AI_PROD.UNIVERSAL_CHAT TO ROLE SOPHIA_AI_CHAT_SERVICE",
                "Chat service schema access",
            ),
            (
                "GRANT SELECT, INSERT, UPDATE ON ALL TABLES IN SCHEMA SOPHIA_AI_PROD.UNIVERSAL_CHAT TO ROLE SOPHIA_AI_CHAT_SERVICE",
                "Chat service table permissions",
            ),
            (
                "GRANT SELECT, INSERT, UPDATE ON ALL VIEWS IN SCHEMA SOPHIA_AI_PROD.UNIVERSAL_CHAT TO ROLE SOPHIA_AI_CHAT_SERVICE",
                "Chat service view permissions",
            ),
            # Analytics permissions
            (
                "GRANT USAGE ON ALL SCHEMAS IN DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ANALYTICS_SERVICE",
                "Analytics service all schemas",
            ),
            (
                "GRANT SELECT ON ALL TABLES IN DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ANALYTICS_SERVICE",
                "Analytics service read access",
            ),
            (
                "GRANT SELECT ON ALL VIEWS IN DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ANALYTICS_SERVICE",
                "Analytics service view access",
            ),
            # ETL permissions
            (
                "GRANT USAGE ON ALL SCHEMAS IN DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ETL_SERVICE",
                "ETL service all schemas",
            ),
            (
                "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN DATABASE SOPHIA_AI_PROD TO ROLE SOPHIA_AI_ETL_SERVICE",
                "ETL service full data access",
            ),
        ]

        success_count = 0
        for query, description in security_queries:
            if self.execute_query(query, description):
                success_count += 1

        if success_count == len(security_queries):
            self.deployment_status["security_roles"]["status"] = "completed"
            self.deployment_status["security_roles"]["details"] = [
                "SOPHIA_AI_CHAT_SERVICE (chat operations)",
                "SOPHIA_AI_ANALYTICS_SERVICE (read-only analytics)",
                "SOPHIA_AI_ETL_SERVICE (data processing)",
                "SOPHIA_AI_ADMIN_SERVICE (full admin access)",
            ]
            return True
        else:
            self.deployment_status["security_roles"]["status"] = "failed"
            return False

    def deploy_performance_optimization(self) -> bool:
        """Deploy performance optimization features."""
        logger.info("‚ö° Deploying performance optimization...")

        optimization_queries = [
            # Clustering for chat tables
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.KNOWLEDGE_ENTRIES CLUSTER BY (CREATED_AT, CATEGORY_ID)",
                "Knowledge entries clustering",
            ),
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.CONVERSATION_MESSAGES CLUSTER BY (SESSION_ID, CREATED_AT)",
                "Conversation messages clustering",
            ),
            # Clustering for AI Memory (if exists)
            (
                "ALTER TABLE SOPHIA_AI_PROD.AI_MEMORY.BUSINESS_MEMORIES CLUSTER BY (CREATED_AT, IMPORTANCE_SCORE)",
                "Business memories clustering",
            ),
            # Automatic clustering
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.KNOWLEDGE_ENTRIES SET ENABLE_AUTOMATIC_CLUSTERING = TRUE",
                "Knowledge entries auto-clustering",
            ),
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.CONVERSATION_MESSAGES SET ENABLE_AUTOMATIC_CLUSTERING = TRUE",
                "Conversation messages auto-clustering",
            ),
            (
                "ALTER TABLE SOPHIA_AI_PROD.AI_MEMORY.BUSINESS_MEMORIES SET ENABLE_AUTOMATIC_CLUSTERING = TRUE",
                "Business memories auto-clustering",
            ),
        ]

        success_count = 0
        for query, description in optimization_queries:
            if self.execute_query(query, description):
                success_count += 1
                self.deployment_status["performance_optimization"]["details"].append(
                    description
                )

        # Consider partial success acceptable since some tables might not exist
        if success_count >= 2:  # At least basic clustering
            self.deployment_status["performance_optimization"]["status"] = "completed"
            return True
        else:
            self.deployment_status["performance_optimization"]["status"] = "failed"
            return False

    def deploy_backup_recovery(self) -> bool:
        """Deploy backup and recovery features."""
        logger.info("üíæ Deploying backup and recovery features...")

        backup_queries = [
            # Extended time travel for critical tables
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.KNOWLEDGE_ENTRIES SET DATA_RETENTION_TIME_IN_DAYS = 7",
                "Knowledge entries 7-day retention",
            ),
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.CONVERSATION_MESSAGES SET DATA_RETENTION_TIME_IN_DAYS = 7",
                "Conversation messages 7-day retention",
            ),
            (
                "ALTER TABLE SOPHIA_AI_PROD.AI_MEMORY.BUSINESS_MEMORIES SET DATA_RETENTION_TIME_IN_DAYS = 7",
                "Business memories 7-day retention",
            ),
            # Schema evolution
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.KNOWLEDGE_ENTRIES SET ENABLE_SCHEMA_EVOLUTION = TRUE",
                "Knowledge entries schema evolution",
            ),
            (
                "ALTER TABLE SOPHIA_AI_PROD.UNIVERSAL_CHAT.CONVERSATION_MESSAGES SET ENABLE_SCHEMA_EVOLUTION = TRUE",
                "Conversation messages schema evolution",
            ),
        ]

        success_count = 0
        for query, description in backup_queries:
            if self.execute_query(query, description):
                success_count += 1
                self.deployment_status["backup_recovery"]["details"].append(description)

        # Consider partial success acceptable
        if success_count >= 2:
            self.deployment_status["backup_recovery"]["status"] = "completed"
            return True
        else:
            self.deployment_status["backup_recovery"]["status"] = "failed"
            return False

    def deploy_monitoring_schemas(self) -> bool:
        """Deploy monitoring and quality schemas."""
        logger.info("üìä Deploying monitoring schemas...")

        monitoring_queries = [
            # Monitoring schemas
            (
                "CREATE SCHEMA IF NOT EXISTS SOPHIA_AI_PROD.MONITORING",
                "Monitoring schema",
            ),
            ("CREATE SCHEMA IF NOT EXISTS SOPHIA_AI_PROD.QUALITY", "Quality schema"),
            ("CREATE SCHEMA IF NOT EXISTS SOPHIA_AI_PROD.BACKUPS", "Backups schema"),
            # Monitoring tables
            (
                """
            CREATE OR REPLACE TABLE SOPHIA_AI_PROD.MONITORING.QUERY_PERFORMANCE (
                query_id VARCHAR(255),
                query_text TEXT,
                execution_time_ms INTEGER,
                warehouse_name VARCHAR(255),
                user_name VARCHAR(255),
                execution_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
                bytes_scanned INTEGER,
                partitions_scanned INTEGER,
                cache_result BOOLEAN
            )
            """,
                "Query performance monitoring table",
            ),
            (
                """
            CREATE OR REPLACE TABLE SOPHIA_AI_PROD.MONITORING.WAREHOUSE_USAGE (
                warehouse_name VARCHAR(255),
                usage_date DATE,
                total_credits_used DECIMAL(10,2),
                total_queries INTEGER,
                avg_execution_time_ms INTEGER,
                peak_concurrent_queries INTEGER,
                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
            )
            """,
                "Warehouse usage monitoring table",
            ),
            (
                """
            CREATE OR REPLACE TABLE SOPHIA_AI_PROD.QUALITY.DATA_QUALITY_CHECKS (
                check_id VARCHAR(255),
                table_name VARCHAR(255),
                check_type VARCHAR(100),
                check_result VARCHAR(50),
                error_count INTEGER,
                total_rows INTEGER,
                check_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
                details TEXT
            )
            """,
                "Data quality checks table",
            ),
            # Grant permissions
            (
                "GRANT ALL ON SCHEMA SOPHIA_AI_PROD.MONITORING TO ROLE SOPHIA_AI_ADMIN_SERVICE",
                "Monitoring schema permissions",
            ),
            (
                "GRANT ALL ON SCHEMA SOPHIA_AI_PROD.QUALITY TO ROLE SOPHIA_AI_ADMIN_SERVICE",
                "Quality schema permissions",
            ),
            (
                "GRANT ALL ON SCHEMA SOPHIA_AI_PROD.BACKUPS TO ROLE SOPHIA_AI_ADMIN_SERVICE",
                "Backups schema permissions",
            ),
        ]

        success_count = 0
        for query, description in monitoring_queries:
            if self.execute_query(query, description):
                success_count += 1
                self.deployment_status["monitoring_schemas"]["details"].append(
                    description
                )

        if success_count == len(monitoring_queries):
            self.deployment_status["monitoring_schemas"]["status"] = "completed"
            return True
        else:
            self.deployment_status["monitoring_schemas"]["status"] = "failed"
            return False

    def generate_deployment_report(self) -> dict:
        """Generate comprehensive deployment report."""
        report = {
            "deployment_timestamp": datetime.now().isoformat(),
            "overall_status": (
                "success"
                if all(
                    component["status"] == "completed"
                    for component in self.deployment_status.values()
                )
                else "partial_failure"
            ),
            "components": self.deployment_status,
            "summary": {
                "total_components": len(self.deployment_status),
                "successful_components": sum(
                    1
                    for component in self.deployment_status.values()
                    if component["status"] == "completed"
                ),
                "failed_components": sum(
                    1
                    for component in self.deployment_status.values()
                    if component["status"] == "failed"
                ),
            },
        }

        # Save report to file
        report_file = f"snowflake_stability_deployment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)

        logger.info(f"üìã Deployment report saved to {report_file}")
        return report

    def deploy_all(self) -> bool:
        """Deploy all Snowflake stability enhancements."""
        logger.info("üöÄ Starting comprehensive Snowflake stability deployment...")

        # Initialize connection
        if not self.connect():
            return False

        # Deploy components in order
        components = [
            ("Resource Monitors", self.deploy_resource_monitors),
            ("Specialized Warehouses", self.deploy_specialized_warehouses),
            ("Security Roles", self.deploy_security_roles),
            ("Performance Optimization", self.deploy_performance_optimization),
            ("Backup & Recovery", self.deploy_backup_recovery),
            ("Monitoring Schemas", self.deploy_monitoring_schemas),
        ]

        success_count = 0
        for component_name, deploy_func in components:
            logger.info(f"üì¶ Deploying {component_name}...")
            if deploy_func():
                success_count += 1
                logger.info(f"‚úÖ {component_name} deployed successfully")
            else:
                logger.error(f"‚ùå {component_name} deployment failed")

        # Close connection
        if self.conn:
            self.conn.close()
            logger.info("Snowflake connection closed")

        # Generate report
        report = self.generate_deployment_report()

        # Log summary
        logger.info(
            f"""
        ============================================================
        üéâ SNOWFLAKE STABILITY DEPLOYMENT COMPLETED
        ============================================================
        üìä Summary:
           Total Components: {len(components)}
           Successful: {success_count}
           Failed: {len(components) - success_count}
           Overall Status: {report["overall_status"]}

        üìã Components Status:
        """
        )

        for comp_name, status in self.deployment_status.items():
            status_icon = "‚úÖ" if status["status"] == "completed" else "‚ùå"
            logger.info(f"   {status_icon} {comp_name}: {status['status']}")

        logger.info("============================================================")

        return success_count == len(components)


def main():
    """Main deployment function."""
    deployer = SnowflakeStabilityDeployer()
    success = deployer.deploy_all()

    if success:
        logger.info("üéâ All Snowflake stability enhancements deployed successfully!")
        sys.exit(0)
    else:
        logger.error("‚ùå Some components failed to deploy. Check logs for details.")
        sys.exit(1)


if __name__ == "__main__":
    main()
