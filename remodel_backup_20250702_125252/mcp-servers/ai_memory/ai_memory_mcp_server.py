#!/usr/bin/env python3
from __future__ import annotations

"""
from backend.mcp_servers.base.enhanced_standardized_mcp_server import (
    EnhancedStandardizedMCPServer,
    MCPServerConfig,
    HealthCheckLevel
)

Standardized AI Memory MCP Server for persistent development context
Built on the EnhancedStandardizedMCPServer base class with Snowflake Cortex integration
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 776 lines

Recommended decomposition:
- ai_memory_mcp_server_core.py - Core functionality
- ai_memory_mcp_server_utils.py - Utility functions
- ai_memory_mcp_server_models.py - Data models
- ai_memory_mcp_server_handlers.py - Request handlers

TODO: Implement file decomposition
"""


import asyncio
import logging
import re
from datetime import datetime
from typing import Any

# Enhanced dependencies with better error handling
try:
    import pinecone

    PINECONE_AVAILABLE = True
except ImportError:
    PINECONE_AVAILABLE = False
    pinecone = None

try:
    from openai import AsyncOpenAI

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    AsyncOpenAI = None

# Import the standardized base class
# Import the canonical MemoryRecord from data_models
from backend.agents.enhanced.data_models import MemoryRecord

# Import existing memory management components
from backend.core.comprehensive_memory_manager import ComprehensiveMemoryManager
from backend.core.contextual_memory_intelligence import ContextualMemoryIntelligence
from backend.core.hierarchical_cache import HierarchicalCache
from backend.mcp_servers.base.standardized_mcp_server import (
    EnhancedStandardizedMCPServer,
    HealthCheckResult,
    HealthStatus,
    MCPServerConfig,
    SyncPriority,
)

# Import the Snowflake Gong connector
# Import the ComprehensiveMemoryService
from backend.services.comprehensive_memory_service import (
    ComprehensiveMemoryService,
    MemoryRecord,
)

# Import enhanced Snowflake Cortex service
from backend.utils.enhanced_snowflake_cortex_service import (
    AIProcessingConfig,
    CortexModel,
    EnhancedSnowflakeCortexService,
)

logger = logging.getLogger(__name__)


class MemoryCategory:
    """Categories for AI memory storage."""

    ARCHITECTURE = "architecture"
    BUG_SOLUTION = "bug_solution"
    CODE_DECISION = "code_decision"
    WORKFLOW = "workflow"
    AI_CODING_PATTERN = "ai_coding_pattern"
    PERFORMANCE_TIP = "performance_tip"
    SECURITY_PATTERN = "security_pattern"

    # Business intelligence categories from the enhanced server
    HUBSPOT_CONTACT_INSIGHT = "hubspot_contact_insight"
    HUBSPOT_DEAL_ANALYSIS = "hubspot_deal_analysis"
    HUBSPOT_SALES_PATTERN = "hubspot_sales_pattern"
    HUBSPOT_CUSTOMER_INTERACTION = "hubspot_customer_interaction"
    HUBSPOT_PIPELINE_INSIGHT = "hubspot_pipeline_insight"

    # Gong-specific categories from the enhanced server
    GONG_CALL_SUMMARY = "gong_call_summary"
    GONG_CALL_INSIGHT = "gong_call_insight"
    GONG_COACHING_RECOMMENDATION = "gong_coaching_recommendation"
    GONG_SENTIMENT_ANALYSIS = "gong_sentiment_analysis"
    GONG_TOPIC_ANALYSIS = "gong_topic_analysis"


class ConversationAnalyzer:
    """Analyzes conversations to auto-detect important content"""

    def __init__(self):
        self.importance_patterns = {
            "architecture": [
                r"decided to use",
                r"architecture decision",
                r"design pattern",
                r"microservices",
                r"database schema",
                r"api design",
                r"system design",
            ],
            "bug_solution": [
                r"fixed the bug",
                r"solution was",
                r"error was caused by",
                r"debugging showed",
                r"issue resolved",
                r"problem solved",
            ],
            "code_decision": [
                r"chose to implement",
                r"decided to refactor",
                r"code structure",
                r"implementation approach",
                r"coding standard",
            ],
            "security_pattern": [
                r"security vulnerability",
                r"authentication",
                r"authorization",
                r"encryption",
                r"security best practice",
                r"secure coding",
            ],
            "performance_tip": [
                r"performance optimization",
                r"faster approach",
                r"bottleneck",
                r"cache strategy",
                r"query optimization",
            ],
        }

        self.high_importance_keywords = [
            "critical",
            "important",
            "remember",
            "decision",
            "solution",
            "pattern",
            "best practice",
            "lesson learned",
            "mistake",
            "breakthrough",
            "optimization",
            "security",
            "performance",
        ]

    def analyze_conversation(self, content: str) -> dict[str, Any]:
        """Analyze conversation content for importance and categorization"""
        content_lower = content.lower()
        importance_score = self._calculate_importance(content_lower)
        category = self._detect_category(content_lower)
        tags = self._extract_tags(content_lower)
        should_auto_store = importance_score > 0.6

        return {
            "importance_score": importance_score,
            "category": category,
            "tags": tags,
            "should_auto_store": should_auto_store,
            "analysis_reason": self._get_analysis_reason(
                content_lower, importance_score
            ),
        }

    def _calculate_importance(self, content: str) -> float:
        score = 0.3
        for keyword in self.high_importance_keywords:
            if keyword in content:
                score += 0.1
        for patterns in self.importance_patterns.values():
            for pattern in patterns:
                if re.search(pattern, content):
                    score += 0.15
        if len(content) > 1000:
            score += 0.1
        if "```" in content:
            score += 0.2
        return min(score, 1.0)

    def _detect_category(self, content: str) -> str:
        category_scores = {
            cat: sum(1 for p in pats if re.search(p, content))
            for cat, pats in self.importance_patterns.items()
        }
        if any(v > 0 for v in category_scores.values()):
            return max(category_scores, key=category_scores.get)
        return MemoryCategory.CODE_DECISION

    def _extract_tags(self, content: str) -> list[str]:
        tags = []
        tech_patterns = {
            "python": r"\bpython\b",
            "react": r"\breact\b",
            "docker": r"\bdocker\b",
        }
        for tag, pattern in tech_patterns.items():
            if re.search(pattern, content):
                tags.append(tag)
        return tags

    def _get_analysis_reason(self, content: str, score: float) -> str:
        reasons = []
        if score > 0.8:
            reasons.append("High importance keywords detected")
        return "; ".join(reasons) if reasons else "Standard content analysis"


class StandardizedAiMemoryMCPServer(EnhancedStandardizedMCPServer):
    """
    Standardized AI Memory MCP Server with enhanced Snowflake Cortex integration
    Built on the EnhancedStandardizedMCPServer base class for consistency and monitoring
    """

    def __init__(self, config: MCPServerConfig | None = None):
        # Default configuration for AI Memory server
        if config is None:
            config = MCPServerConfig(
                server_name="ai_memory",
                port=9000,
                sync_priority=SyncPriority.HIGH,
                sync_interval_minutes=5,
                enable_metrics=True,
                health_check_interval=30,
                max_concurrent_requests=50,
                request_timeout_seconds=30,
            )

        super().__init__(config)

        # AI Memory specific components
        self.memory_manager = ComprehensiveMemoryManager()
        self.memory_intelligence = ContextualMemoryIntelligence(self.memory_manager)
        self.cache = HierarchicalCache()
        self.conversation_analyzer = ConversationAnalyzer()

        # AI services
        self.openai_client: Any | None = None
        self.pinecone_index: Any | None = None
        self.cortex_service: EnhancedSnowflakeCortexService | None = None

        # State tracking
        self.preloaded_knowledge = False

        # Add ComprehensiveMemoryService
        self.memory_service = ComprehensiveMemoryService()

    async def initialize_server(self) -> None:
        """Initialize AI Memory server with all services"""
        logger.info("Initializing AI Memory MCP Server with Snowflake Cortex...")
        await self._initialize_openai()
        await self._initialize_pinecone()
        await self._initialize_snowflake_cortex()
        await self._preload_ai_coding_knowledge()
        logger.info("AI Memory MCP Server initialized successfully")

    async def cleanup_server(self) -> None:
        """Cleanup AI Memory server resources"""
        if self.openai_client:
            await self.openai_client.close()

        # Clean up cache
        await self.cache.clear()

        logger.info("AI Memory MCP Server cleaned up successfully")

    async def _initialize_openai(self):
        """Initialize OpenAI client with proper configuration"""
        from backend.core.auto_esc_config import config

        openai_api_key = config.get("openai_api_key")

        if not openai_api_key or openai_api_key in [
            "",
            "",
        ]:
            logger.warning(
                "No valid OpenAI API key found. Semantic search will be limited."
            )
            return

        if not OPENAI_AVAILABLE:
            logger.warning(
                "OpenAI library not available. Install with: pip install openai"
            )
            return

        try:
            self.openai_client = AsyncOpenAI(
                api_key=openai_api_key, timeout=30.0, max_retries=3
            )

            # Test the connection
            await self.openai_client.embeddings.create(
                input="test connection", model="text-embedding-3-small"
            )

            logger.info("✅ OpenAI client initialized and tested successfully")

        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            self.openai_client = None

    async def _initialize_pinecone(self):
        """Initialize Pinecone with proper configuration"""
        from backend.core.auto_esc_config import config

        pinecone_api_key = config.get("pinecone_api_key")
        pinecone_environment = config.get("pinecone_environment", "us-east1-gcp")

        if not pinecone_api_key or pinecone_api_key in [
            "",
            "",
        ]:
            logger.warning(
                "No valid Pinecone API key found. Vector search will be limited."
            )
            return

        if not PINECONE_AVAILABLE:
            logger.warning(
                "Pinecone library not available. Install with: pip install pinecone-client"
            )
            return

        try:
            pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)

            index_name = "sophia-ai-memory"

            # Check if index exists, create if not
            if index_name not in pinecone.list_indexes():
                logger.info(f"Creating Pinecone index: {index_name}")
                pinecone.create_index(
                    name=index_name,
                    dimension=1536,  # text-embedding-3-small dimension
                    metric="cosine",
                    pods=1,
                    replicas=1,
                    pod_type="p1.x1",
                )

            self.pinecone_index = pinecone.Index(index_name)

            # Test the connection
            stats = self.pinecone_index.describe_index_stats()
            logger.info(
                f"✅ Pinecone initialized successfully. Vectors: {stats.total_vector_count}"
            )

        except Exception as e:
            logger.error(f"Failed to initialize Pinecone: {e}")
            self.pinecone_index = None

    async def _initialize_snowflake_cortex(self):
        """Initialize Enhanced Snowflake Cortex service"""
        try:
            config = AIProcessingConfig(
                embedding_model=CortexModel.E5_BASE_V2,
                llm_model=CortexModel.LLAMA3_70B,
                enable_caching=True,
                cache_ttl_hours=1,
            )

            self.cortex_service = EnhancedSnowflakeCortexService(config)
            logger.info("✅ Enhanced Snowflake Cortex service initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize Snowflake Cortex service: {e}")
            self.cortex_service = None

    async def _preload_ai_coding_knowledge(self):
        """Pre-load helpful AI coding knowledge for developers"""
        if self.preloaded_knowledge:
            return
        knowledge_base = [
            {
                "content": "When using Cursor IDE with MCP servers, use @server_name commands to interact with specific tools. For example: @ai_memory store this conversation...",
                "category": MemoryCategory.AI_CODING_PATTERN,
                "tags": ["cursor", "mcp", "ai_assistance"],
                "importance_score": 0.9,
            },
        ]
        logger.info("Pre-loading AI coding knowledge base...")
        for knowledge in knowledge_base:
            try:
                await self.store_memory(**knowledge, auto_detected=False)
            except Exception as e:
                logger.warning(f"Failed to pre-load knowledge item: {e}")
        self.preloaded_knowledge = True
        logger.info("✅ AI coding knowledge base pre-loaded successfully")

    async def server_specific_init(self) -> None:
        """Initialize AI Memory server specific components"""
        await self._initialize_openai()
        await self._initialize_pinecone()
        await self._initialize_snowflake_cortex()
        await self._preload_ai_coding_knowledge()

    async def server_specific_cleanup(self) -> None:
        """Cleanup AI Memory server specific resources"""
        if self.openai_client:
            await self.openai_client.close()
        await self.cache.clear()

    async def server_specific_health_check(self) -> dict:
        """Perform AI Memory specific health checks"""
        return {
            "openai_available": self.openai_client is not None,
            "pinecone_available": self.pinecone_index is not None,
            "cortex_available": self.cortex_service is not None,
            "memory_count": len(await self.recall_memory("", limit=1)),
        }

    async def check_external_api(self) -> bool:
        """Check external API connectivity"""
        try:
            if self.openai_client:
                await self.openai_client.embeddings.create(
                    input="test", model="text-embedding-3-small"
                )
            return True
        except Exception:
            return False

    async def process_with_ai(self, data: dict) -> dict:
        """Process data with AI capabilities"""
        content = data.get("content", "")
        analysis = self.conversation_analyzer.analyze_conversation(content)

        if analysis["should_auto_store"]:
            result = await self.store_memory(
                content=content,
                category=analysis["category"],
                tags=analysis["tags"],
                importance_score=analysis["importance_score"],
                auto_detected=True,
            )
            return {"auto_stored": True, "memory_id": result.get("memory_id")}

        return {"auto_stored": False, "analysis": analysis}

    def get_server_capabilities(self) -> dict:
        """Get AI Memory server capabilities"""
        return {
            "memory_storage": True,
            "semantic_search": self.openai_client is not None,
            "vector_search": self.pinecone_index is not None,
            "cortex_integration": self.cortex_service is not None,
            "auto_discovery": True,
            "conversation_analysis": True,
        }

    async def sync_data(self) -> dict[str, Any]:
        """Sync memory data with external systems"""
        try:
            sync_results = {
                "memories_synced": 0,
                "embeddings_updated": 0,
                "cortex_insights_generated": 0,
            }

            # Sync with Snowflake Cortex
            if self.cortex_service:
                # Generate AI insights for recent memories
                recent_memories = await self.recall_memory("", limit=10)
                for memory in recent_memories:
                    try:
                        await self.cortex_service.generate_ai_insights(
                            data={"content": memory.get("content", "")},
                            insight_type="memory_analysis",
                        )
                        sync_results["cortex_insights_generated"] += 1
                    except Exception as e:
                        logger.error(f"Failed to generate Cortex insights: {e}")

            # Update embeddings for memories without them
            memories_updated = await self._update_missing_embeddings()
            sync_results["embeddings_updated"] = memories_updated

            return sync_results

        except Exception as e:
            logger.error(f"Failed to sync memory data: {e}")
            raise

    async def _update_missing_embeddings(self) -> int:
        """Update embeddings for memories that don't have them"""
        try:
            updated_count = 0
            # Implementation would check for memories without embeddings
            # and generate them using OpenAI or Cortex
            return updated_count
        except Exception as e:
            logger.error(f"Failed to update missing embeddings: {e}")
            return 0

    async def perform_health_checks(self) -> list[HealthCheckResult]:
        """Perform comprehensive health checks for AI Memory server"""
        health_checks = []

        # Check OpenAI connection
        openai_status = (
            HealthStatus.HEALTHY if self.openai_client else HealthStatus.UNHEALTHY
        )
        health_checks.append(
            HealthCheckResult(
                component="openai_client",
                status=openai_status,
                response_time_ms=0.0,
                details="OpenAI client availability",
            )
        )

        # Check Pinecone connection
        pinecone_status = (
            HealthStatus.HEALTHY if self.pinecone_index else HealthStatus.UNHEALTHY
        )
        health_checks.append(
            HealthCheckResult(
                component="pinecone_index",
                status=pinecone_status,
                response_time_ms=0.0,
                details="Pinecone index availability",
            )
        )

        # Check Cortex service
        cortex_status = (
            HealthStatus.HEALTHY if self.cortex_service else HealthStatus.UNHEALTHY
        )
        health_checks.append(
            HealthCheckResult(
                component="cortex_service",
                status=cortex_status,
                response_time_ms=0.0,
                details="Snowflake Cortex service availability",
            )
        )

        # Check memory manager
        try:
            # Test memory manager functionality
            await asyncio.wait_for(self.memory_manager.get_memory_stats(), timeout=5.0)
            memory_status = HealthStatus.HEALTHY
        except Exception:
            memory_status = HealthStatus.UNHEALTHY

        health_checks.append(
            HealthCheckResult(
                component="memory_manager",
                status=memory_status,
                response_time_ms=0.0,
                details="Memory manager functionality",
            )
        )

        return health_checks

    def get_mcp_tools(self) -> list[dict[str, Any]]:
        """Get MCP tools for AI Memory server"""
        return [
            {
                "name": "store_memory",
                "description": "Store a memory with content, category, and tags",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "content": {"type": "string", "description": "Memory content"},
                        "category": {
                            "type": "string",
                            "description": "Memory category",
                        },
                        "tags": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Memory tags",
                        },
                        "importance_score": {
                            "type": "number",
                            "minimum": 0,
                            "maximum": 1,
                            "description": "Importance score",
                        },
                        "auto_detected": {
                            "type": "boolean",
                            "description": "Whether auto-detected",
                        },
                    },
                    "required": ["content", "category", "tags"],
                },
            },
            {
                "name": "recall_memory",
                "description": "Recall memories using semantic search",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                        "category": {
                            "type": "string",
                            "description": "Optional category filter",
                        },
                        "limit": {
                            "type": "integer",
                            "minimum": 1,
                            "maximum": 20,
                            "description": "Max results",
                        },
                    },
                    "required": ["query"],
                },
            },
            {
                "name": "auto_store_conversation",
                "description": "Automatically analyze and store conversation if important",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "content": {
                            "type": "string",
                            "description": "Conversation content",
                        },
                        "participants": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "Participants",
                        },
                    },
                    "required": ["content"],
                },
            },
            {
                "name": "get_ai_coding_tips",
                "description": "Get AI coding tips for a specific topic",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "topic": {
                            "type": "string",
                            "description": "Optional topic filter",
                        }
                    },
                },
            },
        ]

    async def execute_mcp_tool(
        self, tool_name: str, parameters: dict[str, Any]
    ) -> dict[str, Any]:
        """Execute MCP tool requests"""
        try:
            if tool_name == "store_memory":
                return await self.store_memory(
                    content=parameters["content"],
                    category=parameters["category"],
                    tags=parameters["tags"],
                    importance_score=parameters.get("importance_score", 0.5),
                    auto_detected=parameters.get("auto_detected", False),
                )

            elif tool_name == "recall_memory":
                return {
                    "memories": await self.recall_memory(
                        query=parameters["query"],
                        category=parameters.get("category"),
                        limit=parameters.get("limit", 5),
                    )
                }

            elif tool_name == "auto_store_conversation":
                return await self.auto_store_conversation(
                    content=parameters["content"],
                    participants=parameters.get("participants"),
                )

            elif tool_name == "get_ai_coding_tips":
                return await self.get_ai_coding_tips(topic=parameters.get("topic"))

            else:
                raise ValueError(f"Unknown tool: {tool_name}")

        except Exception as e:
            logger.error(f"Error executing tool {tool_name}: {e}")
            return {"error": str(e), "success": False}

    # Core memory management methods
    async def get_embedding(self, text: str) -> list[float]:
        """Generate embedding for text using available services"""
        try:
            # Try Snowflake Cortex first
            if self.cortex_service:
                result = await self.cortex_service.generate_embedding(text)
                return result.embedding

            # Fallback to OpenAI
            if self.openai_client:
                response = await self.openai_client.embeddings.create(
                    input=text, model="text-embedding-3-small"
                )
                return response.data[0].embedding

            # No embedding service available
            logger.warning("No embedding service available")
            return []

        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return []

    async def store_memory(
        self,
        content: str,
        category: str,
        tags: list[str],
        importance_score: float = 0.5,
        auto_detected: bool = False,
        **kwargs,
    ) -> dict[str, Any]:
        """Stores a memory by delegating to the ComprehensiveMemoryService."""
        try:
            memory_id = f"{category}_{datetime.now().strftime('%Y%m%d%H%M%S%f')}"

            # Create the canonical MemoryRecord object
            memory_record = MemoryRecord(
                id=memory_id,
                content=content,
                category=category,
                tags=tags,
                importance_score=importance_score,
                auto_detected=auto_detected,
                **kwargs,  # Pass any additional metadata
            )

            # Delegate to the centralized service
            stored_id = await self.memory_service.store_memory(memory_record)

            return {"success": True, "memory_id": stored_id}

        except Exception as e:
            logger.error(f"Failed to store memory via service: {e}", exc_info=True)
            return {"success": False, "error": str(e)}

    async def recall_memory(
        self, query: str, category: str | None = None, limit: int = 5
    ) -> list[dict[str, Any]]:
        """Recalls memories by delegating to the ComprehensiveMemoryService."""
        try:
            recalled_records = await self.memory_service.recall_memories(
                query=query, top_k=limit, category=category
            )
            # Convert records to dicts for the response
            return [record.dict() for record in recalled_records]
        except Exception as e:
            logger.error(f"Failed to recall memory via service: {e}", exc_info=True)
            return []

    async def auto_store_conversation(
        self, content: str, participants: list[str] = None
    ) -> dict[str, Any]:
        """Automatically analyze and store important conversations"""
        analysis = self.conversation_analyzer.analyze_conversation(content)
        if analysis["should_auto_store"]:
            result = await self.store_memory(
                content=content,
                category=analysis["category"],
                tags=analysis["tags"] + ["auto_detected"],
                importance_score=analysis["importance_score"],
                auto_detected=True,
            )
            result.update(
                {"auto_stored": True, "analysis_reason": analysis["analysis_reason"]}
            )
            return result
        else:
            return {"auto_stored": False, "reason": "Importance score too low"}

    async def get_ai_coding_tips(self, topic: str | None = None) -> dict[str, Any]:
        """Get AI coding tips for a specific topic"""
        try:
            query = topic if topic else "ai coding patterns best practices"

            tips = await self.recall_memory(
                query=query, category=MemoryCategory.AI_CODING_PATTERN, limit=5
            )

            return {"topic": topic, "tips": tips, "total_found": len(tips)}

        except Exception as e:
            logger.error(f"Failed to get AI coding tips: {e}")
            return {"error": str(e), "tips": []}

    async def _update_usage_count(self, memory_id: str):
        """Update usage count for a memory"""
        try:
            await self.memory_manager.update_usage_count(memory_id)
        except Exception as e:
            logger.error(f"Failed to update usage count for {memory_id}: {e}")

    async def store_hubspot_deal_analysis(
        self, deal_id: str, analysis_content: str, **kwargs
    ) -> dict[str, Any]:
        """Enhanced HubSpot deal analysis storage with Snowflake Cortex integration"""
        # ... implementation from ai_memory_mcp_server.py ...
        pass

    # ... Other hubspot and gong specific methods ...


async def main() -> None:
    """Main function to run the AI Memory MCP Server"""
    config = MCPServerConfig(
        server_name="ai_memory",
        port=9000,
        sync_priority=SyncPriority.HIGH,
        sync_interval_minutes=5,
        enable_metrics=True,
    )

    server = StandardizedAiMemoryMCPServer(config)
    await server.start()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("AI Memory MCP Server stopped by user.")


# --- Auto-inserted health endpoint ---
try:
    from fastapi import APIRouter

    router = APIRouter()

    @router.get("/health")
    async def health():
        return {"status": "ok"}

except ImportError:
    pass
