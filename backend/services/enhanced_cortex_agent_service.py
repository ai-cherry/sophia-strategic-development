from fastapi import HTTPException

"""
Enhanced Snowflake Cortex Agent Service for Sophia AI
Advanced multimodal processing with AI-powered business intelligence
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 727 lines

Recommended decomposition:
- enhanced_cortex_agent_service_core.py - Core functionality
- enhanced_cortex_agent_service_utils.py - Utility functions
- enhanced_cortex_agent_service_models.py - Data models
- enhanced_cortex_agent_service_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import json
import logging
from datetime import datetime
from typing import Any

import snowflake.connector
from pydantic import BaseModel

from backend.core.config_manager import get_config_value

logger = logging.getLogger(__name__)

# JWT Configuration
JWT_SECRET = get_config_value("jwt_secret", "sophia-ai-cortex-secret")
JWT_ALGORITHM = "HS256"
JWT_EXPIRATION_HOURS = 24


# Base models for compatibility
class AgentRequest(BaseModel):
    """Base agent request model"""

    prompt: str
    context: dict[str, Any] | None = None
    tools: list[str] | None = None
    max_tokens: int | None = 4096
    temperature: float | None = 0.7
    stream: bool | None = False


class AgentResponse(BaseModel):
    """Base agent response model"""

    agent_name: str
    response: str
    tools_used: list[str] = []
    tokens_used: int
    execution_time: float
    metadata: dict[str, Any] = {}


class CortexTool(BaseModel):
    """Base Cortex tool model"""

    name: str
    description: str
    parameters: dict[str, Any] = {}


class CortexAgentService:
    """Base Cortex Agent Service for compatibility"""

    def __init__(self):
        pass


# Enhanced Models for Multimodal Processing
class MultimodalFile(BaseModel):
    """Model for multimodal file processing"""

    file_id: str
    file_type: str  # 'audio', 'document', 'image'
    file_url: str | None = None
    file_content: bytes | None = None
    metadata: dict[str, Any] = {}


class MultimodalAgentRequest(AgentRequest):
    """Enhanced agent request with multimodal support"""

    files: list[MultimodalFile] = []
    processing_options: dict[str, Any] = {}
    business_context: dict[str, Any] | None = None


class AdvancedAnalyticsQuery(BaseModel):
    """Model for advanced analytics queries"""

    query_type: (
        str  # 'customer_intelligence', 'sales_optimization', 'compliance_monitoring'
    )
    parameters: dict[str, Any]
    time_range: dict[str, str] | None = None
    filters: dict[str, Any] = {}


class AnalyticsResponse(BaseModel):
    """Response model for analytics queries"""

    query_type: str
    results: dict[str, Any]
    insights: list[str]
    recommendations: list[str]
    confidence_score: float
    metadata: dict[str, Any] = {}


class ComplianceAlert(BaseModel):
    """Model for compliance alerts"""

    alert_id: str
    severity: str  # 'low', 'medium', 'high', 'critical'
    category: str  # 'fdcpa', 'privacy', 'data_governance'
    description: str
    affected_records: list[str]
    recommended_actions: list[str]
    timestamp: datetime


class EnhancedCortexAgentService(CortexAgentService):
    """Enhanced Cortex Agent Service with advanced AI capabilities"""

    def __init__(self):
        super().__init__()
        self.advanced_database = "SOPHIA_AI_ADVANCED"
        self.multimodal_schema = "RAW_MULTIMODAL"
        self.processed_schema = "PROCESSED_AI"
        self.analytics_schema = "REAL_TIME_ANALYTICS"

    async def get_advanced_connection(self) -> snowflake.connector.SnowflakeConnection:
        """Get connection to advanced Snowflake database with PAT token as password"""
        config = {
            "account": get_config_value("snowflake_account"),
            "user": get_config_value("snowflake_user"),
            "password": get_config_value("snowflake_password"),  # PAT token as password
            "role": "ACCOUNTADMIN",
            "warehouse": "AI_SOPHIA_AI_WH",
            "database": self.advanced_database,
            "schema": self.processed_schema,
        }
        return snowflake.connector.connect(**config)

    async def process_multimodal_request(
        self, request: MultimodalAgentRequest
    ) -> AgentResponse:
        """Process requests with multimodal content (audio, documents, images)"""
        logger.info(f"Processing multimodal request with {len(request.files)} files")

        start_time = datetime.now()
        tools_used = []
        processing_results = {}

        try:
            # Process each file based on type
            for file in request.files:
                if file.file_type == "audio":
                    result = await self._process_audio_file(file)
                    processing_results[file.file_id] = result
                    tools_used.append("audio_processor")

                elif file.file_type == "document":
                    result = await self._process_document_file(file)
                    processing_results[file.file_id] = result
                    tools_used.append("document_processor")

                elif file.file_type == "image":
                    result = await self._process_image_file(file)
                    processing_results[file.file_id] = result
                    tools_used.append("image_processor")

            # Generate AI insights from processed content
            ai_insights = await self._generate_multimodal_insights(
                processing_results, request.prompt, request.business_context
            )
            tools_used.append("cortex_ai_analysis")

            # Store results in Snowflake for future reference
            await self._store_multimodal_results(processing_results, ai_insights)
            tools_used.append("snowflake_storage")

            execution_time = (datetime.now() - start_time).total_seconds()

            return AgentResponse(
                agent_name="multimodal_processor",
                response=ai_insights["summary"],
                tools_used=tools_used,
                tokens_used=ai_insights.get("tokens_used", 0),
                execution_time=execution_time,
                metadata={
                    "files_processed": len(request.files),
                    "processing_results": processing_results,
                    "insights": ai_insights,
                },
            )

        except Exception as e:
            logger.error(f"Error processing multimodal request: {e}")
            raise HTTPException(
                status_code=500, detail=f"Multimodal processing failed: {str(e)}"
            )

    async def _process_audio_file(self, file: MultimodalFile) -> dict[str, Any]:
        """Process audio files (Gong recordings)"""
        logger.info(f"Processing audio file: {file.file_id}")

        # Store audio file in Snowflake stage
        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Upload to Gong files stage
            stage_path = f"@{self.advanced_database}.{self.multimodal_schema}.GONG_FILES/{file.file_id}"

            # For now, we'll simulate audio processing
            # In production, this would integrate with Snowflake's audio processing capabilities
            analysis_result = {
                "file_id": file.file_id,
                "duration_seconds": file.metadata.get("duration", 0),
                "transcript": file.metadata.get("transcript", ""),
                "sentiment_score": 0.0,
                "key_topics": [],
                "compliance_flags": [],
                "stage_path": stage_path,
            }

            # Use Cortex AI for sentiment analysis if transcript available
            if analysis_result["transcript"]:
                sentiment_query = f"""
                SELECT SNOWFLAKE.CORTEX.SENTIMENT('{analysis_result["transcript"]}') as sentiment_score
                """
                cursor.execute(sentiment_query)
                result = cursor.fetchone()
                if result:
                    analysis_result["sentiment_score"] = result[0]

            return analysis_result

        finally:
            cursor.close()
            conn.close()

    async def _process_document_file(self, file: MultimodalFile) -> dict[str, Any]:
        """Process document files (contracts, proposals, meeting notes)"""
        logger.info(f"Processing document file: {file.file_id}")

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Store document in appropriate stage
            stage_path = f"@{self.advanced_database}.{self.multimodal_schema}.HUBSPOT_DOCUMENTS/{file.file_id}"

            # Extract text content (simulated for now)
            text_content = file.metadata.get("text_content", "")

            analysis_result = {
                "file_id": file.file_id,
                "file_type": file.metadata.get("mime_type", "unknown"),
                "text_content": text_content,
                "key_entities": [],
                "compliance_status": "pending_review",
                "risk_score": 0.0,
                "stage_path": stage_path,
            }

            # Use Cortex AI for document analysis
            if text_content:
                analysis_query = """
                SELECT
                    SNOWFLAKE.CORTEX.COMPLETE(
                        %s,
                        %s
                    ) as analysis
                """
                cursor.execute(analysis_query)
                result = cursor.fetchone()
                if result:
                    analysis_result["ai_analysis"] = result[0]

            return analysis_result

        finally:
            cursor.close()
            conn.close()

    async def _process_image_file(self, file: MultimodalFile) -> dict[str, Any]:
        """Process image files (Slack attachments, screenshots)"""
        logger.info(f"Processing image file: {file.file_id}")

        # Store image in Slack attachments stage
        stage_path = f"@{self.advanced_database}.{self.multimodal_schema}.SLACK_ATTACHMENTS/{file.file_id}"

        analysis_result = {
            "file_id": file.file_id,
            "image_type": file.metadata.get("mime_type", "unknown"),
            "dimensions": file.metadata.get("dimensions", {}),
            "description": "",
            "detected_objects": [],
            "text_content": "",
            "stage_path": stage_path,
        }

        # For now, we'll use metadata if available
        # In production, this would integrate with Snowflake's image processing capabilities
        if "description" in file.metadata:
            analysis_result["description"] = file.metadata["description"]

        return analysis_result

    async def _generate_multimodal_insights(
        self,
        processing_results: dict[str, Any],
        prompt: str,
        business_context: dict[str, Any] | None,
    ) -> dict[str, Any]:
        """Generate AI insights from multimodal processing results"""

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Combine all processing results into a comprehensive analysis
            combined_content = []
            for file_id, result in processing_results.items():
                if "transcript" in result and result["transcript"]:
                    combined_content.append(f"Audio {file_id}: {result['transcript']}")
                if "text_content" in result and result["text_content"]:
                    combined_content.append(
                        f"Document {file_id}: {result['text_content']}"
                    )
                if "description" in result and result["description"]:
                    combined_content.append(f"Image {file_id}: {result['description']}")

            content_summary = " ".join(combined_content)[:4000]  # Limit for API

            # Generate comprehensive AI analysis
            analysis_prompt = f"""
            Analyze the following multimodal content in the context of real estate collections business:

            User Request: {prompt}
            Business Context: {json.dumps(business_context) if business_context else "None"}

            Content Analysis:
            {content_summary}

            Provide:
            1. Executive summary of key insights
            2. Business implications and opportunities
            3. Risk assessment and compliance considerations
            4. Recommended actions with priorities
            5. Confidence score (0-1) for the analysis

            Format as JSON with fields: summary, business_implications, risks, recommendations, confidence_score
            """

            ai_query = """
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                %s,
                %s
            ) as ai_insights
            """

            cursor.execute(ai_query)
            result = cursor.fetchone()

            if result and result[0]:
                try:
                    insights = json.loads(result[0])
                    insights["tokens_used"] = (
                        len(analysis_prompt.split()) * 1.3
                    )  # Estimate
                    return insights
                except json.JSONDecodeError:
                    # Fallback if JSON parsing fails
                    return {
                        "summary": result[0],
                        "business_implications": [],
                        "risks": [],
                        "recommendations": [],
                        "confidence_score": 0.7,
                        "tokens_used": len(analysis_prompt.split()) * 1.3,
                    }

            return {
                "summary": "Multimodal content processed successfully",
                "business_implications": [],
                "risks": [],
                "recommendations": [],
                "confidence_score": 0.5,
                "tokens_used": 0,
            }

        finally:
            cursor.close()
            conn.close()

    async def _store_multimodal_results(
        self, processing_results: dict[str, Any], ai_insights: dict[str, Any]
    ):
        """Store multimodal processing results in Snowflake"""

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Store in multimodal processing log table
            insert_query = f"""
            INSERT INTO {self.advanced_database}.{self.processed_schema}.MULTIMODAL_PROCESSING_LOG
            (processing_id, timestamp, files_processed, ai_insights, processing_results)
            VALUES (?, ?, ?, ?, ?)
            """

            processing_id = f"multimodal_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            cursor.execute(
                insert_query,
                (
                    processing_id,
                    datetime.now(),
                    len(processing_results),
                    json.dumps(ai_insights),
                    json.dumps(processing_results),
                ),
            )

            conn.commit()
            logger.info(f"Stored multimodal results with ID: {processing_id}")

        except Exception as e:
            logger.error(f"Error storing multimodal results: {e}")
        finally:
            cursor.close()
            conn.close()

    async def execute_advanced_analytics(
        self, query: AdvancedAnalyticsQuery
    ) -> AnalyticsResponse:
        """Execute advanced analytics using Cortex AI functions"""
        logger.info(f"Executing advanced analytics query: {query.query_type}")

        start_time = datetime.now()

        try:
            if query.query_type == "customer_intelligence":
                results = await self._analyze_customer_intelligence(query)
            elif query.query_type == "sales_optimization":
                results = await self._analyze_sales_optimization(query)
            elif query.query_type == "compliance_monitoring":
                results = await self._analyze_compliance_monitoring(query)
            else:
                raise ValueError(f"Unknown query type: {query.query_type}")

            execution_time = (datetime.now() - start_time).total_seconds()

            return AnalyticsResponse(
                query_type=query.query_type,
                results=results["data"],
                insights=results["insights"],
                recommendations=results["recommendations"],
                confidence_score=results["confidence_score"],
                metadata={
                    "execution_time": execution_time,
                    "parameters": query.parameters,
                },
            )

        except Exception as e:
            logger.error(f"Error executing advanced analytics: {e}")
            raise HTTPException(
                status_code=500, detail=f"Analytics execution failed: {str(e)}"
            )

    async def _analyze_customer_intelligence(
        self, query: AdvancedAnalyticsQuery
    ) -> dict[str, Any]:
        """Analyze customer intelligence with AI insights"""

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Execute customer intelligence query - SECURE VERSION with parameterized query
            intelligence_query = """
            SELECT
                record_id,
                email,
                firstname,
                lastname,
                company,
                overall_sentiment,
                churn_risk,
                ai_customer_insights
            FROM SOPHIA_AI_ADVANCED.STG_TRANSFORMED.CUSTOMER_INTELLIGENCE_ADVANCED
            WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '30 days'
            ORDER BY overall_sentiment ASC
            LIMIT 100
            """

            cursor.execute(intelligence_query)
            results = cursor.fetchall()

            # Generate AI insights - SECURE VERSION with parameterized query
            insights_query = """
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                %s,
                %s
            ) as insights
            """

            insights_prompt = f"Analyze customer intelligence data and provide key insights about customer health, churn risks, and retention opportunities. Data shows {len(results)} customers with varying sentiment scores."

            cursor.execute(insights_query, ("claude-3-5-sonnet", insights_prompt))
            insights_result = cursor.fetchone()

            return {
                "data": {
                    "total_customers": len(results),
                    "high_risk_customers": len(
                        [r for r in results if r[6] == "HIGH_RISK"]
                    ),
                    "avg_sentiment": (
                        sum([r[5] for r in results if r[5]]) / len(results)
                        if results
                        else 0
                    ),
                    "customer_details": results[:10],  # Top 10 for response size
                },
                "insights": [
                    (
                        insights_result[0]
                        if insights_result
                        else "Customer intelligence analysis completed"
                    )
                ],
                "recommendations": [
                    "Focus on high-risk customers for retention campaigns",
                    "Implement proactive outreach for negative sentiment customers",
                    "Analyze successful customer patterns for replication",
                ],
                "confidence_score": 0.85,
            }

        finally:
            cursor.close()
            conn.close()

    async def _analyze_sales_optimization(
        self, query: AdvancedAnalyticsQuery
    ) -> dict[str, Any]:
        """Analyze sales optimization opportunities"""

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Execute sales optimization query - SECURE VERSION with parameterized query
            sales_query = """
            SELECT
                record_id,
                deal_name,
                deal_amount,
                deal_stage,
                deal_sentiment,
                ai_priority_score,
                ai_deal_analysis
            FROM SOPHIA_AI_ADVANCED.STG_TRANSFORMED.SALES_OPPORTUNITY_INTELLIGENCE
            WHERE created_at >= CURRENT_TIMESTAMP - INTERVAL '30 days'
            ORDER BY deal_amount DESC
            LIMIT 50
            """

            cursor.execute(sales_query)
            results = cursor.fetchall()

            # Calculate key metrics
            total_pipeline = sum([r[2] for r in results if r[2]])
            critical_deals = len([r for r in results if r[5] == "CRITICAL"])

            return {
                "data": {
                    "total_opportunities": len(results),
                    "total_pipeline_value": total_pipeline,
                    "critical_deals": critical_deals,
                    "avg_deal_size": total_pipeline / len(results) if results else 0,
                    "top_opportunities": results[:10],
                },
                "insights": [
                    f"Pipeline value: ${total_pipeline:,.2f}",
                    f"Critical deals requiring attention: {critical_deals}",
                    "Focus on high-value opportunities with positive sentiment",
                ],
                "recommendations": [
                    "Prioritize critical deals for immediate action",
                    "Analyze successful deal patterns",
                    "Implement AI-driven sales coaching",
                ],
                "confidence_score": 0.88,
            }

        finally:
            cursor.close()
            conn.close()

    async def _analyze_compliance_monitoring(
        self, query: AdvancedAnalyticsQuery
    ) -> dict[str, Any]:
        """Analyze compliance monitoring status"""

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Execute compliance monitoring query - SECURE VERSION with parameterized query
            compliance_query = """
            SELECT
                compliance_area,
                total_communications,
                high_urgency_items,
                potential_violations,
                avg_sentiment,
                last_updated
            FROM SOPHIA_AI_ADVANCED.AI_ANALYTICS.COMPLIANCE_MONITORING_DASHBOARD
            """

            cursor.execute(compliance_query)
            results = cursor.fetchall()

            # Calculate compliance score
            total_violations = sum([r[3] for r in results if r[3]])
            total_communications = sum([r[1] for r in results if r[1]])
            compliance_score = (
                max(0, 1 - (total_violations / total_communications))
                if total_communications > 0
                else 1
            )

            return {
                "data": {
                    "compliance_areas": len(results),
                    "total_communications": total_communications,
                    "potential_violations": total_violations,
                    "compliance_score": compliance_score,
                    "area_details": results,
                },
                "insights": [
                    f"Overall compliance score: {compliance_score:.2%}",
                    f"Potential violations detected: {total_violations}",
                    "Automated monitoring active across all areas",
                ],
                "recommendations": [
                    "Review flagged communications immediately",
                    "Implement additional training for high-risk areas",
                    "Enhance automated compliance checking",
                ],
                "confidence_score": 0.92,
            }

        finally:
            cursor.close()
            conn.close()

    async def monitor_compliance_realtime(self) -> list[ComplianceAlert]:
        """Real-time compliance monitoring with automated alerts"""
        logger.info("Executing real-time compliance monitoring")

        conn = await self.get_advanced_connection()
        cursor = conn.cursor()

        try:
            # Check for compliance violations in recent communications - SECURE VERSION
            violation_query = """
            SELECT
                message_id,
                user_id,
                message_text,
                ai_message_classification,
                message_sentiment,
                message_timestamp
            FROM SOPHIA_AI_ADVANCED.STG_TRANSFORMED.COMMUNICATION_INTELLIGENCE_REALTIME
            WHERE message_timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 hour'
            AND (
                message_sentiment < %s
                OR ai_message_classification:urgency::STRING = %s
                OR LOWER(message_text) LIKE %s
                OR LOWER(message_text) LIKE %s
                OR LOWER(message_text) LIKE %s
            )
            """

            cursor.execute(
                violation_query,
                (-0.5, "high", "%lawsuit%", "%attorney%", "%legal action%"),
            )
            violations = cursor.fetchall()

            alerts = []
            for violation in violations:
                alert = ComplianceAlert(
                    alert_id=f"compliance_{violation[0]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    severity=self._determine_alert_severity(violation[4], violation[3]),
                    category="fdcpa",
                    description="Potential compliance issue detected in communication",
                    affected_records=[violation[0]],
                    recommended_actions=[
                        "Review communication for compliance violations",
                        "Escalate to compliance team if necessary",
                        "Document review outcome",
                    ],
                    timestamp=violation[5],
                )
                alerts.append(alert)

            return alerts

        finally:
            cursor.close()
            conn.close()

    def _determine_alert_severity(self, sentiment: float, classification: str) -> str:
        """Determine alert severity based on sentiment and classification"""
        if sentiment < -0.8:
            return "critical"
        elif sentiment < -0.5:
            return "high"
        elif classification and "high" in str(classification).lower():
            return "medium"
        else:
            return "low"


# Service instance
enhanced_cortex_service = None


async def get_enhanced_cortex_agent_service() -> EnhancedCortexAgentService:
    """Get enhanced Cortex agent service instance"""
    global enhanced_cortex_service
    if enhanced_cortex_service is None:
        enhanced_cortex_service = EnhancedCortexAgentService()
    return enhanced_cortex_service
