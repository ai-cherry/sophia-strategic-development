"""
Hybrid Search Engine for Sophia AI Memory Ecosystem.

Combines BM25 keyword search with vector similarity search
for improved relevance and performance.
"""

import asyncio
import hashlib
import logging
from typing import Any, Optional

from backend.services.unified_memory_service import get_unified_memory_service
from shared.utils.monitoring import log_execution_time

logger = logging.getLogger(__name__)


class SearchResult:
    """Represents a hybrid search result with multiple scores"""

    def __init__(
        self,
        id: str,
        content: str,
        source: str,
        metadata: dict[str, Any],
        bm25_score: float = 0.0,
        vector_score: float = 0.0,
        fused_score: float = 0.0,
    ):
        self.id = id
        self.content = content
        self.source = source
        self.metadata = metadata
        self.bm25_score = bm25_score
        self.vector_score = vector_score
        self.fused_score = fused_score

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation"""
        return {
            "id": self.id,
            "content": self.content,
            "source": self.source,
            "metadata": self.metadata,
            "scores": {
                "bm25": self.bm25_score,
                "vector": self.vector_score,
                "fused": self.fused_score,
            },
        }


class HybridSearchEngine:
    """
    Combines BM25 keyword search with vector similarity search.

    This engine provides superior search results by leveraging both
    exact keyword matching and semantic understanding.
    """

    def __init__(
        self,
        bm25_weight: float = 0.3,
        vector_weight: float = 0.7,
        personalization_weight: float = 0.0,
    ):
        """
        Initialize the hybrid search engine.

        Args:
            bm25_weight: Weight for BM25 scores (default 0.3)
            vector_weight: Weight for vector similarity scores (default 0.7)
            personalization_weight: Weight for user personalization (default 0.0)
        """
        self.memory_service = get_unified_memory_service()
        self.bm25_weight = bm25_weight
        self.vector_weight = vector_weight
        self.personalization_weight = personalization_weight

        # Validate weights sum to 1.0
        total_weight = bm25_weight + vector_weight + personalization_weight
        if abs(total_weight - 1.0) > 0.01:
            logger.warning(f"Weights don't sum to 1.0 ({total_weight}), normalizing...")
            self.bm25_weight /= total_weight
            self.vector_weight /= total_weight
            self.personalization_weight /= total_weight

        logger.info(
            f"HybridSearchEngine initialized - BM25: {self.bm25_weight}, "
            f"Vector: {self.vector_weight}, Personalization: {self.personalization_weight}"
        )

    @log_execution_time
    async def search(
        self,
        query: str,
        user_id: str = "default",
        limit: int = 10,
        metadata_filter: dict[str, Any] | None = None,
        use_cache: bool = True,
    ) -> list[SearchResult]:
        """
        Perform hybrid search combining BM25 and vector similarity.

        Args:
            query: Search query
            user_id: User identifier for personalization
            limit: Maximum results to return
            metadata_filter: Optional metadata filters
            use_cache: Whether to use cached results

        Returns:
            List of SearchResult objects sorted by relevance
        """
        # Check cache first if enabled
        cache_key = None
        if use_cache:
            cache_key = self._generate_cache_key(query, user_id, metadata_filter)
            cached = await self._get_cached_results(cache_key)
            if cached:
                logger.info(f"Cache hit for hybrid search: {query[:50]}...")
                return cached[:limit]

        # Execute searches in parallel
        logger.info(f"Executing hybrid search for: {query[:50]}...")

        bm25_task = self._bm25_search(query, metadata_filter)
        vector_task = self._vector_search(query, metadata_filter, user_id)

        bm25_results, vector_results = await asyncio.gather(bm25_task, vector_task)

        # Fuse scores
        fused_results = self._fuse_scores(bm25_results, vector_results, user_id)

        # Apply reranking if personalization is enabled
        if self.personalization_weight > 0:
            fused_results = await self._personalize_results(fused_results, user_id)

        # Sort by fused score and limit
        fused_results.sort(key=lambda x: x.fused_score, reverse=True)
        final_results = fused_results[:limit]

        # Cache results
        if use_cache:
            await self._cache_results(cache_key, final_results)

        # Update access patterns for tiering
        await self._update_access_patterns(final_results)

        return final_results

    async def _bm25_search(
        self,
        query: str,
        metadata_filter: dict[str, Any] | None = None,
    ) -> list[dict[str, Any]]:
        """
        Perform BM25 keyword search using modern_stack.

        This uses modern_stack's built-in text search capabilities
        with BM25-like scoring.
        """
        try:
            # Build search query with full-text search
            search_sql = """
            WITH search_terms AS (
                SELECT SPLIT(%s, ' ') as terms
            )
            SELECT
                kb.id,
                kb.content,
                kb.source,
                kb.metadata,
                kb.created_at,
                -- Simple TF-IDF approximation for BM25
                (
                    LENGTH(kb.content) - 
                    LENGTH(REPLACE(LOWER(kb.content), LOWER(%s), ''))
                ) / LENGTH(%s) as relevance_score
            FROM AI_MEMORY.VECTORS.KNOWLEDGE_BASE kb
            WHERE CONTAINS(LOWER(kb.content), LOWER(%s))
            """

            params = [query, query, query, query]

            # Add metadata filters if provided
            if metadata_filter:
                for key, value in metadata_filter.items():
                    search_sql += f" AND kb.metadata:{key} = %s"
                    params.append(value)

            search_sql += " ORDER BY relevance_score DESC LIMIT 100"

            results = await self.memory_service.execute_modern_stack_query(
                search_sql, tuple(params)
            )

            # Normalize scores to 0-1 range
            if results:
                max_score = max(r.get("RELEVANCE_SCORE", 0) for r in results)
                if max_score > 0:
                    for result in results:
                        result["normalized_score"] = (
                            result.get("RELEVANCE_SCORE", 0) / max_score
                        )
                else:
                    for result in results:
                        result["normalized_score"] = 0.0

            return results

        except Exception as e:
            logger.error(f"BM25 search failed: {e}")
            return []

    async def _vector_search(
        self,
        query: str,
        metadata_filter: dict[str, Any] | None = None,
        user_id: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Perform vector similarity search.

        This delegates to the UnifiedMemoryService's vector search
        which uses modern_stack Cortex.
        """
        try:
            results = await self.memory_service.search_knowledge(
                query=query,
                limit=100,  # Get more for better fusion
                metadata_filter=metadata_filter,
                user_id=user_id,
            )

            # Results already have similarity scores normalized to 0-1
            return results

        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            return []

    def _fuse_scores(
        self,
        bm25_results: list[dict[str, Any]],
        vector_results: list[dict[str, Any]],
        user_id: str,
    ) -> list[SearchResult]:
        """
        Fuse BM25 and vector similarity scores.

        This implements a weighted linear combination of scores
        with optional normalization.
        """
        # Create lookup dictionaries
        bm25_dict = {r["ID"]: r for r in bm25_results}

        vector_dict = {r["id"]: r for r in vector_results}

        # Combine all unique document IDs
        all_ids = set(bm25_dict.keys()) | set(vector_dict.keys())

        # Create fused results
        fused_results = []

        for doc_id in all_ids:
            # Get scores from both searches
            bm25_data = bm25_dict.get(doc_id, {})
            vector_data = vector_dict.get(doc_id, {})

            # Use vector data as base (it has better structure)
            if doc_id in vector_dict:
                content = vector_data["content"]
                source = vector_data["source"]
                metadata = vector_data["metadata"]
            else:
                content = bm25_data.get("CONTENT", "")
                source = bm25_data.get("SOURCE", "")
                metadata = bm25_data.get("METADATA", {})

            # Get individual scores
            bm25_score = bm25_data.get("normalized_score", 0.0)
            vector_score = vector_data.get("similarity", 0.0)

            # Calculate fused score
            fused_score = (
                bm25_score * self.bm25_weight + vector_score * self.vector_weight
            )

            # Create result object
            result = SearchResult(
                id=doc_id,
                content=content,
                source=source,
                metadata=metadata,
                bm25_score=bm25_score,
                vector_score=vector_score,
                fused_score=fused_score,
            )

            fused_results.append(result)

        return fused_results

    async def _personalize_results(
        self,
        results: list[SearchResult],
        user_id: str,
    ) -> list[SearchResult]:
        """
        Apply user-specific personalization to results.

        This could include:
        - Boosting recently accessed documents
        - Considering user preferences
        - Click-through rate optimization
        """
        # TODO: Implement personalization based on user history
        # For now, just return results as-is
        return results

    def _generate_cache_key(
        self,
        query: str,
        user_id: str,
        metadata_filter: dict[str, Any] | None,
    ) -> str:
        """Generate a unique cache key for the search"""
        key_parts = [
            "hybrid_search",
            query,
            user_id,
            str(sorted(metadata_filter.items()) if metadata_filter else ""),
        ]

        key_string = ":".join(key_parts)
        return hashlib.sha256(key_string.encode()).hexdigest()[:16]

    async def _get_cached_results(self, cache_key: str) -> Optional[list[SearchResult]]:
        """Get cached search results"""
        # This would use the enhanced Redis caching from Phase 3
        # For now, return None to skip cache
        return None

    async def _cache_results(
        self,
        cache_key: str,
        results: list[SearchResult],
    ) -> None:
        """Cache search results"""
        # This would use the enhanced Redis caching from Phase 3
        # TODO: Implement caching with RedisHelper
        pass

    async def _update_access_patterns(
        self,
        results: list[SearchResult],
    ) -> None:
        """
        Update access patterns for tiering decisions.

        This tracks which documents are accessed frequently
        to inform hot/cold data tiering.
        """
        # TODO: Implement access pattern tracking
        pass

    def update_weights(
        self,
        bm25_weight: float,
        vector_weight: float,
        personalization_weight: float = 0.0,
    ) -> None:
        """
        Update the scoring weights dynamically.

        This allows for A/B testing and optimization.
        """
        total = bm25_weight + vector_weight + personalization_weight

        self.bm25_weight = bm25_weight / total
        self.vector_weight = vector_weight / total
        self.personalization_weight = personalization_weight / total

        logger.info(
            f"Updated weights - BM25: {self.bm25_weight}, "
            f"Vector: {self.vector_weight}, "
            f"Personalization: {self.personalization_weight}"
        )


# Singleton instance
_hybrid_search_engine = None


def get_hybrid_search_engine() -> HybridSearchEngine:
    """Get the singleton HybridSearchEngine instance"""
    global _hybrid_search_engine

    if _hybrid_search_engine is None:
        _hybrid_search_engine = HybridSearchEngine()

    return _hybrid_search_engine
