#!/usr/bin/env python3
"""
Large File Ingestion Service for Sophia AI
Handles large files with intelligent chunking, async processing, and context preservation
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 724 lines

Recommended decomposition:
- large_file_ingestion_service_core.py - Core functionality
- large_file_ingestion_service_utils.py - Utility functions  
- large_file_ingestion_service_models.py - Data models
- large_file_ingestion_service_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import logging
import json
import io
from datetime import datetime
from typing import Dict, List, Optional, Any
from uuid import uuid4
from enum import Enum
import snowflake.connector
from snowflake.connector import DictCursor

# Enhanced imports with fallbacks
try:
    import pandas as pd

    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    logging.warning("pandas not available - CSV processing will be limited")

try:
    from docx import Document as DocxDocument

    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False
    logging.warning("python-docx not available - DOCX processing will be limited")

try:
    import PyPDF2

    HAS_PDF = True
except ImportError:
    HAS_PDF = False
    logging.warning("PyPDF2 not available - PDF processing will be limited")

try:
    import openpyxl

    HAS_EXCEL = True
except ImportError:
    HAS_EXCEL = False
    logging.warning("openpyxl not available - Excel processing will be limited")

try:
    import pptx

    HAS_PPTX = True
except ImportError:
    HAS_PPTX = False
    logging.warning("python-pptx not available - PowerPoint processing will be limited")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class JobStatus(str, Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    CHUNKING = "chunking"
    STORING = "storing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class FileProcessor:
    """Enhanced file processor with context-aware chunking"""

    def __init__(self):
        # Optimized for large context windows
        self.chunk_size = 4000  # Characters per chunk (optimal for LLM context)
        self.chunk_overlap = 200  # Overlap for context preservation
        self.max_file_size = 100 * 1024 * 1024  # 100MB limit

    async def process_file_content(
        self, file_content: bytes, file_type: str, filename: str
    ) -> str:
        """Extract text from various file types with enhanced support"""

        if file_type in ["text/plain", "text/markdown"]:
            return file_content.decode("utf-8", errors="ignore")

        elif file_type == "text/csv" and HAS_PANDAS:
            return await self._process_csv(file_content, filename)

        elif file_type == "application/json":
            return await self._process_json(file_content, filename)

        elif file_type == "application/pdf" and HAS_PDF:
            return await self._process_pdf(file_content, filename)

        elif (
            file_type
            == "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            and HAS_DOCX
        ):
            return await self._process_docx(file_content, filename)

        elif (
            file_type
            in [
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                "application/vnd.ms-excel",
            ]
            and HAS_EXCEL
        ):
            return await self._process_excel(file_content, filename)

        elif (
            file_type
            in [
                "application/vnd.openxmlformats-officedocument.presentationml.presentation",
                "application/vnd.ms-powerpoint",
            ]
            and HAS_PPTX
        ):
            return await self._process_powerpoint(file_content, filename)

        else:
            # Fallback to text extraction
            return file_content.decode("utf-8", errors="ignore")

    async def _process_csv(self, file_content: bytes, filename: str) -> str:
        """Process CSV with comprehensive data extraction"""
        try:
            df = pd.read_csv(io.BytesIO(file_content))

            text = f"# CSV Dataset: {filename}\n\n"
            text += "**Dataset Overview:**\n"
            text += f"- Total Records: {len(df):,}\n"
            text += f"- Columns: {len(df.columns)}\n"
            text += f"- Column Names: {', '.join(df.columns)}\n\n"

            # Add data type information
            text += "**Column Details:**\n"
            for col in df.columns:
                dtype = str(df[col].dtype)
                non_null = df[col].count()
                text += f"- {col}: {dtype} ({non_null:,} non-null values)\n"
            text += "\n"

            # Include all data rows for full context
            text += "**Complete Dataset:**\n"
            for index, row in df.iterrows():
                row_data = []
                for col, val in row.items():
                    if pd.notna(val):
                        row_data.append(f"{col}: {str(val)}")
                    else:
                        row_data.append(f"{col}: [blank]")
                text += f"Record {index + 1}: {' | '.join(row_data)}\n"

            return text

        except Exception as e:
            logger.error(f"CSV processing failed: {e}")
            return f"CSV File: {filename}\n(Processing failed: {str(e)})"

    async def _process_json(self, file_content: bytes, filename: str) -> str:
        """Process JSON with structured formatting"""
        try:
            data = json.loads(file_content.decode("utf-8"))
            text = f"# JSON Data: {filename}\n\n"

            # Add metadata
            if isinstance(data, dict):
                text += f"**Structure:** Object with {len(data)} keys\n"
                text += f"**Keys:** {', '.join(data.keys())}\n\n"
            elif isinstance(data, list):
                text += f"**Structure:** Array with {len(data)} items\n\n"

            text += "**Complete JSON Content:**\n"
            text += json.dumps(data, indent=2, ensure_ascii=False)

            return text

        except Exception as e:
            logger.error(f"JSON processing failed: {e}")
            return f"JSON File: {filename}\n(Processing failed: {str(e)})"

    async def _process_pdf(self, file_content: bytes, filename: str) -> str:
        """Process PDF with page-by-page extraction"""
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
            text = f"# PDF Document: {filename}\n\n"
            text += "**Document Info:**\n"
            text += f"- Total Pages: {len(pdf_reader.pages)}\n"

            # Extract metadata if available
            if pdf_reader.metadata:
                if pdf_reader.metadata.title:
                    text += f"- Title: {pdf_reader.metadata.title}\n"
                if pdf_reader.metadata.author:
                    text += f"- Author: {pdf_reader.metadata.author}\n"
            text += "\n"

            # Extract text from all pages
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                if page_text.strip():
                    text += f"**Page {page_num + 1}:**\n{page_text.strip()}\n\n"

            return text

        except Exception as e:
            logger.error(f"PDF processing failed: {e}")
            return f"PDF Document: {filename}\n(Text extraction failed: {str(e)})"

    async def _process_docx(self, file_content: bytes, filename: str) -> str:
        """Process Word document with paragraph structure"""
        try:
            doc = DocxDocument(io.BytesIO(file_content))
            text = f"# Word Document: {filename}\n\n"

            # Add document properties
            core_props = doc.core_properties
            if core_props.title:
                text += f"**Title:** {core_props.title}\n"
            if core_props.author:
                text += f"**Author:** {core_props.author}\n"
            if core_props.created:
                text += f"**Created:** {core_props.created}\n"
            text += f"**Paragraphs:** {len(doc.paragraphs)}\n\n"

            # Extract all paragraphs
            for i, paragraph in enumerate(doc.paragraphs):
                if paragraph.text.strip():
                    text += f"{paragraph.text.strip()}\n\n"

            return text

        except Exception as e:
            logger.error(f"DOCX processing failed: {e}")
            return f"Word Document: {filename}\n(Text extraction failed: {str(e)})"

    async def _process_excel(self, file_content: bytes, filename: str) -> str:
        """Process Excel with all sheets"""
        try:
            if HAS_PANDAS:
                # Use pandas for comprehensive Excel processing
                all_sheets = pd.read_excel(io.BytesIO(file_content), sheet_name=None)
                text = f"# Excel Workbook: {filename}\n\n"
                text += "**Workbook Info:**\n"
                text += f"- Total Sheets: {len(all_sheets)}\n"
                text += f"- Sheet Names: {', '.join(all_sheets.keys())}\n\n"

                for sheet_name, df in all_sheets.items():
                    text += f"## Sheet: {sheet_name}\n"
                    text += f"- Rows: {len(df)}, Columns: {len(df.columns)}\n"
                    text += f"- Column Names: {', '.join(df.columns)}\n\n"

                    # Include all data
                    text += "**Complete Data:**\n"
                    text += df.to_string(index=False)
                    text += "\n\n"

                return text
            else:
                return f"Excel Workbook: {filename}\n(Excel processing not available)"

        except Exception as e:
            logger.error(f"Excel processing failed: {e}")
            return f"Excel Workbook: {filename}\n(Processing failed: {str(e)})"

    async def _process_powerpoint(self, file_content: bytes, filename: str) -> str:
        """Process PowerPoint with slide content"""
        try:
            presentation = pptx.Presentation(io.BytesIO(file_content))
            text = f"# PowerPoint Presentation: {filename}\n\n"
            text += "**Presentation Info:**\n"
            text += f"- Total Slides: {len(presentation.slides)}\n\n"

            for slide_num, slide in enumerate(presentation.slides):
                text += f"## Slide {slide_num + 1}\n"

                slide_content = []
                for shape in slide.shapes:
                    if hasattr(shape, "text") and shape.text.strip():
                        slide_content.append(shape.text.strip())

                if slide_content:
                    text += "\n".join(slide_content)
                else:
                    text += "[No text content]"
                text += "\n\n"

            return text

        except Exception as e:
            logger.error(f"PowerPoint processing failed: {e}")
            return f"PowerPoint Presentation: {filename}\n(Processing failed: {str(e)})"

    async def create_intelligent_chunks(
        self, text_content: str, filename: str, job_id: str
    ) -> List[Dict[str, Any]]:
        """Create intelligent chunks with context preservation"""

        # Split into sentences for better chunking
        sentences = self._split_sentences(text_content)
        chunks = []

        current_chunk = ""
        chunk_metadata = {
            "filename": filename,
            "job_id": job_id,
            "total_length": len(text_content),
            "processing_timestamp": datetime.now().isoformat(),
        }

        chunk_sequence = 0

        for sentence in sentences:
            # Check if adding sentence exceeds chunk size
            potential_chunk = (
                current_chunk + " " + sentence if current_chunk else sentence
            )

            if len(potential_chunk) > self.chunk_size and current_chunk:
                # Create chunk with context preservation
                chunk_data = {
                    "chunk_id": str(uuid4()),
                    "sequence": chunk_sequence,
                    "content": current_chunk.strip(),
                    "word_count": len(current_chunk.split()),
                    "char_count": len(current_chunk),
                    "metadata": {**chunk_metadata, "sequence": chunk_sequence},
                }
                chunks.append(chunk_data)

                # Start new chunk with overlap for context
                overlap_text = self._extract_overlap_context(current_chunk)
                current_chunk = overlap_text + " " + sentence
                chunk_sequence += 1
            else:
                current_chunk = potential_chunk

        # Add final chunk
        if current_chunk.strip():
            chunk_data = {
                "chunk_id": str(uuid4()),
                "sequence": chunk_sequence,
                "content": current_chunk.strip(),
                "word_count": len(current_chunk.split()),
                "char_count": len(current_chunk),
                "metadata": {**chunk_metadata, "sequence": chunk_sequence},
            }
            chunks.append(chunk_data)

        logger.info(f"Created {len(chunks)} intelligent chunks from {filename}")
        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences"""
        import re

        # Enhanced sentence splitting
        sentences = re.split(r"(?<=[.!?])\s+(?=[A-Z])", text)
        return [s.strip() for s in sentences if s.strip()]

    def _extract_overlap_context(self, text: str) -> str:
        """Extract context for chunk overlap"""
        if len(text) <= self.chunk_overlap:
            return text

        # Find good break point near the overlap boundary
        overlap_start = len(text) - self.chunk_overlap

        # Try to break at sentence or word boundary
        overlap_text = text[overlap_start:]
        sentence_break = overlap_text.find(". ")
        if sentence_break != -1:
            return overlap_text[sentence_break + 2 :]

        word_break = overlap_text.find(" ")
        if word_break != -1:
            return overlap_text[word_break + 1 :]

        return overlap_text


class LargeFileIngestionService:
    """Service for processing large files with async job management"""

    def __init__(self, snowflake_config: Dict[str, str]):
        self.snowflake_config = snowflake_config
        self.connection = None
        self.processor = FileProcessor()
        self.active_jobs: Dict[str, Dict] = {}

    async def connect(self):
        """Connect to Snowflake"""
        try:
            self.connection = snowflake.connector.connect(**self.snowflake_config)
            await self._ensure_tables_exist()
            logger.info("✅ Large File Ingestion Service connected")
        except Exception as e:
            logger.error(f"❌ Connection failed: {e}")
            raise

    async def _ensure_tables_exist(self):
        """Ensure required tables exist"""
        cursor = self.connection.cursor()

        # Ingestion jobs table
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS INGESTION_JOBS (
            JOB_ID VARCHAR(255) PRIMARY KEY,
            FILENAME VARCHAR(500) NOT NULL,
            FILE_TYPE VARCHAR(100) NOT NULL,
            FILE_SIZE INTEGER NOT NULL,
            STATUS VARCHAR(50) NOT NULL,
            PROGRESS FLOAT DEFAULT 0.0,
            CHUNKS_TOTAL INTEGER DEFAULT 0,
            CHUNKS_PROCESSED INTEGER DEFAULT 0,
            ENTRIES_CREATED INTEGER DEFAULT 0,
            ERROR_MESSAGE TEXT,
            CREATED_AT TIMESTAMP_NTZ NOT NULL,
            UPDATED_AT TIMESTAMP_NTZ NOT NULL,
            ESTIMATED_COMPLETION TIMESTAMP_NTZ
        )
        """)

        cursor.close()

    async def create_ingestion_job(
        self, filename: str, file_content: bytes, file_type: str
    ) -> str:
        """Create new ingestion job"""

        job_id = str(uuid4())
        file_size = len(file_content)

        # Validate file size
        if file_size > self.processor.max_file_size:
            raise ValueError(
                f"File too large: {file_size} bytes (max: {self.processor.max_file_size})"
            )

        # Store job info
        job_data = {
            "job_id": job_id,
            "filename": filename,
            "file_content": file_content,
            "file_type": file_type,
            "file_size": file_size,
            "status": JobStatus.PENDING,
            "created_at": datetime.now(),
        }

        self.active_jobs[job_id] = job_data

        # Save to database
        await self._save_job_status(job_id, JobStatus.PENDING, 0, 0, 0, 0)

        logger.info(
            f"Created ingestion job {job_id} for {filename} ({file_size:,} bytes)"
        )
        return job_id

    async def process_job_async(self, job_id: str):
        """Process job asynchronously"""
        try:
            job_data = self.active_jobs.get(job_id)
            if not job_data:
                logger.error(f"Job {job_id} not found")
                return

            # Update status
            await self._save_job_status(job_id, JobStatus.PROCESSING, 0, 0, 0, 0)

            # Extract text content
            logger.info(f"Extracting text from {job_data['filename']}")
            text_content = await self.processor.process_file_content(
                job_data["file_content"], job_data["file_type"], job_data["filename"]
            )

            # Update status
            await self._save_job_status(job_id, JobStatus.CHUNKING, 10, 0, 0, 0)

            # Create intelligent chunks
            logger.info(f"Creating intelligent chunks for {job_data['filename']}")
            chunks = await self.processor.create_intelligent_chunks(
                text_content, job_data["filename"], job_id
            )

            total_chunks = len(chunks)
            await self._save_job_status(
                job_id, JobStatus.STORING, 20, total_chunks, 0, 0
            )

            # Process chunks into knowledge entries
            entries_created = 0
            for i, chunk in enumerate(chunks):
                await self._create_knowledge_entry(chunk, job_data["filename"])
                entries_created += 1

                progress = 20 + (
                    70 * (i + 1) / total_chunks
                )  # 20-90% for chunk processing
                await self._save_job_status(
                    job_id,
                    JobStatus.STORING,
                    progress,
                    total_chunks,
                    i + 1,
                    entries_created,
                )

                # Small delay to prevent overwhelming
                await asyncio.sleep(0.05)

            # Complete job
            await self._save_job_status(
                job_id,
                JobStatus.COMPLETED,
                100,
                total_chunks,
                total_chunks,
                entries_created,
            )
            logger.info(
                f"Completed job {job_id}: {entries_created} entries created from {total_chunks} chunks"
            )

            # Clean up
            if job_id in self.active_jobs:
                del self.active_jobs[job_id]

        except Exception as e:
            logger.error(f"Job {job_id} failed: {e}")
            await self._save_job_status(job_id, JobStatus.FAILED, 0, 0, 0, 0, str(e))
            if job_id in self.active_jobs:
                del self.active_jobs[job_id]

    async def _create_knowledge_entry(
        self, chunk: Dict[str, Any], filename: str
    ) -> str:
        """Create knowledge entry from chunk"""
        entry_id = str(uuid4())

        # Create title with context
        title = f"{filename} - Part {chunk['sequence'] + 1}"
        if chunk["sequence"] == 0:
            title = f"{filename} - Overview"

        # Enhanced content with metadata
        content = f"**Source:** {filename}\n"
        content += f"**Part:** {chunk['sequence'] + 1}\n"
        content += f"**Word Count:** {chunk['word_count']:,}\n\n"
        content += chunk["content"]

        # Auto-categorize
        category_id = self._categorize_content(filename, chunk["content"])

        # Ensure category exists
        await self._ensure_category_exists(category_id)

        # Insert entry
        cursor = self.connection.cursor()
        cursor.execute(
            """
        INSERT INTO KNOWLEDGE_BASE_ENTRIES 
        (ENTRY_ID, TITLE, CONTENT, CATEGORY_ID, STATUS, METADATA, CREATED_AT, UPDATED_AT)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                entry_id,
                title,
                content,
                category_id,
                "published",
                json.dumps(chunk["metadata"]),
                datetime.now(),
                datetime.now(),
            ),
        )
        cursor.close()

        return entry_id

    def _categorize_content(self, filename: str, content: str) -> str:
        """Auto-categorize content"""
        filename_lower = filename.lower()
        content_lower = content.lower()

        # Enhanced categorization logic
        categories = {
            "customers": [
                "customer",
                "client",
                "contact",
                "company",
                "tenant",
                "account",
            ],
            "products": [
                "product",
                "service",
                "feature",
                "solution",
                "offering",
                "tool",
            ],
            "employees": ["employee", "staff", "team", "personnel", "hr", "person"],
            "financial": [
                "financial",
                "revenue",
                "budget",
                "accounting",
                "payment",
                "invoice",
            ],
            "processes": [
                "process",
                "procedure",
                "workflow",
                "manual",
                "instruction",
                "guide",
            ],
            "contracts": ["contract", "agreement", "terms", "legal", "policy"],
            "reports": ["report", "analysis", "summary", "overview", "dashboard"],
        }

        for category, keywords in categories.items():
            if any(
                keyword in filename_lower or keyword in content_lower
                for keyword in keywords
            ):
                return category

        return "general"

    async def _ensure_category_exists(self, category_id: str):
        """Ensure category exists"""
        cursor = self.connection.cursor()
        cursor.execute(
            "SELECT CATEGORY_ID FROM KNOWLEDGE_CATEGORIES WHERE CATEGORY_ID = ?",
            (category_id,),
        )

        if not cursor.fetchone():
            category_names = {
                "customers": "Customer Information",
                "products": "Products & Services",
                "employees": "Employee Directory",
                "financial": "Financial Information",
                "processes": "Business Processes",
                "contracts": "Contracts & Legal",
                "reports": "Reports & Analytics",
                "general": "General Knowledge",
            }

            name = category_names.get(category_id, category_id.title())
            cursor.execute(
                """
            INSERT INTO KNOWLEDGE_CATEGORIES (CATEGORY_ID, CATEGORY_NAME, DESCRIPTION, CREATED_AT)
            VALUES (?, ?, ?, ?)
            """,
                (category_id, name, f"Auto-created for {category_id}", datetime.now()),
            )

        cursor.close()

    async def _save_job_status(
        self,
        job_id: str,
        status: JobStatus,
        progress: float,
        total_chunks: int,
        processed_chunks: int,
        entries_created: int,
        error_message: str = None,
    ):
        """Save job status to database"""
        cursor = self.connection.cursor()
        cursor.execute(
            """
        INSERT OR REPLACE INTO INGESTION_JOBS 
        (JOB_ID, FILENAME, FILE_TYPE, FILE_SIZE, STATUS, PROGRESS, 
         CHUNKS_TOTAL, CHUNKS_PROCESSED, ENTRIES_CREATED, ERROR_MESSAGE, 
         CREATED_AT, UPDATED_AT, ESTIMATED_COMPLETION)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                job_id,
                self.active_jobs.get(job_id, {}).get("filename", ""),
                self.active_jobs.get(job_id, {}).get("file_type", ""),
                self.active_jobs.get(job_id, {}).get("file_size", 0),
                status.value,
                progress,
                total_chunks,
                processed_chunks,
                entries_created,
                error_message,
                self.active_jobs.get(job_id, {}).get("created_at", datetime.now()),
                datetime.now(),
                None,
            ),
        )
        cursor.close()

    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Get job status"""
        cursor = self.connection.cursor(DictCursor)
        cursor.execute("SELECT * FROM INGESTION_JOBS WHERE JOB_ID = ?", (job_id,))
        result = cursor.fetchone()
        cursor.close()
        return result

    async def cancel_job(self, job_id: str) -> bool:
        """Cancel job"""
        if job_id in self.active_jobs:
            await self._save_job_status(job_id, JobStatus.CANCELLED, 0, 0, 0, 0)
            del self.active_jobs[job_id]
            return True
        return False
