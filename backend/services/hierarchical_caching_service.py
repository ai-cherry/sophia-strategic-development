#!/usr/bin/env python3
"""
Hierarchical Caching Service
HIGH PRIORITY: Three-tier caching strategy for improved performance and cost savings
Addresses: Performance optimization and cost reduction requirements
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 880 lines

Recommended decomposition:
- hierarchical_caching_service_core.py - Core functionality
- hierarchical_caching_service_utils.py - Utility functions
- hierarchical_caching_service_models.py - Data models
- hierarchical_caching_service_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import gzip
import json
import logging
import pickle
from abc import ABC, abstractmethod
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any

import redis.asyncio as redis

logger = logging.getLogger(__name__)


class CacheTier(str, Enum):
    HOT = "hot"  # In-memory, ultra-fast access (<1ms)
    WARM = "warm"  # Redis, fast access (<5ms)
    COLD = "cold"  # Compressed storage, slower access (<50ms)


class CacheType(str, Enum):
    QUERY_RESULT = "query_result"
    EMBEDDING = "embedding"
    ANALYTICS = "analytics"
    USER_SESSION = "user_session"
    KNOWLEDGE_ENTRY = "knowledge_entry"
    SEARCH_RESULT = "search_result"


@dataclass
class CacheEntry:
    """Cache entry with metadata"""

    key: str
    data: Any
    tier: CacheTier
    cache_type: CacheType
    size_bytes: int
    ttl_seconds: int
    created_at: datetime
    expires_at: datetime
    access_count: int = 0
    last_accessed: datetime = None
    hit_count: int = 0
    miss_count: int = 0
    compression_ratio: float = 1.0


@dataclass
class CacheStats:
    """Cache performance statistics"""

    total_entries: int
    total_size_bytes: int
    hit_rate: float
    miss_rate: float
    average_response_time_ms: float
    tier_distribution: dict[str, int]
    eviction_count: int
    memory_usage_mb: float


class CacheTierInterface(ABC):
    """Abstract interface for cache tiers"""

    @abstractmethod
    async def get(self, key: str) -> CacheEntry | None:
        pass

    @abstractmethod
    async def set(self, entry: CacheEntry) -> bool:
        pass

    @abstractmethod
    async def delete(self, key: str) -> bool:
        pass

    @abstractmethod
    async def exists(self, key: str) -> bool:
        pass

    @abstractmethod
    async def get_stats(self) -> dict[str, Any]:
        pass


class HotCache(CacheTierInterface):
    """Hot tier - In-memory cache for ultra-fast access"""

    def __init__(self, max_size_mb: int = 100):
        self.cache: dict[str, CacheEntry] = {}
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.current_size_bytes = 0
        self.access_order: list[str] = []  # For LRU eviction

        self.stats = {"hits": 0, "misses": 0, "evictions": 0, "total_requests": 0}

    async def get(self, key: str) -> CacheEntry | None:
        """Get entry from hot cache"""
        start_time = datetime.now()

        if key in self.cache:
            entry = self.cache[key]

            # Check expiration
            if entry.expires_at <= datetime.now():
                await self.delete(key)
                self.stats["misses"] += 1
                return None

            # Update access info
            entry.access_count += 1
            entry.last_accessed = datetime.now()
            entry.hit_count += 1

            # Update LRU order
            if key in self.access_order:
                self.access_order.remove(key)
            self.access_order.append(key)

            self.stats["hits"] += 1
            self.stats["total_requests"] += 1

            response_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.debug(f"🔥 HOT cache HIT: {key} ({response_time:.2f}ms)")

            return entry

        self.stats["misses"] += 1
        self.stats["total_requests"] += 1
        return None

    async def set(self, entry: CacheEntry) -> bool:
        """Set entry in hot cache with LRU eviction"""
        try:
            # Check if we need to evict entries
            while (
                self.current_size_bytes + entry.size_bytes > self.max_size_bytes
                and self.access_order
            ):
                await self._evict_lru()

            # Add new entry
            self.cache[entry.key] = entry
            self.current_size_bytes += entry.size_bytes

            # Update access order
            if entry.key in self.access_order:
                self.access_order.remove(entry.key)
            self.access_order.append(entry.key)

            logger.debug(f"🔥 HOT cache SET: {entry.key} ({entry.size_bytes} bytes)")
            return True

        except Exception as e:
            logger.error(f"Error setting hot cache entry: {e}")
            return False

    async def _evict_lru(self):
        """Evict least recently used entry"""
        if not self.access_order:
            return

        lru_key = self.access_order.pop(0)
        if lru_key in self.cache:
            entry = self.cache[lru_key]
            self.current_size_bytes -= entry.size_bytes
            del self.cache[lru_key]
            self.stats["evictions"] += 1
            logger.debug(f"🔥 HOT cache EVICTED: {lru_key}")

    async def delete(self, key: str) -> bool:
        """Delete entry from hot cache"""
        if key in self.cache:
            entry = self.cache[key]
            self.current_size_bytes -= entry.size_bytes
            del self.cache[key]

            if key in self.access_order:
                self.access_order.remove(key)

            return True
        return False

    async def exists(self, key: str) -> bool:
        """Check if key exists in hot cache"""
        return key in self.cache and self.cache[key].expires_at > datetime.now()

    async def get_stats(self) -> dict[str, Any]:
        """Get hot cache statistics"""
        hit_rate = (self.stats["hits"] / max(self.stats["total_requests"], 1)) * 100

        return {
            "tier": "hot",
            "entries": len(self.cache),
            "size_mb": self.current_size_bytes / (1024 * 1024),
            "max_size_mb": self.max_size_bytes / (1024 * 1024),
            "utilization_percent": (self.current_size_bytes / self.max_size_bytes)
            * 100,
            "hit_rate_percent": hit_rate,
            "stats": self.stats,
        }


class WarmCache(CacheTierInterface):
    """Warm tier - Redis cache for fast access"""

    def __init__(self, redis_url: str = "redis://localhost:6379", db: int = 0):
        self.redis_url = redis_url
        self.db = db
        self.redis_client = None
        self.key_prefix = "sophia:warm:"

        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "total_requests": 0,
        }

    async def initialize(self):
        """Initialize Redis connection"""
        try:
            self.redis_client = redis.from_url(self.redis_url, db=self.db)
            await self.redis_client.ping()
            logger.info("✅ Warm cache (Redis) connected")
        except Exception as e:
            logger.error(f"❌ Failed to connect to Redis: {e}")
            raise

    async def get(self, key: str) -> CacheEntry | None:
        """Get entry from warm cache"""
        if not self.redis_client:
            return None

        start_time = datetime.now()
        redis_key = f"{self.key_prefix}{key}"

        try:
            data = await self.redis_client.get(redis_key)

            if data:
                # Deserialize cache entry
                entry_dict = json.loads(data)
                entry = CacheEntry(**entry_dict)

                # Check expiration
                if entry.expires_at <= datetime.now():
                    await self.delete(key)
                    self.stats["misses"] += 1
                    return None

                # Update access info
                entry.access_count += 1
                entry.last_accessed = datetime.now()
                entry.hit_count += 1

                # Update in Redis
                await self._update_entry_metadata(redis_key, entry)

                self.stats["hits"] += 1
                self.stats["total_requests"] += 1

                response_time = (datetime.now() - start_time).total_seconds() * 1000
                logger.debug(f"🔶 WARM cache HIT: {key} ({response_time:.2f}ms)")

                return entry

            self.stats["misses"] += 1
            self.stats["total_requests"] += 1
            return None

        except Exception as e:
            logger.error(f"Error getting warm cache entry: {e}")
            self.stats["misses"] += 1
            return None

    async def set(self, entry: CacheEntry) -> bool:
        """Set entry in warm cache"""
        if not self.redis_client:
            return False

        try:
            redis_key = f"{self.key_prefix}{entry.key}"

            # Serialize entry (excluding data for metadata storage)
            entry_dict = asdict(entry)
            entry_dict["created_at"] = entry.created_at.isoformat()
            entry_dict["expires_at"] = entry.expires_at.isoformat()
            if entry.last_accessed:
                entry_dict["last_accessed"] = entry.last_accessed.isoformat()

            # Store with TTL
            await self.redis_client.setex(
                redis_key, entry.ttl_seconds, json.dumps(entry_dict)
            )

            self.stats["sets"] += 1
            logger.debug(f"🔶 WARM cache SET: {entry.key}")
            return True

        except Exception as e:
            logger.error(f"Error setting warm cache entry: {e}")
            return False

    async def _update_entry_metadata(self, redis_key: str, entry: CacheEntry):
        """Update entry metadata in Redis"""
        try:
            entry_dict = asdict(entry)
            entry_dict["created_at"] = entry.created_at.isoformat()
            entry_dict["expires_at"] = entry.expires_at.isoformat()
            if entry.last_accessed:
                entry_dict["last_accessed"] = entry.last_accessed.isoformat()

            await self.redis_client.set(redis_key, json.dumps(entry_dict))
        except Exception as e:
            logger.error(f"Error updating entry metadata: {e}")

    async def delete(self, key: str) -> bool:
        """Delete entry from warm cache"""
        if not self.redis_client:
            return False

        try:
            redis_key = f"{self.key_prefix}{key}"
            result = await self.redis_client.delete(redis_key)

            if result:
                self.stats["deletes"] += 1
                return True
            return False

        except Exception as e:
            logger.error(f"Error deleting warm cache entry: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """Check if key exists in warm cache"""
        if not self.redis_client:
            return False

        try:
            redis_key = f"{self.key_prefix}{key}"
            return await self.redis_client.exists(redis_key)
        except Exception as e:
            logger.error(f"Error checking warm cache existence: {e}")
            return False

    async def get_stats(self) -> dict[str, Any]:
        """Get warm cache statistics"""
        hit_rate = (self.stats["hits"] / max(self.stats["total_requests"], 1)) * 100

        redis_stats = {}
        if self.redis_client:
            try:
                info = await self.redis_client.info()
                redis_stats = {
                    "used_memory_mb": info.get("used_memory", 0) / (1024 * 1024),
                    "connected_clients": info.get("connected_clients", 0),
                    "keyspace_hits": info.get("keyspace_hits", 0),
                    "keyspace_misses": info.get("keyspace_misses", 0),
                }
            except Exception as e:
                logger.warning(f"Could not get Redis stats: {e}")

        return {
            "tier": "warm",
            "hit_rate_percent": hit_rate,
            "stats": self.stats,
            "redis_stats": redis_stats,
        }


class ColdCache(CacheTierInterface):
    """Cold tier - Compressed file storage for large data"""

    def __init__(self, storage_path: str = "/tmp/sophia_cold_cache"):
        self.storage_path = storage_path
        self.index: dict[str, dict[str, Any]] = {}  # In-memory index

        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "compressions": 0,
            "decompressions": 0,
            "total_requests": 0,
        }

        # Create storage directory
        import os

        os.makedirs(storage_path, exist_ok=True)

    async def get(self, key: str) -> CacheEntry | None:
        """Get entry from cold cache"""
        start_time = datetime.now()

        if key not in self.index:
            self.stats["misses"] += 1
            self.stats["total_requests"] += 1
            return None

        try:
            entry_info = self.index[key]

            # Check expiration
            expires_at = datetime.fromisoformat(entry_info["expires_at"])
            if expires_at <= datetime.now():
                await self.delete(key)
                self.stats["misses"] += 1
                return None

            # Load and decompress data
            file_path = f"{self.storage_path}/{key}.gz"

            with open(file_path, "rb") as f:
                compressed_data = f.read()

            # Decompress data
            decompressed_data = gzip.decompress(compressed_data)
            data = pickle.loads(decompressed_data)

            # Create cache entry
            entry = CacheEntry(
                key=key,
                data=data,
                tier=CacheTier.COLD,
                cache_type=CacheType(entry_info["cache_type"]),
                size_bytes=entry_info["size_bytes"],
                ttl_seconds=entry_info["ttl_seconds"],
                created_at=datetime.fromisoformat(entry_info["created_at"]),
                expires_at=expires_at,
                access_count=entry_info.get("access_count", 0) + 1,
                hit_count=entry_info.get("hit_count", 0) + 1,
                compression_ratio=entry_info.get("compression_ratio", 1.0),
            )

            entry.last_accessed = datetime.now()

            # Update index
            self.index[key].update(
                {
                    "access_count": entry.access_count,
                    "hit_count": entry.hit_count,
                    "last_accessed": entry.last_accessed.isoformat(),
                }
            )

            self.stats["hits"] += 1
            self.stats["decompressions"] += 1
            self.stats["total_requests"] += 1

            response_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.debug(f"🔵 COLD cache HIT: {key} ({response_time:.2f}ms)")

            return entry

        except Exception as e:
            logger.error(f"Error getting cold cache entry: {e}")
            self.stats["misses"] += 1
            return None

    async def set(self, entry: CacheEntry) -> bool:
        """Set entry in cold cache with compression"""
        try:
            # Serialize and compress data
            serialized_data = pickle.dumps(entry.data)
            compressed_data = gzip.compress(serialized_data)

            compression_ratio = len(serialized_data) / len(compressed_data)

            # Save to file
            file_path = f"{self.storage_path}/{entry.key}.gz"
            with open(file_path, "wb") as f:
                f.write(compressed_data)

            # Update index
            self.index[entry.key] = {
                "cache_type": entry.cache_type.value,
                "size_bytes": len(compressed_data),
                "ttl_seconds": entry.ttl_seconds,
                "created_at": entry.created_at.isoformat(),
                "expires_at": entry.expires_at.isoformat(),
                "access_count": entry.access_count,
                "hit_count": entry.hit_count,
                "compression_ratio": compression_ratio,
                "file_path": file_path,
            }

            self.stats["sets"] += 1
            self.stats["compressions"] += 1

            logger.debug(
                f"🔵 COLD cache SET: {entry.key} (compression: {compression_ratio:.2f}x)"
            )
            return True

        except Exception as e:
            logger.error(f"Error setting cold cache entry: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """Delete entry from cold cache"""
        try:
            if key in self.index:
                # Delete file
                file_path = self.index[key]["file_path"]
                import os

                if os.path.exists(file_path):
                    os.remove(file_path)

                # Remove from index
                del self.index[key]
                self.stats["deletes"] += 1
                return True
            return False

        except Exception as e:
            logger.error(f"Error deleting cold cache entry: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """Check if key exists in cold cache"""
        if key not in self.index:
            return False

        # Check expiration
        expires_at = datetime.fromisoformat(self.index[key]["expires_at"])
        return expires_at > datetime.now()

    async def get_stats(self) -> dict[str, Any]:
        """Get cold cache statistics"""
        hit_rate = (self.stats["hits"] / max(self.stats["total_requests"], 1)) * 100

        total_size_bytes = sum(info["size_bytes"] for info in self.index.values())
        avg_compression = sum(
            info.get("compression_ratio", 1.0) for info in self.index.values()
        )
        avg_compression = avg_compression / max(len(self.index), 1)

        return {
            "tier": "cold",
            "entries": len(self.index),
            "total_size_mb": total_size_bytes / (1024 * 1024),
            "hit_rate_percent": hit_rate,
            "average_compression_ratio": avg_compression,
            "stats": self.stats,
        }


class HierarchicalCachingService:
    """
    Hierarchical Caching Service with three-tier architecture
    Provides sub-millisecond access to hot data with intelligent tiering
    """

    def __init__(
        self,
        hot_cache_size_mb: int = 100,
        redis_url: str = "redis://localhost:6379",
        cold_storage_path: str = "/tmp/sophia_cold_cache",
    ):
        # Initialize cache tiers
        self.hot_cache = HotCache(hot_cache_size_mb)
        self.warm_cache = WarmCache(redis_url)
        self.cold_cache = ColdCache(cold_storage_path)

        # Cache management
        self.promotion_threshold = 3  # Promote after 3 accesses
        self.demotion_threshold = timedelta(
            hours=1
        )  # Demote after 1 hour of inactivity

        # Performance monitoring
        self.global_stats = {
            "total_requests": 0,
            "total_hits": 0,
            "total_misses": 0,
            "tier_hits": {"hot": 0, "warm": 0, "cold": 0},
            "promotion_count": 0,
            "demotion_count": 0,
        }

        # Background tasks
        self._maintenance_task = None

        logger.info("✅ Hierarchical Caching Service initialized")

    async def initialize(self):
        """Initialize all cache tiers"""
        try:
            await self.warm_cache.initialize()

            # Start background maintenance
            self._maintenance_task = asyncio.create_task(self._background_maintenance())

            logger.info("✅ Hierarchical caching fully initialized")
        except Exception as e:
            logger.error(f"❌ Failed to initialize caching service: {e}")
            raise

    async def get(self, key: str) -> Any | None:
        """Get data from hierarchical cache with intelligent promotion"""
        start_time = datetime.now()
        self.global_stats["total_requests"] += 1

        # Try hot cache first
        entry = await self.hot_cache.get(key)
        if entry:
            self.global_stats["total_hits"] += 1
            self.global_stats["tier_hits"]["hot"] += 1

            response_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.debug(f"🚀 Cache HIT (HOT): {key} ({response_time:.2f}ms)")
            return entry.data

        # Try warm cache
        entry = await self.warm_cache.get(key)
        if entry:
            self.global_stats["total_hits"] += 1
            self.global_stats["tier_hits"]["warm"] += 1

            # Consider promotion to hot cache
            if entry.access_count >= self.promotion_threshold:
                await self._promote_to_hot(entry)

            response_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.debug(f"🚀 Cache HIT (WARM): {key} ({response_time:.2f}ms)")
            return entry.data

        # Try cold cache
        entry = await self.cold_cache.get(key)
        if entry:
            self.global_stats["total_hits"] += 1
            self.global_stats["tier_hits"]["cold"] += 1

            # Consider promotion to warm cache
            if entry.access_count >= self.promotion_threshold:
                await self._promote_to_warm(entry)

            response_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.debug(f"🚀 Cache HIT (COLD): {key} ({response_time:.2f}ms)")
            return entry.data

        # Cache miss
        self.global_stats["total_misses"] += 1
        logger.debug(f"❌ Cache MISS: {key}")
        return None

    async def set(
        self,
        key: str,
        data: Any,
        cache_type: CacheType,
        ttl_seconds: int = 3600,
        tier: CacheTier = None,
    ) -> bool:
        """Set data in hierarchical cache with intelligent placement"""
        try:
            # Calculate data size
            serialized_data = pickle.dumps(data)
            size_bytes = len(serialized_data)

            # Create cache entry
            entry = CacheEntry(
                key=key,
                data=data,
                tier=tier or self._determine_optimal_tier(size_bytes, cache_type),
                cache_type=cache_type,
                size_bytes=size_bytes,
                ttl_seconds=ttl_seconds,
                created_at=datetime.now(),
                expires_at=datetime.now() + timedelta(seconds=ttl_seconds),
            )

            # Set in appropriate tier
            if entry.tier == CacheTier.HOT:
                success = await self.hot_cache.set(entry)
            elif entry.tier == CacheTier.WARM:
                success = await self.warm_cache.set(entry)
            else:  # COLD
                success = await self.cold_cache.set(entry)

            if success:
                logger.debug(f"✅ Cache SET ({entry.tier.value.upper()}): {key}")

            return success

        except Exception as e:
            logger.error(f"Error setting cache entry: {e}")
            return False

    def _determine_optimal_tier(
        self, size_bytes: int, cache_type: CacheType
    ) -> CacheTier:
        """Determine optimal cache tier based on size and type"""

        # Small, frequently accessed data goes to hot cache
        if size_bytes < 10 * 1024:  # < 10KB
            if cache_type in [CacheType.USER_SESSION, CacheType.ANALYTICS]:
                return CacheTier.HOT

        # Medium size data goes to warm cache
        if size_bytes < 1024 * 1024:  # < 1MB
            if cache_type in [CacheType.QUERY_RESULT, CacheType.SEARCH_RESULT]:
                return CacheTier.WARM

        # Large data goes to cold cache
        return CacheTier.COLD

    async def _promote_to_hot(self, entry: CacheEntry):
        """Promote entry from warm to hot cache"""
        try:
            entry.tier = CacheTier.HOT
            success = await self.hot_cache.set(entry)

            if success:
                await self.warm_cache.delete(entry.key)
                self.global_stats["promotion_count"] += 1
                logger.debug(f"⬆️ PROMOTED to HOT: {entry.key}")

        except Exception as e:
            logger.error(f"Error promoting to hot cache: {e}")

    async def _promote_to_warm(self, entry: CacheEntry):
        """Promote entry from cold to warm cache"""
        try:
            entry.tier = CacheTier.WARM
            success = await self.warm_cache.set(entry)

            if success:
                await self.cold_cache.delete(entry.key)
                self.global_stats["promotion_count"] += 1
                logger.debug(f"⬆️ PROMOTED to WARM: {entry.key}")

        except Exception as e:
            logger.error(f"Error promoting to warm cache: {e}")

    async def delete(self, key: str) -> bool:
        """Delete key from all cache tiers"""
        results = []

        results.append(await self.hot_cache.delete(key))
        results.append(await self.warm_cache.delete(key))
        results.append(await self.cold_cache.delete(key))

        return any(results)

    async def exists(self, key: str) -> bool:
        """Check if key exists in any cache tier"""
        return (
            await self.hot_cache.exists(key)
            or await self.warm_cache.exists(key)
            or await self.cold_cache.exists(key)
        )

    async def _background_maintenance(self):
        """Background maintenance tasks"""
        while True:
            try:
                # Run maintenance every 5 minutes
                await asyncio.sleep(300)

                # Clean expired entries (handled by individual caches)
                # Optimize cache distribution
                await self._optimize_cache_distribution()

            except Exception as e:
                logger.error(f"Error in background maintenance: {e}")

    async def _optimize_cache_distribution(self):
        """Optimize data distribution across cache tiers"""
        # This would implement sophisticated algorithms to:
        # 1. Demote unused entries from hot to warm
        # 2. Demote unused entries from warm to cold
        # 3. Promote frequently accessed entries
        # For now, this is a placeholder
        pass

    async def get_comprehensive_stats(self) -> dict[str, Any]:
        """Get comprehensive caching statistics"""
        hot_stats = await self.hot_cache.get_stats()
        warm_stats = await self.warm_cache.get_stats()
        cold_stats = await self.cold_cache.get_stats()

        total_requests = self.global_stats["total_requests"]
        hit_rate = (self.global_stats["total_hits"] / max(total_requests, 1)) * 100

        return {
            "global_stats": {
                **self.global_stats,
                "overall_hit_rate_percent": hit_rate,
                "miss_rate_percent": 100 - hit_rate,
            },
            "tier_stats": {"hot": hot_stats, "warm": warm_stats, "cold": cold_stats},
            "performance": {
                "sub_ms_access_rate": (
                    self.global_stats["tier_hits"]["hot"] / max(total_requests, 1)
                )
                * 100,
                "fast_access_rate": (
                    (
                        self.global_stats["tier_hits"]["hot"]
                        + self.global_stats["tier_hits"]["warm"]
                    )
                    / max(total_requests, 1)
                )
                * 100,
            },
        }

    async def flush_all(self):
        """Flush all cache tiers"""
        # Note: Implementing full flush for testing/admin purposes
        self.hot_cache.cache.clear()
        self.hot_cache.current_size_bytes = 0
        self.hot_cache.access_order.clear()

        if self.warm_cache.redis_client:
            await self.warm_cache.redis_client.flushdb()

        self.cold_cache.index.clear()
        import shutil

        shutil.rmtree(self.cold_cache.storage_path, ignore_errors=True)
        import os

        os.makedirs(self.cold_cache.storage_path, exist_ok=True)

        # Reset stats
        self.global_stats = {
            "total_requests": 0,
            "total_hits": 0,
            "total_misses": 0,
            "tier_hits": {"hot": 0, "warm": 0, "cold": 0},
            "promotion_count": 0,
            "demotion_count": 0,
        }

        logger.info("🧹 All cache tiers flushed")

    async def stop(self):
        """Stop the caching service"""
        try:
            if self._maintenance_task:
                self._maintenance_task.cancel()

            if self.warm_cache.redis_client:
                await self.warm_cache.redis_client.close()

            logger.info("✅ Hierarchical Caching Service stopped")

        except Exception as e:
            logger.error(f"Error stopping caching service: {e}")


# Factory function
async def create_caching_service(
    hot_cache_size_mb: int = 100,
    redis_url: str = "redis://localhost:6379",
    cold_storage_path: str = "/tmp/sophia_cold_cache",
) -> HierarchicalCachingService:
    """Create and initialize hierarchical caching service"""
    service = HierarchicalCachingService(
        hot_cache_size_mb, redis_url, cold_storage_path
    )
    await service.initialize()
    return service
