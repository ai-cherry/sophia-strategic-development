-- AI Usage Analytics Schema for SmartAIService
-- Comprehensive logging and monitoring of LLM usage across Portkey/OpenRouter

-- Create schema if it doesn't exist
CREATE SCHEMA IF NOT EXISTS OPS_MONITORING;

-- AI Usage Analytics Table
CREATE TABLE IF NOT EXISTS OPS_MONITORING.AI_USAGE_ANALYTICS (
    -- Request identification
    REQUEST_ID VARCHAR(100) NOT NULL,
    TIMESTAMP TIMESTAMP_NTZ NOT NULL,
    
    -- Provider and model information
    PROVIDER VARCHAR(50) NOT NULL,  -- portkey, openrouter, fallback
    MODEL VARCHAR(100) NOT NULL,
    
    -- Request context
    TASK_TYPE VARCHAR(50) NOT NULL,
    USER_ID VARCHAR(100) NOT NULL,
    SESSION_ID VARCHAR(100),
    
    -- Performance metrics
    COST_USD DECIMAL(10, 6) NOT NULL DEFAULT 0,
    LATENCY_MS INTEGER NOT NULL DEFAULT 0,
    TOKENS_USED INTEGER NOT NULL DEFAULT 0,
    
    -- Quality and caching
    CACHE_HIT BOOLEAN NOT NULL DEFAULT FALSE,
    QUALITY_SCORE DECIMAL(3, 2) DEFAULT 0,
    
    -- Routing intelligence
    PERFORMANCE_PRIORITY BOOLEAN NOT NULL DEFAULT TRUE,
    COST_SENSITIVITY DECIMAL(3, 2) NOT NULL DEFAULT 0.5,
    ROUTING_REASONING VARCHAR(500),
    
    -- Error handling
    ERROR VARCHAR(1000),
    
    -- Metadata
    METADATA VARIANT,
    
    -- Audit fields
    CREATED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    
    -- Primary key
    PRIMARY KEY (REQUEST_ID, TIMESTAMP)
);

-- Create indexes for performance
CREATE INDEX IF NOT EXISTS IDX_AI_USAGE_TIMESTAMP ON OPS_MONITORING.AI_USAGE_ANALYTICS (TIMESTAMP);
CREATE INDEX IF NOT EXISTS IDX_AI_USAGE_PROVIDER ON OPS_MONITORING.AI_USAGE_ANALYTICS (PROVIDER);
CREATE INDEX IF NOT EXISTS IDX_AI_USAGE_MODEL ON OPS_MONITORING.AI_USAGE_ANALYTICS (MODEL);
CREATE INDEX IF NOT EXISTS IDX_AI_USAGE_TASK_TYPE ON OPS_MONITORING.AI_USAGE_ANALYTICS (TASK_TYPE);
CREATE INDEX IF NOT EXISTS IDX_AI_USAGE_USER ON OPS_MONITORING.AI_USAGE_ANALYTICS (USER_ID);

-- Cost Analytics View
CREATE OR REPLACE VIEW OPS_MONITORING.V_AI_COST_ANALYTICS AS
SELECT 
    DATE_TRUNC('day', TIMESTAMP) AS USAGE_DATE,
    PROVIDER,
    MODEL,
    TASK_TYPE,
    COUNT(*) AS TOTAL_REQUESTS,
    SUM(COST_USD) AS TOTAL_COST,
    AVG(COST_USD) AS AVG_COST_PER_REQUEST,
    SUM(TOKENS_USED) AS TOTAL_TOKENS,
    AVG(LATENCY_MS) AS AVG_LATENCY,
    AVG(QUALITY_SCORE) AS AVG_QUALITY,
    SUM(CASE WHEN CACHE_HIT THEN 1 ELSE 0 END) AS CACHE_HITS,
    SUM(CASE WHEN CACHE_HIT THEN 1 ELSE 0 END) / COUNT(*) AS CACHE_HIT_RATE,
    SUM(CASE WHEN ERROR IS NOT NULL THEN 1 ELSE 0 END) AS ERROR_COUNT,
    SUM(CASE WHEN ERROR IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*) AS ERROR_RATE
FROM OPS_MONITORING.AI_USAGE_ANALYTICS
WHERE TIMESTAMP >= CURRENT_DATE - 30  -- Last 30 days
GROUP BY 1, 2, 3, 4
ORDER BY USAGE_DATE DESC, TOTAL_COST DESC;

-- Performance Analytics View
CREATE OR REPLACE VIEW OPS_MONITORING.V_AI_PERFORMANCE_ANALYTICS AS
SELECT 
    PROVIDER,
    MODEL,
    COUNT(*) AS TOTAL_REQUESTS,
    AVG(LATENCY_MS) AS AVG_LATENCY_MS,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LATENCY_MS) AS MEDIAN_LATENCY_MS,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY LATENCY_MS) AS P95_LATENCY_MS,
    AVG(QUALITY_SCORE) AS AVG_QUALITY_SCORE,
    SUM(CASE WHEN ERROR IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*) AS ERROR_RATE,
    SUM(CASE WHEN CACHE_HIT THEN 1 ELSE 0 END) / COUNT(*) AS CACHE_HIT_RATE,
    SUM(COST_USD) / COUNT(*) AS AVG_COST_PER_REQUEST
FROM OPS_MONITORING.AI_USAGE_ANALYTICS
WHERE TIMESTAMP >= CURRENT_DATE - 7  -- Last 7 days
GROUP BY PROVIDER, MODEL
ORDER BY TOTAL_REQUESTS DESC;

-- Strategic Model Usage View
CREATE OR REPLACE VIEW OPS_MONITORING.V_AI_STRATEGIC_USAGE AS
SELECT 
    TASK_TYPE,
    MODEL,
    PROVIDER,
    COUNT(*) AS USAGE_COUNT,
    AVG(COST_USD) AS AVG_COST,
    AVG(QUALITY_SCORE) AS AVG_QUALITY,
    AVG(LATENCY_MS) AS AVG_LATENCY,
    ROUTING_REASONING,
    COUNT(DISTINCT USER_ID) AS UNIQUE_USERS
FROM OPS_MONITORING.AI_USAGE_ANALYTICS
WHERE TIMESTAMP >= CURRENT_DATE - 14  -- Last 2 weeks
GROUP BY TASK_TYPE, MODEL, PROVIDER, ROUTING_REASONING
ORDER BY USAGE_COUNT DESC;

-- Cost Optimization Opportunities View
CREATE OR REPLACE VIEW OPS_MONITORING.V_AI_COST_OPTIMIZATION AS
WITH cost_analysis AS (
    SELECT 
        TASK_TYPE,
        MODEL,
        PROVIDER,
        COUNT(*) AS request_count,
        AVG(COST_USD) AS avg_cost,
        AVG(QUALITY_SCORE) AS avg_quality,
        AVG(LATENCY_MS) AS avg_latency,
        SUM(COST_USD) AS total_cost
    FROM OPS_MONITORING.AI_USAGE_ANALYTICS
    WHERE TIMESTAMP >= CURRENT_DATE - 30
    GROUP BY TASK_TYPE, MODEL, PROVIDER
),
task_benchmarks AS (
    SELECT 
        TASK_TYPE,
        MIN(avg_cost) AS min_cost_for_task,
        MAX(avg_quality) AS max_quality_for_task
    FROM cost_analysis
    GROUP BY TASK_TYPE
)
SELECT 
    ca.TASK_TYPE,
    ca.MODEL,
    ca.PROVIDER,
    ca.request_count,
    ca.avg_cost,
    ca.avg_quality,
    ca.total_cost,
    tb.min_cost_for_task,
    (ca.avg_cost - tb.min_cost_for_task) AS cost_premium,
    (ca.avg_cost - tb.min_cost_for_task) * ca.request_count AS potential_savings,
    CASE 
        WHEN ca.avg_cost > tb.min_cost_for_task * 1.5 AND ca.avg_quality < tb.max_quality_for_task * 0.9 
        THEN 'HIGH_OPTIMIZATION_OPPORTUNITY'
        WHEN ca.avg_cost > tb.min_cost_for_task * 1.2 
        THEN 'MEDIUM_OPTIMIZATION_OPPORTUNITY'
        ELSE 'WELL_OPTIMIZED'
    END AS optimization_opportunity
FROM cost_analysis ca
JOIN task_benchmarks tb ON ca.TASK_TYPE = tb.TASK_TYPE
WHERE ca.total_cost > 1.0  -- Focus on significant cost items
ORDER BY potential_savings DESC;

-- Real-time Monitoring View
CREATE OR REPLACE VIEW OPS_MONITORING.V_AI_REALTIME_MONITORING AS
SELECT 
    DATE_TRUNC('hour', TIMESTAMP) AS HOUR,
    PROVIDER,
    COUNT(*) AS REQUESTS_PER_HOUR,
    SUM(COST_USD) AS COST_PER_HOUR,
    AVG(LATENCY_MS) AS AVG_LATENCY,
    SUM(CASE WHEN ERROR IS NOT NULL THEN 1 ELSE 0 END) AS ERRORS_PER_HOUR,
    SUM(CASE WHEN CACHE_HIT THEN 1 ELSE 0 END) / COUNT(*) AS CACHE_HIT_RATE
FROM OPS_MONITORING.AI_USAGE_ANALYTICS
WHERE TIMESTAMP >= CURRENT_TIMESTAMP - INTERVAL '24 HOURS'
GROUP BY 1, 2
ORDER BY HOUR DESC;

-- Gateway Health Status View
CREATE OR REPLACE VIEW OPS_MONITORING.V_AI_GATEWAY_HEALTH AS
WITH recent_usage AS (
    SELECT 
        PROVIDER,
        COUNT(*) AS total_requests,
        SUM(CASE WHEN ERROR IS NOT NULL THEN 1 ELSE 0 END) AS error_count,
        AVG(LATENCY_MS) AS avg_latency,
        MAX(TIMESTAMP) AS last_request_time
    FROM OPS_MONITORING.AI_USAGE_ANALYTICS
    WHERE TIMESTAMP >= CURRENT_TIMESTAMP - INTERVAL '1 HOUR'
    GROUP BY PROVIDER
)
SELECT 
    PROVIDER,
    total_requests,
    error_count,
    CASE WHEN total_requests > 0 THEN error_count / total_requests ELSE 0 END AS error_rate,
    avg_latency,
    last_request_time,
    CASE 
        WHEN last_request_time < CURRENT_TIMESTAMP - INTERVAL '30 MINUTES' THEN 'INACTIVE'
        WHEN error_count / GREATEST(total_requests, 1) > 0.1 THEN 'DEGRADED'
        WHEN avg_latency > 5000 THEN 'SLOW'
        ELSE 'HEALTHY'
    END AS health_status
FROM recent_usage
ORDER BY total_requests DESC;

-- Stored procedure for cost analytics
CREATE OR REPLACE PROCEDURE OPS_MONITORING.SP_GET_AI_COST_SUMMARY(
    START_DATE DATE,
    END_DATE DATE
)
RETURNS TABLE (
    provider VARCHAR,
    model VARCHAR,
    total_requests NUMBER,
    total_cost NUMBER,
    avg_cost_per_request NUMBER,
    total_tokens NUMBER,
    avg_quality NUMBER,
    cache_hit_rate NUMBER
)
LANGUAGE SQL
AS
$$
BEGIN
    RETURN TABLE(
        SELECT 
            PROVIDER,
            MODEL,
            COUNT(*) AS total_requests,
            SUM(COST_USD) AS total_cost,
            AVG(COST_USD) AS avg_cost_per_request,
            SUM(TOKENS_USED) AS total_tokens,
            AVG(QUALITY_SCORE) AS avg_quality,
            SUM(CASE WHEN CACHE_HIT THEN 1 ELSE 0 END) / COUNT(*) AS cache_hit_rate
        FROM OPS_MONITORING.AI_USAGE_ANALYTICS
        WHERE DATE(TIMESTAMP) BETWEEN START_DATE AND END_DATE
        GROUP BY PROVIDER, MODEL
        ORDER BY total_cost DESC
    );
END;
$$;

-- Grant permissions
GRANT USAGE ON SCHEMA OPS_MONITORING TO ROLE SOPHIA_AI_ROLE;
GRANT SELECT, INSERT ON TABLE OPS_MONITORING.AI_USAGE_ANALYTICS TO ROLE SOPHIA_AI_ROLE;
GRANT SELECT ON ALL VIEWS IN SCHEMA OPS_MONITORING TO ROLE SOPHIA_AI_ROLE;
GRANT USAGE ON PROCEDURE OPS_MONITORING.SP_GET_AI_COST_SUMMARY(DATE, DATE) TO ROLE SOPHIA_AI_ROLE;

-- Comments for documentation
COMMENT ON TABLE OPS_MONITORING.AI_USAGE_ANALYTICS IS 'Comprehensive logging of AI/LLM usage across Portkey and OpenRouter gateways';
COMMENT ON COLUMN OPS_MONITORING.AI_USAGE_ANALYTICS.PROVIDER IS 'AI gateway provider: portkey, openrouter, or fallback';
COMMENT ON COLUMN OPS_MONITORING.AI_USAGE_ANALYTICS.ROUTING_REASONING IS 'Explanation of why this provider/model was selected';
COMMENT ON COLUMN OPS_MONITORING.AI_USAGE_ANALYTICS.PERFORMANCE_PRIORITY IS 'Whether performance was prioritized over cost';
COMMENT ON COLUMN OPS_MONITORING.AI_USAGE_ANALYTICS.COST_SENSITIVITY IS 'Cost sensitivity score (0.0 = cost-focused, 1.0 = performance-focused)';

COMMENT ON VIEW OPS_MONITORING.V_AI_COST_ANALYTICS IS 'Daily cost analytics across all AI providers and models';
COMMENT ON VIEW OPS_MONITORING.V_AI_PERFORMANCE_ANALYTICS IS 'Performance metrics and SLA monitoring for AI services';
COMMENT ON VIEW OPS_MONITORING.V_AI_COST_OPTIMIZATION IS 'Identifies cost optimization opportunities by comparing model performance vs cost';
COMMENT ON VIEW OPS_MONITORING.V_AI_GATEWAY_HEALTH IS 'Real-time health monitoring of AI gateway providers'; 