from __future__ import annotations

from datetime import UTC, datetime
import uuid

"""
Snowflake Cortex AI Service

Utility module for leveraging Snowflake Cortex AI capabilities directly within Snowflake.
This module provides a Python interface to Snowflake's native AI functions including
text summarization, sentiment analysis, embeddings, and vector search.

Key Features:
- Native Snowflake AI processing with SNOWFLAKE.CORTEX functions
- Vector embeddings and semantic search within Snowflake
- Text analysis and summarization for business intelligence
- Integration with HubSpot and Gong data for contextual AI
- Reduced dependency on external AI services for specific use cases
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 2237 lines

Recommended decomposition:
- snowflake_cortex_service_core.py - Core functionality
- snowflake_cortex_service_utils.py - Utility functions
- snowflake_cortex_service_models.py - Data models
- snowflake_cortex_service_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import json
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any

import pandas as pd

from backend.core.auto_esc_config import get_config_value

# Import ConnectionType from optimized connection manager
from backend.core.optimized_connection_manager import ConnectionType

logger = logging.getLogger(__name__)


# Custom Exception Classes
class CortexEmbeddingError(Exception):
    """Raised when Snowflake Cortex embedding generation fails"""

    pass


class InsufficientPermissionsError(Exception):
    """Raised when user lacks required Snowflake permissions"""

    pass


class BusinessTableNotFoundError(Exception):
    """Raised when business table doesn't exist or is not accessible"""

    pass


class InvalidInputError(Exception):
    """Raised when input parameters are invalid"""

    pass


class CortexModel(Enum):
    """Available Snowflake Cortex models"""

    # Text generation models
    LLAMA2_70B = "llama2-70b-chat"
    MISTRAL_7B = "mistral-7b"
    MISTRAL_LARGE = "mistral-large"
    MIXTRAL_8X7B = "mixtral-8x7b"

    # Embedding models
    E5_BASE_V2 = "e5-base-v2"
    MULTILINGUAL_E5_LARGE = "multilingual-e5-large"

    # Analysis models
    SENTIMENT_ANALYSIS = "sentiment"
    SUMMARIZATION = "summarize"


@dataclass
class CortexQuery:
    """Configuration for Cortex AI queries"""

    model: CortexModel
    input_text: str
    parameters: dict[str, Any] | None = None
    context: str | None = None
    max_tokens: int | None = None


@dataclass
class VectorSearchResult:
    """Result from vector similarity search"""

    content: str
    similarity_score: float
    metadata: dict[str, Any]
    source_table: str
    source_id: str


class SnowflakeCortexService:
    """
    Service for accessing Snowflake Cortex AI capabilities

    This class provides methods to use Snowflake's native AI functions
    for text processing, embeddings, and vector search directly within
    the data warehouse.
    """

    def __init__(self):
        # Remove individual connection - use optimized connection manager
        from backend.core.optimized_connection_manager import connection_manager

        self.connection_manager = connection_manager

        self.database = get_config_value("snowflake_database", "SOPHIA_AI")
        self.schema = get_config_value("snowflake_schema", "AI_PROCESSING")
        self.warehouse = get_config_value("snowflake_warehouse", "SOPHIA_AI_WH")
        self.initialized = False

        # Vector storage tables
        self.vector_tables = {
            "hubspot_embeddings": "HUBSPOT_CONTACT_EMBEDDINGS",
            "gong_embeddings": "GONG_CALL_EMBEDDINGS",
            "document_embeddings": "DOCUMENT_EMBEDDINGS",
            "memory_embeddings": "AI_MEMORY_EMBEDDINGS",
        }

    async def __aenter__(self):
        """Async context manager entry - initialize connection"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit - cleanup resources"""
        # No need to close individual connection - managed by connection manager
        pass

    async def initialize(self) -> None:
        """Initialize Snowflake connection for Cortex AI processing"""
        if self.initialized:
            return

        try:
            # Use connection manager instead of individual connection
            await self.connection_manager.initialize()

            # Set database and schema context
            await self.connection_manager.execute_query(f"USE DATABASE {self.database}")
            await self.connection_manager.execute_query(f"USE SCHEMA {self.schema}")
            await self.connection_manager.execute_query(
                f"USE WAREHOUSE {self.warehouse}"
            )

            # Ensure vector tables exist
            await self._create_vector_tables()

            self.initialized = True
            logger.info(
                "✅ Snowflake Cortex service initialized successfully with optimized connection manager"
            )

        except Exception as e:
            logger.error(f"Failed to initialize Snowflake Cortex service: {e}")
            raise

    async def summarize_text_in_snowflake(
        self,
        text_column: str,
        table_name: str,
        conditions: str | None = None,
        max_length: int = 200,
    ) -> list[dict[str, Any]]:
        """
        Summarize text data using Snowflake Cortex SUMMARIZE function

        Args:
            text_column: Column containing text to summarize
            table_name: Source table name
            conditions: Optional WHERE conditions
            max_length: Maximum summary length

        Returns:
            List of records with original text and AI-generated summaries
        """
        if not self.initialized:
            await self.initialize()

        # Build query using Snowflake Cortex SUMMARIZE function
        query = f"""
        SELECT
            id,
            {text_column} as original_text,
            SNOWFLAKE.CORTEX.SUMMARIZE(
                {text_column},
                {max_length}
            ) as ai_summary,
            CURRENT_TIMESTAMP() as processed_at
        FROM {table_name}
        """

        if conditions:
            query += f" WHERE {conditions}"

        query += " ORDER BY id"

        try:
            # Use connection manager instead of direct cursor
            results = await self.connection_manager.execute_query(query)

            summaries = []
            for row in results:
                # Convert row to dictionary format
                record = {
                    "id": row[0],
                    "original_text": row[1],
                    "ai_summary": row[2],
                    "processed_at": row[3],
                }
                summaries.append(record)

            logger.info(f"Generated {len(summaries)} text summaries using Cortex")
            return summaries

        except Exception as e:
            logger.error(f"Error generating summaries with Cortex: {e}")
            raise

    async def analyze_sentiment_in_snowflake(
        self, text_column: str, table_name: str, conditions: str | None = None
    ) -> list[dict[str, Any]]:
        """
        Analyze sentiment using Snowflake Cortex SENTIMENT function

        Args:
            text_column: Column containing text to analyze
            table_name: Source table name
            conditions: Optional WHERE conditions

        Returns:
            List of records with sentiment scores and classifications
        """
        if not self.initialized:
            await self.initialize()

        query = f"""
        SELECT
            id,
            {text_column} as text,
            SNOWFLAKE.CORTEX.SENTIMENT({text_column}) as sentiment_score,
            CASE
                WHEN SNOWFLAKE.CORTEX.SENTIMENT({text_column}) > 0.1 THEN 'POSITIVE'
                WHEN SNOWFLAKE.CORTEX.SENTIMENT({text_column}) < -0.1 THEN 'NEGATIVE'
                ELSE 'NEUTRAL'
            END as sentiment_label,
            CURRENT_TIMESTAMP() as analyzed_at
        FROM {table_name}
        """

        if conditions:
            query += f" WHERE {conditions}"

        try:
            # Use connection manager instead of direct cursor
            results = await self.connection_manager.execute_query(query)

            sentiment_analysis = []
            for row in results:
                # Convert row to dictionary format
                record = {
                    "id": row[0],
                    "text": row[1],
                    "sentiment_score": row[2],
                    "sentiment_label": row[3],
                    "analyzed_at": row[4],
                }
                sentiment_analysis.append(record)

            logger.info(
                f"Analyzed sentiment for {len(sentiment_analysis)} records using Cortex"
            )
            return sentiment_analysis

        except Exception as e:
            logger.error(f"Error analyzing sentiment with Cortex: {e}")
            raise

    async def generate_embedding_in_snowflake(
        self,
        text_column: str,
        table_name: str,
        conditions: str | None = None,
        model: str = "e5-base-v2",
        store_embeddings: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Generate embeddings using Snowflake Cortex EMBED_TEXT function

        Args:
            text_column: Column containing text to embed
            table_name: Source table name
            conditions: Optional WHERE conditions
            model: Embedding model to use
            store_embeddings: Whether to store embeddings in vector table

        Returns:
            List of records with generated embeddings
        """
        if not self.initialized:
            await self.initialize()

        query = f"""
        SELECT
            id,
            {text_column} as text,
            SNOWFLAKE.CORTEX.EMBED_TEXT('{model}', {text_column}) as embedding_vector,
            '{model}' as embedding_model,
            CURRENT_TIMESTAMP() as embedded_at
        FROM {table_name}
        """

        if conditions:
            query += f" WHERE {conditions}"

        try:
            # Use connection manager instead of direct cursor
            results = await self.connection_manager.execute_query(query)

            embeddings = []
            for row in results:
                # Convert row to dictionary format
                record = {
                    "id": row[0],
                    "text": row[1],
                    "embedding_vector": row[2],
                    "embedding_model": row[3],
                    "embedded_at": row[4],
                }
                embeddings.append(record)

            # Optionally store embeddings in dedicated vector table
            if store_embeddings and embeddings:
                await self._store_embeddings(embeddings, table_name)

            logger.info(f"Generated {len(embeddings)} embeddings using Cortex {model}")
            return embeddings

        except Exception as e:
            logger.error(f"Error generating embeddings with Cortex: {e}")
            raise

    async def vector_search_in_snowflake(
        self,
        query_text: str,
        vector_table: str,
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        model: str = "e5-base-v2",
    ) -> list[VectorSearchResult]:
        """
        Perform vector similarity search using Snowflake native functions

        Args:
            query_text: Text to search for
            vector_table: Table containing stored embeddings
            top_k: Number of top results to return
            similarity_threshold: Minimum similarity score
            model: Embedding model for query encoding

        Returns:
            List of similar content with scores and metadata
        """
        if not self.initialized:
            await self.initialize()

        # Generate embedding for query text
        query = f"""
        WITH query_embedding AS (
            SELECT SNOWFLAKE.CORTEX.EMBED_TEXT('{model}', '{query_text}') as query_vector
        ),
        similarity_scores AS (
            SELECT
                v.id,
                v.original_text,
                v.metadata,
                v.source_table,
                v.source_id,
                VECTOR_COSINE_SIMILARITY(q.query_vector, v.embedding_vector) as similarity_score
            FROM {vector_table} v
            CROSS JOIN query_embedding q
            WHERE VECTOR_COSINE_SIMILARITY(q.query_vector, v.embedding_vector) >= {similarity_threshold}
        )
        SELECT
            id,
            original_text as content,
            metadata,
            source_table,
            source_id,
            similarity_score
        FROM similarity_scores
        ORDER BY similarity_score DESC
        LIMIT {top_k}
        """

        try:
            # Use connection manager to execute query
            results = await self.connection_manager.execute_query(query)
            
            vector_results = []
            for row in results:
                result = VectorSearchResult(
                    content=row[1],
                    similarity_score=row[5],
                    metadata=row[2] if row[2] else {},
                    source_table=row[3],
                    source_id=row[4],
                )
                vector_results.append(result)

            logger.info(
                f"Found {len(vector_results)} similar results for query: {query_text[:50]}..."
            )
            return vector_results

        except Exception as e:
            logger.error(f"Error performing vector search: {e}")
            raise

    async def complete_text_with_cortex(
        self,
        prompt: str,
        model: CortexModel = CortexModel.MISTRAL_7B,
        max_tokens: int = 500,
        temperature: float = 0.7,
        context: str | None = None,
    ) -> str:
        """
        Generate text completion using Snowflake Cortex LLM functions

        Args:
            prompt: Input prompt for text generation
            model: Cortex model to use
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            context: Optional context for the prompt

        Returns:
            Generated text completion
        """
        if not self.initialized:
            await self.initialize()

        # Combine context and prompt if provided
        full_prompt = f"{context}\n\n{prompt}" if context else prompt

        query = f"""
        SELECT SNOWFLAKE.CORTEX.COMPLETE(
            '{model.value}',
            '{full_prompt}',
            {{
                'max_tokens': {max_tokens},
                'temperature': {temperature}
            }}
        ) as completion
        """

        try:
            # Use connection manager to execute query
            results = await self.connection_manager.execute_query(query)
            
            if results and len(results) > 0:
                completion = results[0][0] if results[0][0] else ""
            else:
                completion = ""

            logger.info(f"Generated text completion using {model.value}")
            return completion

        except Exception as e:
            logger.error(f"Error generating text completion: {e}")
            raise

    async def extract_entities_from_text(
        self,
        text_column: str,
        table_name: str,
        entity_types: list[str],
        conditions: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Extract named entities from text using Cortex capabilities

        Args:
            text_column: Column containing text to analyze
            table_name: Source table name
            entity_types: Types of entities to extract (PERSON, ORG, MONEY, etc.)
            conditions: Optional WHERE conditions

        Returns:
            List of records with extracted entities
        """
        if not self.initialized:
            await self.initialize()

        # Use Cortex completion to extract entities
        entity_prompt = f"Extract the following entity types from the text: {', '.join(entity_types)}. Return as JSON."

        query = f"""
        SELECT
            id,
            {text_column} as text,
            SNOWFLAKE.CORTEX.COMPLETE(
                'mistral-7b',
                CONCAT('{entity_prompt}\\n\\nText: ', {text_column}),
                {{'max_tokens': 200}}
            ) as extracted_entities,
            CURRENT_TIMESTAMP() as extracted_at
        FROM {table_name}
        """

        if conditions:
            query += f" WHERE {conditions}"

        try:
            # Use connection manager to execute query
            results = await self.connection_manager.execute_query(query)
            
            entities = []
            if hasattr(results, 'to_dict'):
                # Handle pandas DataFrame
                entities = results.to_dict('records')
            else:
                # Handle raw results
                for row in results:
                    record = {
                        'id': row[0],
                        'text': row[1],
                        'extracted_entities': row[2],
                        'extracted_at': row[3]
                    }
                    entities.append(record)

            logger.info(f"Extracted entities from {len(entities)} records")
            return entities

        except Exception as e:
            logger.error(f"Error extracting entities: {e}")
            raise

    async def _create_vector_tables(self):
        """Create vector storage tables if they don't exist"""

        for _table_key, table_name in self.vector_tables.items():
            create_query = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                id STRING,
                original_text STRING,
                embedding_vector VECTOR(FLOAT, 768),  -- Adjust dimensions based on model
                embedding_model STRING,
                metadata VARIANT,
                source_table STRING,
                source_id STRING,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
            )
            """

            try:
                await self.connection_manager.execute_query(create_query)
                logger.debug(f"Ensured vector table {table_name} exists")
            except Exception as e:
                logger.warning(f"Could not create vector table {table_name}: {e}")

    async def _store_embeddings(
        self, embeddings: list[dict[str, Any]], source_table: str
    ):
        """Store embeddings in dedicated vector table"""

        vector_table = self.vector_tables.get(
            "document_embeddings", "DOCUMENT_EMBEDDINGS"
        )

        # Prepare insert data
        insert_data = []
        for embedding in embeddings:
            insert_data.append(
                (
                    embedding["id"],
                    embedding["text"],
                    embedding["embedding_vector"],
                    embedding["embedding_model"],
                    None,  # metadata
                    source_table,
                    embedding["id"],
                )
            )

        insert_query = f"""
        INSERT INTO {vector_table}
        (id, original_text, embedding_vector, embedding_model, metadata, source_table, source_id)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """

        try:
            # Use connection manager's pool to get connection
            pool = self.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
            if not pool:
                raise ValueError("Snowflake connection pool not available")
                
            async with pool.get_connection() as conn:
                cursor = conn.cursor()
                cursor.executemany(insert_query, insert_data)
                cursor.close()
                
            logger.info(f"Stored {len(insert_data)} embeddings in {vector_table}")
        except Exception as e:
            logger.error(f"Error storing embeddings: {e}")
            raise

    async def close(self):
        """Close Snowflake connection"""
        # Connection managed by connection manager
        self.initialized = False
        logger.info("Snowflake Cortex service connection closed")

    async def store_embedding_in_business_table(
        self,
        table_name: str,
        record_id: str,
        text_content: str,
        embedding_column: str = "ai_memory_embedding",
        metadata: dict[str, Any] | None = None,
        model: str = "e5-base-v2",
    ) -> bool:
        """
        Store embedding directly in business table using SNOWFLAKE.CORTEX.EMBED_TEXT_768

        Args:
            table_name: Business table name (e.g., ENRICHED_GONG_CALLS, ENRICHED_HUBSPOT_DEALS)
            record_id: Primary key value for the record
            text_content: Text content to embed
            embedding_column: Column name for storing embedding (should be VECTOR(FLOAT, 768))
            metadata: Optional metadata to store
            model: Embedding model to use (should produce 768-dimensional vectors)

        Returns:
            True if successful, False otherwise

        Raises:
            ValueError: If record doesn't exist or invalid parameters
            CortexEmbeddingError: If embedding generation fails
            InsufficientPermissionsError: If lacking required permissions
        """
        if not self.initialized:
            await self.initialize()

        # Input validation
        ALLOWED_TABLES = {"ENRICHED_HUBSPOT_DEALS", "ENRICHED_GONG_CALLS"}
        ALLOWED_EMBEDDING_COLUMNS = {"ai_memory_embedding"}
        ALLOWED_MODELS = {"e5-base-v2", "multilingual-e5-large"}

        if table_name not in ALLOWED_TABLES:
            raise ValueError(
                f"Table {table_name} not allowed. Allowed: {ALLOWED_TABLES}"
            )

        if embedding_column not in ALLOWED_EMBEDDING_COLUMNS:
            raise ValueError(f"Embedding column {embedding_column} not allowed")

        if model not in ALLOWED_MODELS:
            raise ValueError(f"Model {model} not allowed. Allowed: {ALLOWED_MODELS}")

        if not text_content or not text_content.strip():
            raise ValueError("Text content cannot be empty")

        if len(text_content) > 8000:  # Snowflake Cortex limit
            logger.warning(
                f"Text content truncated from {len(text_content)} to 8000 characters"
            )
            text_content = text_content[:8000]

        cursor = None
        try:
            pool = self.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
            if not pool:
                raise ValueError("Snowflake connection pool not available")
                
            async with pool.get_connection() as conn:
                cursor = conn.cursor()

                # Step 1: Verify record exists
                check_query = f"SELECT 1 FROM {table_name} WHERE id = %s"
                cursor.execute(check_query, (record_id,))
                record_exists = cursor.fetchone() is not None

                if not record_exists:
                    raise ValueError(f"Record {record_id} not found in {table_name}")

                # Step 2: Generate embedding with error handling
                try:
                    embed_query = (
                        "SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(%s, %s) as embedding"
                    )
                    cursor.execute(embed_query, (model, text_content))
                    embedding_result = cursor.fetchone()

                    if not embedding_result or not embedding_result[0]:
                        raise CortexEmbeddingError(
                            f"Failed to generate embedding with model {model}"
                        )

                    generated_embedding = embedding_result[0]

                except Exception as cortex_error:
                    if "insufficient credits" in str(cortex_error).lower():
                        raise CortexEmbeddingError(
                            "Insufficient Snowflake credits for embedding generation"
                        )
                    elif "model not available" in str(cortex_error).lower():
                        raise CortexEmbeddingError(
                            f"Model {model} not available in Snowflake Cortex"
                        )
                    elif "text too long" in str(cortex_error).lower():
                        raise CortexEmbeddingError("Text content exceeds model limits")
                    else:
                        raise CortexEmbeddingError(
                            f"Cortex embedding error: {cortex_error}"
                        )

                # Step 3: Serialize metadata properly
                metadata_json = None
                if metadata:
                    try:
                        metadata_json = json.dumps(
                            metadata, default=str, ensure_ascii=False
                        )
                    except (TypeError, ValueError) as e:
                        logger.warning(f"Failed to serialize metadata: {e}")
                        metadata_json = json.dumps(
                            {"error": "failed_to_serialize", "original_error": str(e)}
                        )

                # Step 4: Update record with embedding and metadata
                update_query = f"""
                UPDATE {table_name}
                SET
                    {embedding_column} = %s,
                    ai_memory_metadata = %s,
                    ai_memory_updated_at = CURRENT_TIMESTAMP()
                WHERE id = %s
                """

                cursor.execute(
                    update_query, (generated_embedding, metadata_json, record_id)
                )
                rows_affected = cursor.rowcount

                if rows_affected > 0:
                    logger.info(
                        f"Successfully stored embedding for {record_id} in {table_name}"
                    )
                    return True
                else:
                    logger.error(
                        f"No rows updated for {record_id} in {table_name} - this should not happen"
                    )
                    return False

        except ValueError:
            # Re-raise validation errors
            raise
        except CortexEmbeddingError:
            # Re-raise Cortex-specific errors
            raise
        except Exception as e:
            if (
                "permission denied" in str(e).lower()
                or "access denied" in str(e).lower()
            ):
                raise InsufficientPermissionsError(
                    f"Insufficient permissions to update {table_name}: {e}"
                )
            else:
                logger.error(
                    f"Unexpected error storing embedding in business table: {e}"
                )
                return False
        finally:
            if cursor:
                cursor.close()

    def _iteration_1(self, metadata_filters):
        """Extracted iteration logic"""
        ALLOWED_FILTER_COLUMNS = {
            "deal_stage",
            "sentiment_category",
            "primary_user_name",
            "hubspot_deal_id",
            "call_type",
            "id",
            "contact_id",
        }
        for key in metadata_filters:
        if key not in ALLOWED_FILTER_COLUMNS:
                raise InvalidInputError(
                    f"Filter column {key} not allowed. Allowed: {ALLOWED_FILTER_COLUMNS}"
                )





    async def vector_search_business_table(
        self,
        query_text: str,
        table_name: str,
        embedding_column: str = "ai_memory_embedding",
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        metadata_filters: dict[str, Any] | None = None,
        model: str = "e5-base-v2",
    ) -> list[dict[str, Any]]:
        """
        Perform vector similarity search directly on business tables with metadata filtering

        Args:
            query_text: Text to search for
            table_name: Business table name (e.g., ENRICHED_GONG_CALLS, ENRICHED_HUBSPOT_DEALS)
            embedding_column: Column containing embeddings
            top_k: Number of top results to return
            similarity_threshold: Minimum similarity score
            metadata_filters: Optional filters (e.g., {'deal_id': '123', 'sentiment_category': 'positive'})
            model: Embedding model for query encoding

        Returns:
            List of similar records with business context

        Raises:
            InvalidInputError: If input parameters are invalid
            BusinessTableNotFoundError: If table doesn't exist
            CortexEmbeddingError: If query embedding generation fails
        """
        if not self.initialized:
            await self.initialize()

        # Input validation
        ALLOWED_TABLES = {"ENRICHED_HUBSPOT_DEALS", "ENRICHED_GONG_CALLS"}
        ALLOWED_EMBEDDING_COLUMNS = {"ai_memory_embedding"}
        ALLOWED_FILTER_COLUMNS = {
            "deal_stage",
            "sentiment_category",
            "primary_user_name",
            "hubspot_deal_id",
            "call_type",
            "id",
            "contact_id",
        }
        ALLOWED_MODELS = {"e5-base-v2", "multilingual-e5-large"}

        # Validate table name
        if table_name not in ALLOWED_TABLES:
            raise InvalidInputError(
                f"Table {table_name} not allowed. Allowed: {ALLOWED_TABLES}"
            )

        # Validate embedding column
        if embedding_column not in ALLOWED_EMBEDDING_COLUMNS:
            raise InvalidInputError(f"Embedding column {embedding_column} not allowed")

        # Validate model
        if model not in ALLOWED_MODELS:
            raise InvalidInputError(
                f"Model {model} not allowed. Allowed: {ALLOWED_MODELS}"
            )

        # Validate query text
        if not query_text or not query_text.strip():
            raise InvalidInputError("Query text cannot be empty")

        if len(query_text) > 8000:  # Snowflake Cortex limit
            logger.warning(
                f"Query text truncated from {len(query_text)} to 8000 characters"
            )
            query_text = query_text[:8000]

        # Validate parameters
        if top_k <= 0 or top_k > 100:
            raise InvalidInputError("top_k must be between 1 and 100")

        if not 0.0 <= similarity_threshold <= 1.0:
            raise InvalidInputError("similarity_threshold must be between 0.0 and 1.0")

        # Validate metadata filters
        if metadata_filters:
            self._iteration_1(metadata_filters)
        
        cursor = None
        try:
            pool = self.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
            if not pool:
                raise ValueError("Snowflake connection pool not available")
                
            async with pool.get_connection() as conn:
                cursor = conn.cursor()

                # Step 1: Verify table exists and has required columns
                table_check_query = """
                SELECT COUNT(*)
                FROM INFORMATION_SCHEMA.TABLES
                WHERE TABLE_NAME = %s
                  AND TABLE_SCHEMA = %s
                  AND TABLE_CATALOG = %s
                """
                cursor.execute(table_check_query, (table_name, self.schema, self.database))
                table_exists = cursor.fetchone()[0] > 0

                if not table_exists:
                    raise BusinessTableNotFoundError(
                        f"Table {table_name} not found in schema {self.schema}"
                    )

                # Step 2: Generate embedding for query text
                try:
                    embed_query = (
                        "SELECT SNOWFLAKE.CORTEX.EMBED_TEXT_768(%s, %s) as query_vector"
                    )
                    cursor.execute(embed_query, (model, query_text))
                    embedding_result = cursor.fetchone()

                    if not embedding_result or not embedding_result[0]:
                        raise CortexEmbeddingError(
                            f"Failed to generate query embedding with model {model}"
                        )

                    query_embedding = embedding_result[0]

                except Exception as cortex_error:
                    if "insufficient credits" in str(cortex_error).lower():
                        raise CortexEmbeddingError(
                            "Insufficient Snowflake credits for query embedding"
                        )
                    elif "model not available" in str(cortex_error).lower():
                        raise CortexEmbeddingError(
                            f"Model {model} not available in Snowflake Cortex"
                        )
                    else:
                        raise CortexEmbeddingError(f"Query embedding error: {cortex_error}")

                # Step 3: Build WHERE clause with parameterized queries - SECURE VERSION
                where_conditions = [f"{embedding_column} IS NOT NULL"]
                query_params = []

                if metadata_filters:
                    for key, value in metadata_filters.items():
                        # Key is already validated against whitelist above
                        where_conditions.append(f"{key} = %s")
                        query_params.append(value)

                where_clause = " AND ".join(where_conditions)

                # Step 4: Build the vector search query with proper parameterization - SECURE VERSION
                # Using validated table_name and embedding_column directly since they're whitelisted
                search_query = f"""
                SELECT
                    *,
                    VECTOR_COSINE_SIMILARITY(%s, {embedding_column}) as similarity_score
                FROM {table_name}
                WHERE {where_clause}
                  AND VECTOR_COSINE_SIMILARITY(%s, {embedding_column}) >= %s
                ORDER BY similarity_score DESC
                LIMIT %s
                """

                # Parameters: [query_embedding, query_embedding, ...metadata_values..., similarity_threshold, top_k]
                final_params = (
                    [query_embedding, query_embedding]
                    + query_params
                    + [similarity_threshold, top_k]
                )

                cursor.execute(search_query, final_params)

                # Get column names and results
                columns = [desc[0] for desc in cursor.description]
                results = cursor.fetchall()

                # Convert to list of dictionaries
                search_results = []
                for row in results:
                    record = dict(zip(columns, row, strict=False))
                    search_results.append(record)

                logger.info(
                    f"Found {len(search_results)} similar records in {table_name} for query: {query_text[:50]}..."
                )
                return search_results

        except InvalidInputError:
            # Re-raise validation errors
            raise
        except BusinessTableNotFoundError:
            # Re-raise table errors
            raise
        except CortexEmbeddingError:
            # Re-raise Cortex errors
            raise
        except Exception as e:
            if "permission denied" in str(e).lower():
                raise InsufficientPermissionsError(
                    f"Insufficient permissions to query {table_name}: {e}"
                )
            else:
                logger.error(f"Unexpected error in vector search: {e}")
                raise
        finally:
            if cursor:
                cursor.close()

    async def ensure_embedding_columns_exist(self, table_name: str) -> bool:
        """
        Ensure AI Memory embedding columns exist in business table

        Args:
            table_name: Business table name

        Returns:
            True if columns exist or were created successfully

        Raises:
            InvalidInputError: If table name is not allowed
            InsufficientPermissionsError: If lacking ALTER TABLE permissions
        """
        if not self.initialized:
            await self.initialize()

        # Input validation - only allow specific business tables
        ALLOWED_TABLES = {"ENRICHED_HUBSPOT_DEALS", "ENRICHED_GONG_CALLS"}

        if table_name not in ALLOWED_TABLES:
            raise InvalidInputError(
                f"Table {table_name} not allowed. Allowed: {ALLOWED_TABLES}"
            )

        cursor = None
        try:
            pool = self.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
            if not pool:
                raise ValueError("Snowflake connection pool not available")
                
            async with pool.get_connection() as conn:
                cursor = conn.cursor()

                # Use IF NOT EXISTS for safe concurrent execution
                alter_statements = [
                    f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS ai_memory_embedding VECTOR(FLOAT, 768)",
                    f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS ai_memory_metadata VARCHAR(16777216)",
                    f"ALTER TABLE {table_name} ADD COLUMN IF NOT EXISTS ai_memory_updated_at TIMESTAMP_NTZ",
                ]

                # Execute within transaction for atomicity
                cursor.execute("BEGIN TRANSACTION")

                try:
                    for statement in alter_statements:
                        cursor.execute(statement)
                        logger.info(f"Executed: {statement}")

                    cursor.execute("COMMIT")
                    logger.info(
                        f"✅ Successfully ensured AI Memory columns exist in {table_name}"
                    )
                    return True

                except Exception as alter_error:
                    cursor.execute("ROLLBACK")

                    if "permission denied" in str(alter_error).lower():
                        raise InsufficientPermissionsError(
                            f"Insufficient permissions to alter {table_name}: {alter_error}"
                        )
                    else:
                        logger.error(f"Error adding columns to {table_name}: {alter_error}")
                        raise

        except InvalidInputError:
            # Re-raise validation errors
            raise
        except InsufficientPermissionsError:
            # Re-raise permission errors
            raise
        except Exception as e:
            logger.error(f"Unexpected error ensuring embedding columns exist: {e}")
            return False
        finally:
            if cursor:
                cursor.close()

    async def search_hubspot_deals_with_ai_memory(
        self,
        query_text: str,
        top_k: int = 5,
        similarity_threshold: float = 0.7,
        deal_stage: str | None = None,
        min_deal_value: float | None = None,
        max_deal_value: float | None = None,
    ) -> list[dict[str, Any]]:
        """
        Search HubSpot deals using AI Memory embeddings with business filters

        Args:
            query_text: Search query
            top_k: Number of results
            similarity_threshold: Minimum similarity
            deal_stage: Optional deal stage filter
            min_deal_value: Minimum deal value filter
            max_deal_value: Maximum deal value filter

        Returns:
            List of matching deals with similarity scores
        """
        # Build metadata filters
        metadata_filters = {}

        if deal_stage:
            metadata_filters["deal_stage"] = deal_stage

        # Add value range filtering in the search query
        additional_conditions = []
        if min_deal_value is not None:
            additional_conditions.append(f"amount >= {min_deal_value}")
        if max_deal_value is not None:
            additional_conditions.append(f"amount <= {max_deal_value}")

        # Use the enhanced vector search
        results = await self.vector_search_business_table(
            query_text=query_text,
            table_name="ENRICHED_HUBSPOT_DEALS",
            embedding_column="ai_memory_embedding",
            top_k=top_k,
            similarity_threshold=similarity_threshold,
            metadata_filters=metadata_filters,
        )

        # Apply additional value filters if needed
        if additional_conditions:
            filtered_results = []
            for result in results:
                amount = result.get("AMOUNT", 0) or 0
                if min_deal_value is not None and amount < min_deal_value:
                    continue
                if max_deal_value is not None and amount > max_deal_value:
                    continue
                filtered_results.append(result)
            results = filtered_results

        return results

    async def search_gong_calls_with_ai_memory(
        self,
        query_text: str,
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        call_direction: str | None = None,
        date_range_days: int | None = None,
        sentiment_filter: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Enhanced semantic search across STG_GONG_CALLS with AI Memory integration

        Args:
            query_text: Natural language search query
            top_k: Maximum number of results to return
            similarity_threshold: Minimum similarity score (0.0 to 1.0)
            call_direction: Filter by call direction ('Inbound', 'Outbound')
            date_range_days: Filter calls from last N days
            sentiment_filter: Filter by sentiment ('positive', 'negative', 'neutral')

        Returns:
            List of matching Gong calls with similarity scores and AI Memory metadata
        """
        try:
            # Generate embedding for the search query
            query_embedding_sql = f"SELECT SNOWFLAKE.CORTEX.EMBED_TEXT('e5-base-v2', '{query_text}') as query_embedding"
            embedding_result = await self.execute_query(query_embedding_sql)

            if embedding_result.empty:
                return []

            # Build the search query with filters
            filters = ["AI_MEMORY_EMBEDDING IS NOT NULL"]

            if call_direction:
                filters.append(f"CALL_DIRECTION = '{call_direction}'")

            if date_range_days:
                filters.append(
                    f"CALL_DATETIME_UTC >= DATEADD(day, -{date_range_days}, CURRENT_TIMESTAMP())"
                )

            if sentiment_filter:
                if sentiment_filter.lower() == "positive":
                    filters.append("SENTIMENT_SCORE > 0.3")
                elif sentiment_filter.lower() == "negative":
                    filters.append("SENTIMENT_SCORE < -0.3")
                elif sentiment_filter.lower() == "neutral":
                    filters.append("SENTIMENT_SCORE BETWEEN -0.3 AND 0.3")

            filter_clause = " AND ".join(filters)

            # Enhanced semantic search query
            search_sql = f"""
            WITH query_embedding AS (
                SELECT SNOWFLAKE.CORTEX.EMBED_TEXT('e5-base-v2', '{query_text}') as embedding
            ),
            similarity_search AS (
                SELECT
                    gc.*,
                    VECTOR_COSINE_SIMILARITY(gc.AI_MEMORY_EMBEDDING, qe.embedding) as SIMILARITY_SCORE,
                    -- Enhanced metadata extraction
                    gc.AI_MEMORY_METADATA:call_id::STRING as MEMORY_CALL_ID,
                    gc.AI_MEMORY_METADATA:account_name::STRING as MEMORY_ACCOUNT_NAME,
                    gc.AI_MEMORY_METADATA:deal_stage::STRING as MEMORY_DEAL_STAGE,
                    gc.AI_MEMORY_METADATA:sentiment_score::FLOAT as MEMORY_SENTIMENT_SCORE,
                    gc.AI_MEMORY_METADATA:talk_ratio::FLOAT as MEMORY_TALK_RATIO,
                    gc.AI_MEMORY_METADATA:primary_user::STRING as MEMORY_PRIMARY_USER,
                    gc.AI_MEMORY_METADATA:embedding_generated_at::TIMESTAMP_NTZ as MEMORY_EMBEDDING_TIMESTAMP
                FROM STG_TRANSFORMED.STG_GONG_CALLS gc
                CROSS JOIN query_embedding qe
                WHERE {filter_clause}
                AND VECTOR_COSINE_SIMILARITY(gc.AI_MEMORY_EMBEDDING, qe.embedding) >= {similarity_threshold}
            )
            SELECT
                CALL_ID,
                CALL_TITLE,
                CALL_DATETIME_UTC,
                CALL_DURATION_SECONDS,
                CALL_DIRECTION,
                PRIMARY_USER_NAME,
                PRIMARY_USER_EMAIL,
                ACCOUNT_NAME,
                CONTACT_NAME,
                DEAL_STAGE,
                DEAL_VALUE,
                SENTIMENT_SCORE,
                CALL_SUMMARY,
                KEY_TOPICS,
                RISK_INDICATORS,
                NEXT_STEPS,
                TALK_RATIO,
                SIMILARITY_SCORE,
                -- AI Memory metadata
                MEMORY_CALL_ID,
                MEMORY_ACCOUNT_NAME,
                MEMORY_DEAL_STAGE,
                MEMORY_SENTIMENT_SCORE,
                MEMORY_TALK_RATIO,
                MEMORY_PRIMARY_USER,
                MEMORY_EMBEDDING_TIMESTAMP,
                AI_MEMORY_UPDATED_AT
            FROM similarity_search
            ORDER BY SIMILARITY_SCORE DESC
            LIMIT {top_k}
            """

            results = await self.execute_query(search_sql)

            if results.empty:
                return []

            # Convert to list of dictionaries with enhanced formatting
            search_results = []
            for _, row in results.iterrows():
                result = {
                    "call_id": row["CALL_ID"],
                    "call_title": row["CALL_TITLE"],
                    "call_datetime": (
                        row["CALL_DATETIME_UTC"].isoformat()
                        if pd.notna(row["CALL_DATETIME_UTC"])
                        else None
                    ),
                    "call_duration_seconds": (
                        int(row["CALL_DURATION_SECONDS"])
                        if pd.notna(row["CALL_DURATION_SECONDS"])
                        else 0
                    ),
                    "call_direction": row["CALL_DIRECTION"],
                    "primary_user": {
                        "name": row["PRIMARY_USER_NAME"],
                        "email": row["PRIMARY_USER_EMAIL"],
                    },
                    "account_info": {
                        "account_name": row["ACCOUNT_NAME"],
                        "contact_name": row["CONTACT_NAME"],
                        "deal_stage": row["DEAL_STAGE"],
                        "deal_value": (
                            float(row["DEAL_VALUE"])
                            if pd.notna(row["DEAL_VALUE"])
                            else None
                        ),
                    },
                    "ai_insights": {
                        "sentiment_score": (
                            float(row["SENTIMENT_SCORE"])
                            if pd.notna(row["SENTIMENT_SCORE"])
                            else None
                        ),
                        "call_summary": row["CALL_SUMMARY"],
                        "key_topics": row["KEY_TOPICS"],
                        "risk_indicators": row["RISK_INDICATORS"],
                        "next_steps": row["NEXT_STEPS"],
                        "talk_ratio": (
                            float(row["TALK_RATIO"])
                            if pd.notna(row["TALK_RATIO"])
                            else None
                        ),
                    },
                    "search_metadata": {
                        "similarity_score": float(row["SIMILARITY_SCORE"]),
                        "ai_memory_updated_at": (
                            row["AI_MEMORY_UPDATED_AT"].isoformat()
                            if pd.notna(row["AI_MEMORY_UPDATED_AT"])
                            else None
                        ),
                        "embedding_timestamp": (
                            row["MEMORY_EMBEDDING_TIMESTAMP"].isoformat()
                            if pd.notna(row["MEMORY_EMBEDDING_TIMESTAMP"])
                            else None
                        ),
                    },
                }
                search_results.append(result)

            logger.info(
                f"Found {len(search_results)} Gong calls matching query: '{query_text}'"
            )
            return search_results

        except Exception as e:
            logger.error(f"Error in Gong calls semantic search: {e}")
            return []

    async def search_gong_transcripts_with_ai_memory(
        self,
        query_text: str,
        top_k: int = 10,
        similarity_threshold: float = 0.7,
        speaker_type: str | None = None,
        call_id: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Enhanced semantic search across STG_GONG_CALL_TRANSCRIPTS with AI Memory integration

        Args:
            query_text: Natural language search query
            top_k: Maximum number of results to return
            similarity_threshold: Minimum similarity score (0.0 to 1.0)
            speaker_type: Filter by speaker type ('Internal', 'External')
            call_id: Filter by specific call ID

        Returns:
            List of matching transcript segments with similarity scores
        """
        try:
            # Build filters
            filters = ["AI_MEMORY_EMBEDDING IS NOT NULL"]

            if speaker_type:
                filters.append(f"SPEAKER_TYPE = '{speaker_type}'")

            if call_id:
                filters.append(f"CALL_ID = '{call_id}'")

            filter_clause = " AND ".join(filters)

            # Enhanced transcript search query
            search_sql = f"""
            WITH query_embedding AS (
                SELECT SNOWFLAKE.CORTEX.EMBED_TEXT('e5-base-v2', '{query_text}') as embedding
            ),
            similarity_search AS (
                SELECT
                    gt.*,
                    VECTOR_COSINE_SIMILARITY(gt.AI_MEMORY_EMBEDDING, qe.embedding) as SIMILARITY_SCORE,
                    -- Join with call data for context
                    gc.CALL_TITLE,
                    gc.ACCOUNT_NAME,
                    gc.DEAL_STAGE,
                    gc.CALL_DATETIME_UTC
                FROM STG_TRANSFORMED.STG_GONG_CALL_TRANSCRIPTS gt
                CROSS JOIN query_embedding qe
                LEFT JOIN STG_TRANSFORMED.STG_GONG_CALLS gc ON gt.CALL_ID = gc.CALL_ID
                WHERE {filter_clause}
                AND VECTOR_COSINE_SIMILARITY(gt.AI_MEMORY_EMBEDDING, qe.embedding) >= {similarity_threshold}
            )
            SELECT
                TRANSCRIPT_ID,
                CALL_ID,
                SPEAKER_NAME,
                SPEAKER_EMAIL,
                SPEAKER_TYPE,
                TRANSCRIPT_TEXT,
                START_TIME_SECONDS,
                END_TIME_SECONDS,
                SEGMENT_DURATION_SECONDS,
                SEGMENT_SENTIMENT,
                SEGMENT_SUMMARY,
                EXTRACTED_ENTITIES,
                KEY_PHRASES,
                SIMILARITY_SCORE,
                -- Call context
                CALL_TITLE,
                ACCOUNT_NAME,
                DEAL_STAGE,
                CALL_DATETIME_UTC,
                AI_MEMORY_UPDATED_AT
            FROM similarity_search
            ORDER BY SIMILARITY_SCORE DESC
            LIMIT {top_k}
            """

            results = await self.execute_query(search_sql)

            if results.empty:
                return []

            # Convert to enhanced result format
            search_results = []
            for _, row in results.iterrows():
                result = {
                    "transcript_id": row["TRANSCRIPT_ID"],
                    "call_id": row["CALL_ID"],
                    "speaker": {
                        "name": row["SPEAKER_NAME"],
                        "email": row["SPEAKER_EMAIL"],
                        "type": row["SPEAKER_TYPE"],
                    },
                    "content": {
                        "transcript_text": row["TRANSCRIPT_TEXT"],
                        "segment_summary": row["SEGMENT_SUMMARY"],
                        "extracted_entities": row["EXTRACTED_ENTITIES"],
                        "key_phrases": row["KEY_PHRASES"],
                    },
                    "timing": {
                        "start_time_seconds": (
                            int(row["START_TIME_SECONDS"])
                            if pd.notna(row["START_TIME_SECONDS"])
                            else 0
                        ),
                        "end_time_seconds": (
                            int(row["END_TIME_SECONDS"])
                            if pd.notna(row["END_TIME_SECONDS"])
                            else 0
                        ),
                        "segment_duration_seconds": (
                            int(row["SEGMENT_DURATION_SECONDS"])
                            if pd.notna(row["SEGMENT_DURATION_SECONDS"])
                            else 0
                        ),
                    },
                    "ai_insights": {
                        "segment_sentiment": (
                            float(row["SEGMENT_SENTIMENT"])
                            if pd.notna(row["SEGMENT_SENTIMENT"])
                            else None
                        ),
                        "similarity_score": float(row["SIMILARITY_SCORE"]),
                    },
                    "call_context": {
                        "call_title": row["CALL_TITLE"],
                        "account_name": row["ACCOUNT_NAME"],
                        "deal_stage": row["DEAL_STAGE"],
                        "call_datetime": (
                            row["CALL_DATETIME_UTC"].isoformat()
                            if pd.notna(row["CALL_DATETIME_UTC"])
                            else None
                        ),
                    },
                    "ai_memory_updated_at": (
                        row["AI_MEMORY_UPDATED_AT"].isoformat()
                        if pd.notna(row["AI_MEMORY_UPDATED_AT"])
                        else None
                    ),
                }
                search_results.append(result)

            logger.info(
                f"Found {len(search_results)} transcript segments matching query: '{query_text}'"
            )
            return search_results

        except Exception as e:
            logger.error(f"Error in Gong transcripts semantic search: {e}")
            return []

    async def get_gong_call_analytics(
        self, date_range_days: int = 30, include_ai_insights: bool = True
    ) -> dict[str, Any]:
        """
        Get comprehensive Gong call analytics with AI insights

        Args:
            date_range_days: Number of days to analyze
            include_ai_insights: Whether to include AI-generated insights

        Returns:
            Comprehensive analytics dictionary
        """
        try:
            # Base analytics query
            analytics_sql = f"""
            SELECT
                COUNT(*) as total_calls,
                COUNT(DISTINCT PRIMARY_USER_EMAIL) as unique_users,
                COUNT(DISTINCT ACCOUNT_NAME) as unique_accounts,
                AVG(CALL_DURATION_SECONDS) as avg_duration_seconds,
                AVG(TALK_RATIO) as avg_talk_ratio,
                AVG(SENTIMENT_SCORE) as avg_sentiment_score,

                -- Call direction breakdown
                COUNT(CASE WHEN CALL_DIRECTION = 'Inbound' THEN 1 END) as inbound_calls,
                COUNT(CASE WHEN CALL_DIRECTION = 'Outbound' THEN 1 END) as outbound_calls,

                -- Sentiment distribution
                COUNT(CASE WHEN SENTIMENT_SCORE > 0.3 THEN 1 END) as positive_sentiment_calls,
                COUNT(CASE WHEN SENTIMENT_SCORE < -0.3 THEN 1 END) as negative_sentiment_calls,
                COUNT(CASE WHEN SENTIMENT_SCORE BETWEEN -0.3 AND 0.3 THEN 1 END) as neutral_sentiment_calls,

                -- Deal stage distribution
                COUNT(CASE WHEN DEAL_STAGE IS NOT NULL THEN 1 END) as calls_with_deals,
                SUM(CASE WHEN DEAL_VALUE IS NOT NULL THEN DEAL_VALUE ELSE 0 END) as total_deal_value,

                -- AI Memory integration stats
                COUNT(CASE WHEN AI_MEMORY_EMBEDDING IS NOT NULL THEN 1 END) as calls_with_embeddings,
                COUNT(CASE WHEN CALL_SUMMARY IS NOT NULL THEN 1 END) as calls_with_summaries,
                COUNT(CASE WHEN KEY_TOPICS IS NOT NULL THEN 1 END) as calls_with_topics

            FROM STG_TRANSFORMED.STG_GONG_CALLS
            WHERE CALL_DATETIME_UTC >= DATEADD(day, -{date_range_days}, CURRENT_TIMESTAMP())
            """

            analytics_result = await self.execute_query(analytics_sql)

            if analytics_result.empty:
                return {"error": "No analytics data available"}

            analytics_row = analytics_result.iloc[0]

            # Build comprehensive analytics response
            analytics = {
                "summary": {
                    "date_range_days": date_range_days,
                    "total_calls": int(analytics_row["TOTAL_CALLS"]),
                    "unique_users": int(analytics_row["UNIQUE_USERS"]),
                    "unique_accounts": int(analytics_row["UNIQUE_ACCOUNTS"]),
                    "avg_duration_minutes": (
                        round(analytics_row["AVG_DURATION_SECONDS"] / 60, 1)
                        if pd.notna(analytics_row["AVG_DURATION_SECONDS"])
                        else 0
                    ),
                    "avg_talk_ratio": (
                        round(analytics_row["AVG_TALK_RATIO"], 2)
                        if pd.notna(analytics_row["AVG_TALK_RATIO"])
                        else 0
                    ),
                    "avg_sentiment_score": (
                        round(analytics_row["AVG_SENTIMENT_SCORE"], 2)
                        if pd.notna(analytics_row["AVG_SENTIMENT_SCORE"])
                        else 0
                    ),
                },
                "call_direction": {
                    "inbound": int(analytics_row["INBOUND_CALLS"]),
                    "outbound": int(analytics_row["OUTBOUND_CALLS"]),
                },
                "sentiment_distribution": {
                    "positive": int(analytics_row["POSITIVE_SENTIMENT_CALLS"]),
                    "negative": int(analytics_row["NEGATIVE_SENTIMENT_CALLS"]),
                    "neutral": int(analytics_row["NEUTRAL_SENTIMENT_CALLS"]),
                },
                "deal_metrics": {
                    "calls_with_deals": int(analytics_row["CALLS_WITH_DEALS"]),
                    "total_deal_value": (
                        float(analytics_row["TOTAL_DEAL_VALUE"])
                        if pd.notna(analytics_row["TOTAL_DEAL_VALUE"])
                        else 0
                    ),
                },
                "ai_memory_coverage": {
                    "calls_with_embeddings": int(
                        analytics_row["CALLS_WITH_EMBEDDINGS"]
                    ),
                    "calls_with_summaries": int(analytics_row["CALLS_WITH_SUMMARIES"]),
                    "calls_with_topics": int(analytics_row["CALLS_WITH_TOPICS"]),
                    "embedding_coverage_percent": round(
                        (
                            (
                                analytics_row["CALLS_WITH_EMBEDDINGS"]
                                / analytics_row["TOTAL_CALLS"]
                                * 100
                            )
                            if analytics_row["TOTAL_CALLS"] > 0
                            else 0
                        ),
                        1,
                    ),
                },
            }

            # Add AI insights if requested
            if include_ai_insights:
                insights_sql = f"""
                SELECT
                    -- Top topics analysis
                    FLATTEN(KEY_TOPICS) as topic_data,
                    COUNT(*) as topic_frequency
                FROM STG_TRANSFORMED.STG_GONG_CALLS
                WHERE CALL_DATETIME_UTC >= DATEADD(day, -{date_range_days}, CURRENT_TIMESTAMP())
                AND KEY_TOPICS IS NOT NULL
                GROUP BY topic_data.value
                ORDER BY topic_frequency DESC
                LIMIT 10
                """

                try:
                    topics_result = await self.execute_query(insights_sql)

                    if not topics_result.empty:
                        analytics["ai_insights"] = {
                            "top_topics": [
                                {
                                    "topic": row["TOPIC_DATA"],
                                    "frequency": int(row["TOPIC_FREQUENCY"]),
                                }
                                for _, row in topics_result.iterrows()
                            ]
                        }
                except Exception as e:
                    logger.warning(f"Could not generate AI insights: {e}")
                    analytics["ai_insights"] = {"error": "AI insights unavailable"}

            analytics["generated_at"] = datetime.now(UTC).isoformat()

            logger.info(
                f"Generated Gong call analytics for {date_range_days} days: {analytics['summary']['total_calls']} calls analyzed"
            )
            return analytics

        except Exception as e:
            logger.error(f"Error generating Gong call analytics: {e}")
            return {"error": str(e)}

    async def log_etl_job_status(self, job_log: dict[str, Any]) -> bool:
        """
        Log ETL job status to operational monitoring table

        Args:
            job_log: Dictionary containing job status information

        Returns:
            True if successful, False otherwise
        """
        if not self.initialized:
            await self.initialize()

        # Prepare job log data
        log_data = {
            "job_id": job_log.get("job_id", str(uuid.uuid4())),
            "job_name": job_log.get("job_name", "Unknown"),
            "job_type": job_log.get("job_type", "ETL"),
            "status": job_log.get("status", "RUNNING"),
            "start_time": job_log.get("start_time"),
            "end_time": job_log.get("end_time"),
            "records_processed": job_log.get("records_processed", 0),
            "error_message": job_log.get("error_message"),
            "metadata": json.dumps(job_log.get("metadata", {})),
        }

        insert_query = """
        INSERT INTO OPS_MONITORING.ETL_JOB_LOGS
        (job_id, job_name, job_type, status, start_time, end_time, 
         records_processed, error_message, metadata)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """

        cursor = None
        try:
            pool = self.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
            if not pool:
                raise ValueError("Snowflake connection pool not available")
                
            async with pool.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(
                    insert_query,
                    (
                        log_data["job_id"],
                        log_data["job_name"],
                        log_data["job_type"],
                        log_data["status"],
                        log_data["start_time"],
                        log_data["end_time"],
                        log_data["records_processed"],
                        log_data["error_message"],
                        log_data["metadata"],
                    ),
                )
                logger.info(f"✅ Logged ETL job status: {log_data['job_id']}")
                return True

        except Exception as e:
            logger.error(f"❌ Failed to log ETL job status: {e}")
            return False
        finally:
            if cursor:
                cursor.close()

    async def get_connection(self):
        """Get a database connection from the connection manager"""
        pool = self.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
        if not pool:
            raise ValueError("Snowflake connection pool not available")
            
        # Return the connection context manager
        return pool.get_connection()

    async def execute_query(self, query: str, params: tuple | None = None):
        """Execute a query using the connection manager"""
        return await self.connection_manager.execute_query(query, params)


# Global instance
_cortex_service: SnowflakeCortexService | None = None


async def get_cortex_service() -> SnowflakeCortexService:
    """Get or create global Cortex service instance"""
    global _cortex_service
    if _cortex_service is None:
        _cortex_service = SnowflakeCortexService()
        await _cortex_service.initialize()
    return _cortex_service


async def summarize_hubspot_contact_notes(
    contact_id: str, max_length: int = 150
) -> dict[str, Any]:
    """
    Summarize all notes for a HubSpot contact using Cortex AI

    Args:
        contact_id: HubSpot contact ID
        max_length: Maximum summary length

    Returns:
        Summary result with AI-generated content
    """
    service = await get_cortex_service()

    results = await service.summarize_text_in_snowflake(
        text_column="note_content",
        table_name="HUBSPOT_CONTACT_NOTES",
        conditions=f"contact_id = '{contact_id}'",
        max_length=max_length,
    )

    return {"contact_id": contact_id, "summaries": results}


async def analyze_gong_call_sentiment(call_id: str) -> dict[str, Any]:
    """
    Analyze sentiment for a Gong call transcript

    Args:
        call_id: Gong call ID

    Returns:
        Sentiment analysis results
    """
    service = await get_cortex_service()

    # Analyze overall call sentiment
    call_sentiment = await service.analyze_sentiment_in_snowflake(
        text_column="transcript_text",
        table_name="GONG_CALL_TRANSCRIPTS",
        conditions=f"call_id = '{call_id}'",
    )

    # Analyze sentiment by speaker
    speaker_sentiment_query = f"""
    SELECT
        speaker_name,
        speaker_type,
        AVG(SNOWFLAKE.CORTEX.SENTIMENT(transcript_segment)) as avg_sentiment,
        COUNT(*) as segment_count
    FROM GONG_CALL_TRANSCRIPT_SEGMENTS
    WHERE call_id = '{call_id}'
    GROUP BY speaker_name, speaker_type
    """

    cursor = None
    try:
        pool = service.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
        if not pool:
            raise ValueError("Snowflake connection pool not available")
            
        async with pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(speaker_sentiment_query)
            speaker_results = cursor.fetchall()

            speaker_sentiment = []
            for row in speaker_results:
                speaker_sentiment.append(
                    {
                        "speaker_name": row[0],
                        "speaker_type": row[1],
                        "avg_sentiment": row[2],
                        "segment_count": row[3],
                    }
                )

            return {
                "call_id": call_id,
                "overall_sentiment": call_sentiment,
                "speaker_sentiment": speaker_sentiment,
            }

    except Exception as e:
        logger.error(f"Error analyzing Gong call sentiment: {e}")
        return {"call_id": call_id, "error": str(e)}
    finally:
        if cursor:
            cursor.close()


async def summarize_gong_call_with_context(
    call_id: str, max_length: int = 300
) -> dict[str, Any]:
    """
    Generate comprehensive summary of Gong call with business context

    Args:
        call_id: Gong call ID
        max_length: Maximum summary length

    Returns:
        Comprehensive call summary with context
    """
    service = await get_cortex_service()

    # Get call metadata for context
    metadata_query = f"""
    SELECT
        c.call_title,
        c.call_date,
        c.duration_minutes,
        c.account_name,
        c.opportunity_name,
        c.opportunity_stage,
        c.opportunity_amount,
        GROUP_CONCAT(p.participant_name) as participants
    FROM GONG_CALLS c
    LEFT JOIN GONG_CALL_PARTICIPANTS p ON c.call_id = p.call_id
    WHERE c.call_id = '{call_id}'
    GROUP BY c.call_id, c.call_title, c.call_date, c.duration_minutes,
             c.account_name, c.opportunity_name, c.opportunity_stage, c.opportunity_amount
    """

    cursor = None
    try:
        pool = service.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
        if not pool:
            raise ValueError("Snowflake connection pool not available")
            
        async with pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(metadata_query)
            metadata = cursor.fetchone()

            if not metadata:
                return {"call_id": call_id, "error": "Call not found"}

            # Build context for summary
            context = f"""
            Call: {metadata[0]}
            Date: {metadata[1]}
            Duration: {metadata[2]} minutes
            Account: {metadata[3]}
            Opportunity: {metadata[4]} (Stage: {metadata[5]}, Amount: ${metadata[6]:,.2f})
            Participants: {metadata[7]}
            """

            # Generate AI summary with context
            summary_query = f"""
            SELECT SNOWFLAKE.CORTEX.SUMMARIZE(
                CONCAT('{context}\\n\\nTranscript:\\n', transcript_text),
                {max_length}
            ) as ai_summary
            FROM GONG_CALL_TRANSCRIPTS
            WHERE call_id = '{call_id}'
            """

            cursor.execute(summary_query)
            summary_result = cursor.fetchone()

            # Get key topics and action items
            topics_query = f"""
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                'mistral-7b',
                CONCAT('Extract key topics and action items from this call transcript:\\n', transcript_text),
                {{'max_tokens': 200}}
            ) as key_topics_actions
            FROM GONG_CALL_TRANSCRIPTS
            WHERE call_id = '{call_id}'
            """

            cursor.execute(topics_query)
            topics_result = cursor.fetchone()

            return {
                "call_id": call_id,
                "metadata": {
                    "title": metadata[0],
                    "date": metadata[1],
                    "duration_minutes": metadata[2],
                    "account": metadata[3],
                    "opportunity": {
                        "name": metadata[4],
                        "stage": metadata[5],
                        "amount": metadata[6],
                    },
                    "participants": metadata[7].split(",") if metadata[7] else [],
                },
                "ai_summary": summary_result[0] if summary_result else None,
                "key_topics_actions": topics_result[0] if topics_result else None,
            }

    except Exception as e:
        logger.error(f"Error summarizing Gong call: {e}")
        return {"call_id": call_id, "error": str(e)}
    finally:
        if cursor:
            cursor.close()


async def find_similar_gong_calls(
    query_text: str,
    top_k: int = 5,
    similarity_threshold: float = 0.7,
    date_range_days: int = 90,
) -> list[dict[str, Any]]:
    """
    Find similar Gong calls based on semantic search

    Args:
        query_text: Search query
        top_k: Number of results to return
        similarity_threshold: Minimum similarity score
        date_range_days: Search within last N days

    Returns:
        List of similar calls with metadata
    """
    service = await get_cortex_service()

    # Generate embedding for search query
    embed_query = f"SELECT SNOWFLAKE.CORTEX.EMBED_TEXT('e5-base-v2', '{query_text}') as query_embedding"

    cursor = None
    try:
        pool = service.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
        if not pool:
            raise ValueError("Snowflake connection pool not available")
            
        async with pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(embed_query)
            embedding_result = cursor.fetchone()

            if not embedding_result:
                return []

            # Search for similar calls
            search_query = f"""
            WITH query_embedding AS (
                SELECT SNOWFLAKE.CORTEX.EMBED_TEXT('e5-base-v2', '{query_text}') as embedding
            )
            SELECT
                c.call_id,
                c.call_title,
                c.call_date,
                c.account_name,
                c.opportunity_name,
                c.opportunity_stage,
                t.transcript_summary,
                VECTOR_COSINE_SIMILARITY(t.transcript_embedding, q.embedding) as similarity_score
            FROM GONG_CALLS c
            JOIN GONG_CALL_TRANSCRIPTS t ON c.call_id = t.call_id
            CROSS JOIN query_embedding q
            WHERE c.call_date >= DATEADD(day, -{date_range_days}, CURRENT_DATE())
              AND t.transcript_embedding IS NOT NULL
              AND VECTOR_COSINE_SIMILARITY(t.transcript_embedding, q.embedding) >= {similarity_threshold}
            ORDER BY similarity_score DESC
            LIMIT {top_k}
            """

            cursor.execute(search_query)
            results = cursor.fetchall()

            similar_calls = []
            for row in results:
                similar_calls.append(
                    {
                        "call_id": row[0],
                        "call_title": row[1],
                        "call_date": row[2],
                        "account_name": row[3],
                        "opportunity_name": row[4],
                        "opportunity_stage": row[5],
                        "transcript_summary": row[6],
                        "similarity_score": row[7],
                    }
                )

            return similar_calls

    except Exception as e:
        logger.error(f"Error finding similar Gong calls: {e}")
        return []
    finally:
        if cursor:
            cursor.close()


async def get_gong_coaching_insights(
    sales_rep: str, date_range_days: int = 30, min_calls: int = 3
) -> dict[str, Any]:
    """
    Generate AI-powered coaching insights for a sales rep

    Args:
        sales_rep: Sales rep name or email
        date_range_days: Analysis period in days
        min_calls: Minimum calls for analysis

    Returns:
        Coaching insights and recommendations
    """
    service = await get_cortex_service()

    # Analyze call patterns and sentiment
    analysis_query = f"""
    WITH rep_calls AS (
        SELECT
            c.call_id,
            c.call_date,
            c.duration_minutes,
            c.opportunity_stage,
            t.sentiment_score,
            t.talk_ratio,
            t.questions_asked,
            t.objections_handled
        FROM GONG_CALLS c
        JOIN GONG_CALL_ANALYTICS t ON c.call_id = t.call_id
        WHERE c.primary_participant = '{sales_rep}'
          AND c.call_date >= DATEADD(day, -{date_range_days}, CURRENT_DATE())
    )
    SELECT
        COUNT(*) as total_calls,
        AVG(duration_minutes) as avg_duration,
        AVG(sentiment_score) as avg_sentiment,
        AVG(talk_ratio) as avg_talk_ratio,
        AVG(questions_asked) as avg_questions,
        AVG(objections_handled) as avg_objections,
        SUM(CASE WHEN sentiment_score > 0.2 THEN 1 ELSE 0 END) as positive_calls,
        SUM(CASE WHEN sentiment_score < -0.2 THEN 1 ELSE 0 END) as negative_calls
    FROM rep_calls
    """

    cursor = None
    try:
        pool = service.connection_manager.pools.get(ConnectionType.SNOWFLAKE)
        if not pool:
            raise ValueError("Snowflake connection pool not available")
            
        async with pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(analysis_query)
            stats = cursor.fetchone()

            if not stats or stats[0] < min_calls:
                return {
                    "sales_rep": sales_rep,
                    "error": f"Insufficient call data (found {stats[0] if stats else 0} calls, need {min_calls})",
                }

            # Generate AI coaching recommendations
            coaching_prompt = f"""
            Based on the following sales performance data, provide specific coaching recommendations:
            - Total calls: {stats[0]}
            - Average duration: {stats[1]:.1f} minutes
            - Average sentiment: {stats[2]:.2f}
            - Average talk ratio: {stats[3]:.2%}
            - Average questions per call: {stats[4]:.1f}
            - Average objections handled: {stats[5]:.1f}
            - Positive sentiment calls: {stats[6]}
            - Negative sentiment calls: {stats[7]}
            
            Provide 3-5 specific, actionable coaching recommendations.
            """

            recommendations_query = f"""
            SELECT SNOWFLAKE.CORTEX.COMPLETE(
                'mistral-7b',
                '{coaching_prompt}',
                {{'max_tokens': 300}}
            ) as coaching_recommendations
            """

            cursor.execute(recommendations_query)
            recommendations = cursor.fetchone()

            # Get specific call examples for coaching
            examples_query = f"""
            SELECT
                c.call_id,
                c.call_title,
                c.call_date,
                t.sentiment_score,
                t.transcript_summary
            FROM GONG_CALLS c
            JOIN GONG_CALL_TRANSCRIPTS t ON c.call_id = t.call_id
            WHERE c.primary_participant = '{sales_rep}'
              AND c.call_date >= DATEADD(day, -{date_range_days}, CURRENT_DATE())
            ORDER BY 
                CASE 
                    WHEN t.sentiment_score < -0.3 THEN 1  -- Prioritize negative calls
                    WHEN t.talk_ratio > 0.7 THEN 2       -- High talk ratio
                    ELSE 3
                END,
                c.call_date DESC
            LIMIT 3
            """

            cursor.execute(examples_query)
            example_calls = cursor.fetchall()

            coaching_examples = []
            for call in example_calls:
                coaching_examples.append(
                    {
                        "call_id": call[0],
                        "title": call[1],
                        "date": call[2],
                        "sentiment": call[3],
                        "summary": call[4],
                        "coaching_focus": _get_coaching_focus(call[3]),
                    }
                )

            return {
                "sales_rep": sales_rep,
                "period_days": date_range_days,
                "performance_stats": {
                    "total_calls": stats[0],
                    "avg_duration_minutes": round(stats[1], 1),
                    "avg_sentiment_score": round(stats[2], 2),
                    "avg_talk_ratio_percent": round(stats[3] * 100, 1),
                    "avg_questions_per_call": round(stats[4], 1),
                    "avg_objections_handled": round(stats[5], 1),
                    "positive_calls": stats[6],
                    "negative_calls": stats[7],
                },
                "performance_rating": _calculate_performance_rating(stats),
                "ai_coaching_recommendations": (
                    recommendations[0] if recommendations else None
                ),
                "example_calls_for_review": coaching_examples,
            }

    except Exception as e:
        logger.error(f"Error generating coaching insights: {e}")
        return {"sales_rep": sales_rep, "error": str(e)}
    finally:
        if cursor:
            cursor.close()


def _get_coaching_focus(sentiment_score: float) -> str:
    """Determine coaching focus based on sentiment"""
    if sentiment_score < -0.3:
        return "Improve rapport building and objection handling"
    elif sentiment_score < 0:
        return "Focus on positive framing and value articulation"
    elif sentiment_score < 0.3:
        return "Good baseline - work on enthusiasm and energy"
    else:
        return "Excellent sentiment - maintain approach"


def _calculate_performance_rating(stats: tuple) -> dict[str, Any]:
    """Calculate overall performance rating"""
    # Simple scoring based on key metrics
    sentiment_score = (stats[2] + 1) / 2 * 40  # Normalize to 0-40
    talk_ratio_score = (1 - abs(stats[3] - 0.4)) * 30  # Optimal around 40%
    questions_score = min(stats[4] / 10, 1) * 20  # More questions = better
    positive_ratio = stats[6] / max(stats[0], 1) * 10

    total_score = sentiment_score + talk_ratio_score + questions_score + positive_ratio

    if total_score >= 80:
        rating = "Excellent"
    elif total_score >= 65:
        rating = "Good"
    elif total_score >= 50:
        rating = "Needs Improvement"
    else:
        rating = "Requires Coaching"

    return {"rating": rating, "score": round(total_score, 1), "max_score": 100}
