"""
Orchestrator v4 API Routes - The Future of Sophia AI

This module exposes the new SophiaUnifiedOrchestrator through modern API endpoints,
replacing all deprecated chat services with a unified, intelligent orchestration layer.

Features:
- Streaming responses for real-time interaction
- Unified memory integration with modern_stack
- Parallel MCP server orchestration
- ML-based intent classification
- Comprehensive monitoring and metrics
- n8n workflow automation integration

Date: July 10, 2025
"""

import asyncio
import json
import logging
from collections.abc import AsyncGenerator
from datetime import datetime
from typing import Any

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

from backend.services.sophia_unified_orchestrator import (
    get_unified_orchestrator,
)

# Import workflow routes
try:
    from backend.api.v4.workflows import router as workflows_router

    WORKFLOWS_AVAILABLE = True
except ImportError:
    WORKFLOWS_AVAILABLE = False
    workflows_router = None

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v4", tags=["orchestrator"])

# Include workflow routes if available
if WORKFLOWS_AVAILABLE and workflows_router:
    router.include_router(workflows_router)
    logger.info("âœ… Workflow automation routes included in v4 API")


# Simple auth mock for now - replace with proper auth when available
async def get_current_user():
    """Mock auth function - replace with proper implementation"""
    return {"id": "user_default", "name": "Default User", "role": "admin"}


class OrchestrationRequest(BaseModel):
    """Unified request model for all orchestration operations"""

    query: str = Field(..., description="Natural language query from user")
    context: dict[str, Any] = Field(
        default_factory=dict, description="Additional context for the query"
    )
    session_id: str = Field(..., description="Session ID for conversation tracking")
    stream: bool = Field(default=False, description="Enable streaming response")
    include_metrics: bool = Field(
        default=True, description="Include performance metrics in response"
    )


class OrchestrationResponse(BaseModel):
    """Unified response model from orchestrator"""

    response: str = Field(..., description="Generated response text")
    citations: list[dict[str, Any]] = Field(
        default_factory=list, description="Source citations"
    )
    sources: list[str] = Field(default_factory=list, description="MCP servers used")
    metadata: dict[str, Any] = Field(
        default_factory=dict, description="Response metadata and metrics"
    )
    session_id: str = Field(..., description="Session ID for tracking")


@router.post("/orchestrate", response_model=OrchestrationResponse)
async def orchestrate_request(
    request: OrchestrationRequest,
    current_user: dict = Depends(get_current_user),
) -> OrchestrationResponse:
    """
    Main orchestration endpoint - routes all queries through SophiaUnifiedOrchestrator

    This is the future-facing API that will replace /api/v3/chat/unified
    """
    orchestrator = get_unified_orchestrator()

    try:
        # Process request through unified orchestrator
        result = await orchestrator.process_request(
            query=request.query,
            user_id=current_user["id"],
            session_id=request.session_id,
            context=request.context,
        )

        # Transform to response model
        return OrchestrationResponse(
            response=result.get("response", ""),
            citations=result.get("citations", []),
            sources=result.get("sources", []),
            metadata=result.get("metadata", {}),
            session_id=request.session_id,
        )

    except Exception as e:
        logger.exception(f"Orchestration failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/orchestrate/stream")
async def orchestrate_stream(
    request: OrchestrationRequest,
    current_user: dict = Depends(get_current_user),
):
    """
    Streaming orchestration endpoint for real-time responses

    Returns Server-Sent Events (SSE) stream with progressive updates
    """
    orchestrator = get_unified_orchestrator()

    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            # Send initial acknowledgment
            yield f"data: {json.dumps({'type': 'start', 'session_id': request.session_id})}\n\n"

            # Process with streaming updates
            start_time = datetime.utcnow()

            # For now, process normally and stream the result
            # TODO: Implement true streaming in orchestrator
            result = await orchestrator.process_request(
                query=request.query,
                user_id=current_user["id"],
                session_id=request.session_id,
                context=request.context,
            )

            # Stream the response in chunks
            response_text = result.get("response", "")
            chunk_size = 50  # Characters per chunk

            for i in range(0, len(response_text), chunk_size):
                chunk = response_text[i : i + chunk_size]
                yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                await asyncio.sleep(0.01)  # Small delay for streaming effect

            # Send metadata
            yield f"data: {json.dumps({'type': 'metadata', 'data': result.get('metadata', {})})}\n\n"

            # Send citations if any
            if result.get("citations"):
                yield f"data: {json.dumps({'type': 'citations', 'data': result['citations']})}\n\n"

            # Send completion
            yield f"data: {json.dumps({'type': 'complete', 'duration': (datetime.utcnow() - start_time).total_seconds()})}\n\n"

        except Exception as e:
            logger.exception(f"Streaming orchestration failed: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable proxy buffering
        },
    )


@router.get("/orchestrator/health")
async def get_orchestrator_health():
    """Get health status of the orchestrator and all connected services"""
    orchestrator = get_unified_orchestrator()

    try:
        # Get orchestrator metrics
        metrics = orchestrator.get_metrics()

        # Get memory service health
        memory_health = orchestrator.memory_service.health_check()

        # TODO: Get MCP orchestrator health when connected
        mcp_health = {"status": "not_connected", "reason": "MCP integration pending"}

        return {
            "status": "healthy" if memory_health["status"] == "healthy" else "degraded",
            "orchestrator": {
                "initialized": orchestrator.initialized,
                "metrics": metrics,
            },
            "memory_service": memory_health,
            "mcp_orchestrator": mcp_health,
            "timestamp": datetime.utcnow().isoformat(),
        }

    except Exception as e:
        logger.exception(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat(),
        }


@router.get("/orchestrator/metrics")
async def get_orchestrator_metrics(
    current_user: dict = Depends(get_current_user),
):
    """Get detailed metrics from the orchestrator"""
    orchestrator = get_unified_orchestrator()

    return {
        "metrics": orchestrator.get_metrics(),
        "user_id": current_user["id"],
        "timestamp": datetime.utcnow().isoformat(),
    }


# Backward compatibility endpoint
@router.post("/chat/unified")
async def legacy_chat_endpoint(
    request: dict[str, Any],
    current_user: dict = Depends(get_current_user),
):
    """
    Backward compatibility endpoint that redirects to new orchestrator

    This ensures existing frontend code continues to work during migration
    """
    # Transform legacy request to new format
    orchestration_request = OrchestrationRequest(
        query=request.get("message", ""),
        context={"legacy_context": request.get("context", "chat")},
        session_id=request.get("sessionId", f"legacy_{datetime.utcnow().timestamp()}"),
        stream=False,
    )

    # Process through new orchestrator
    response = await orchestrate_request(orchestration_request, current_user)

    # Transform to legacy response format
    return {
        "response": response.response,
        "citations": response.citations,
        "metadata": response.metadata,
    }
