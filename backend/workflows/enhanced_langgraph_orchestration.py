#!/usr/bin/env python3
"""
Enhanced LangGraph Agent Orchestration for Sophia AI
Advanced multi-agent workflows with KB management, Slack analysis, and Linear project health
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 1685 lines

Recommended decomposition:
- enhanced_langgraph_orchestration_core.py - Core functionality
- enhanced_langgraph_orchestration_utils.py - Utility functions  
- enhanced_langgraph_orchestration_models.py - Data models
- enhanced_langgraph_orchestration_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, TypedDict
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

# LangGraph imports
try:
    from langgraph.graph import StateGraph, END
    from langgraph.prebuilt import ToolExecutor

    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    StateGraph = None
    END = None

# Core imports
from backend.utils.snowflake_cortex_service import SnowflakeCortexService
from backend.utils.snowflake_hubspot_connector import SnowflakeHubSpotConnector
from backend.utils.snowflake_gong_connector import SnowflakeGongConnector
from backend.mcp_servers.enhanced_ai_memory_mcp_server import EnhancedAiMemoryMCPServer
from backend.agents.specialized.sales_coach_agent import SalesCoachAgent
from backend.agents.specialized.call_analysis_agent import CallAnalysisAgent
from backend.agents.specialized.slack_analysis_agent import SlackAnalysisAgent
from backend.agents.specialized.linear_project_health_agent import (
    LinearProjectHealthAgent,
)
from backend.services.kb_management_service import KBManagementService
from backend.agents.specialized.marketing_analysis_agent import MarketingAnalysisAgent
from backend.agents.specialized.sales_intelligence_agent import SalesIntelligenceAgent
from backend.services.smart_ai_service import SmartAIService

logger = logging.getLogger(__name__)


class WorkflowType(Enum):
    """Types of enhanced workflows"""

    DEAL_ANALYSIS = "deal_analysis"
    CROSS_SOURCE_ANALYSIS = "cross_source_analysis"
    BUSINESS_INTELLIGENCE = "business_intelligence"
    DEVELOPMENT_INSIGHTS = "development_insights"
    KNOWLEDGE_SYNTHESIS = "knowledge_synthesis"
    PROJECT_HEALTH_MONITORING = "project_health_monitoring"
    SLACK_INTELLIGENCE = "slack_intelligence"
    KB_MANAGEMENT = "kb_management"
    # New workflow types for Marketing and Sales Intelligence
    MARKETING_INTELLIGENCE = "marketing_intelligence"
    SALES_INTELLIGENCE = "sales_intelligence"
    CAMPAIGN_OPTIMIZATION = "campaign_optimization"
    DEAL_RISK_ASSESSMENT = "deal_risk_assessment"
    COMPETITIVE_ANALYSIS = "competitive_analysis"
    REVENUE_INTELLIGENCE = "revenue_intelligence"


class WorkflowState(TypedDict):
    """Enhanced workflow state with new data sources"""

    # Original fields
    query: str
    workflow_type: str
    deal_id: Optional[str]
    hubspot_data: Optional[Dict[str, Any]]
    gong_data: Optional[Dict[str, Any]]

    # Enhanced fields for new integrations
    slack_data: Optional[Dict[str, Any]]
    linear_data: Optional[Dict[str, Any]]
    kb_data: Optional[Dict[str, Any]]

    # New fields for Marketing and Sales Intelligence
    marketing_data: Optional[Dict[str, Any]]
    sales_data: Optional[Dict[str, Any]]
    campaign_data: Optional[Dict[str, Any]]
    competitive_data: Optional[Dict[str, Any]]

    # Analysis results
    call_analysis: Optional[Dict[str, Any]]
    deal_insights: Optional[Dict[str, Any]]
    coaching_recommendations: Optional[Dict[str, Any]]
    slack_insights: Optional[Dict[str, Any]]
    linear_health: Optional[Dict[str, Any]]
    kb_insights: Optional[Dict[str, Any]]

    # New analysis results for Marketing and Sales Intelligence
    marketing_insights: Optional[Dict[str, Any]]
    sales_intelligence: Optional[Dict[str, Any]]
    campaign_analysis: Optional[Dict[str, Any]]
    competitive_analysis: Optional[Dict[str, Any]]
    content_recommendations: Optional[Dict[str, Any]]
    deal_risk_assessment: Optional[Dict[str, Any]]

    # Workflow control
    next_action: str
    error: Optional[str]
    final_response: Optional[Dict[str, Any]]


@dataclass
class WorkflowRequest:
    """Enhanced workflow request"""

    query: str
    workflow_type: WorkflowType
    parameters: Dict[str, Any] = field(default_factory=dict)
    user_context: Optional[Dict[str, Any]] = None
    data_sources: List[str] = field(
        default_factory=lambda: ["hubspot", "gong", "slack", "linear", "kb"]
    )


@dataclass
class WorkflowResult:
    """Enhanced workflow result"""

    success: bool
    workflow_type: WorkflowType
    insights: Dict[str, Any]
    recommendations: List[str]
    data_sources_used: List[str]
    processing_time: float
    confidence_score: float
    error: Optional[str] = None


# Enhanced Agent Classes


@dataclass
class SlackAnalysisAgent:
    """Enhanced Slack Analysis Agent for LangGraph integration"""

    name: str = "slack_analysis_agent"
    description: str = "Analyzes Slack conversations for business insights"

    # Service integrations
    cortex_service: Optional[SnowflakeCortexService] = None
    ai_memory: Optional[EnhancedAiMemoryMCPServer] = None
    slack_agent: Optional[SlackAnalysisAgent] = None

    initialized: bool = False

    async def initialize(self) -> None:
        """Initialize Slack Analysis Agent"""
        if self.initialized:
            return

        try:
            from backend.agents.specialized.slack_analysis_agent import (
                SlackAnalysisAgent as SlackAgent,
            )

            self.cortex_service = SnowflakeCortexService()
            self.ai_memory = EnhancedAiMemoryMCPServer()
            self.slack_agent = SlackAgent()

            await self.ai_memory.initialize()
            await self.slack_agent.initialize()

            self.initialized = True
            logger.info("✅ Slack Analysis Agent initialized for LangGraph")

        except Exception as e:
            logger.error(f"Failed to initialize Slack Analysis Agent: {e}")
            raise

    async def analyze_slack_conversations(self, state: WorkflowState) -> WorkflowState:
        """Analyze Slack conversations for the workflow"""
        if not self.initialized:
            await self.initialize()

        try:
            # Get Slack data from query or parameters
            slack_query = state.get("query", "")

            # Query Slack conversations (placeholder - would query STG_SLACK_CONVERSATIONS)
            conversations = await self._get_relevant_slack_conversations(slack_query)

            if not conversations:
                state["slack_insights"] = {
                    "message": "No relevant Slack conversations found",
                    "conversations_analyzed": 0,
                }
                return state

            # Analyze conversations
            analysis_results = []
            for conversation in conversations[:5]:  # Limit to top 5 conversations
                try:
                    result = await self.slack_agent.analyze_conversation(conversation)
                    analysis_results.append(result)
                except Exception as e:
                    logger.warning(
                        f"Failed to analyze conversation {conversation.conversation_id}: {e}"
                    )

            # Aggregate insights
            slack_insights = self._aggregate_slack_insights(analysis_results)

            state["slack_data"] = {
                "conversations": [c.conversation_id for c in conversations],
                "total_analyzed": len(analysis_results),
            }
            state["slack_insights"] = slack_insights

            logger.info(f"Analyzed {len(analysis_results)} Slack conversations")
            return state

        except Exception as e:
            logger.error(f"Error in Slack analysis: {e}")
            state["error"] = f"Slack analysis failed: {str(e)}"
            return state

    async def _get_relevant_slack_conversations(self, query: str) -> List[Any]:
        """Get relevant Slack conversations (placeholder implementation)"""
        # In production, this would query STG_SLACK_CONVERSATIONS table
        # For now, return empty list
        return []

    def _aggregate_slack_insights(self, results: List[Any]) -> Dict[str, Any]:
        """Aggregate insights from multiple Slack conversation analyses"""
        if not results:
            return {"message": "No analysis results to aggregate"}

        # Calculate aggregate metrics
        total_conversations = len(results)
        avg_sentiment = sum(r.overall_sentiment for r in results) / total_conversations
        avg_business_value = (
            sum(r.business_value_score for r in results) / total_conversations
        )

        # Collect all topics and insights
        all_topics = []
        all_insights = []
        all_action_items = []

        for result in results:
            all_topics.extend(result.key_topics)
            all_insights.extend([insight.summary for insight in result.insights])
            all_action_items.extend(result.action_items)

        # Count topic frequency
        topic_counts = {}
        for topic in all_topics:
            topic_counts[topic] = topic_counts.get(topic, 0) + 1

        top_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:5]

        return {
            "conversations_analyzed": total_conversations,
            "average_sentiment": avg_sentiment,
            "average_business_value": avg_business_value,
            "top_topics": [
                {"topic": topic, "frequency": freq} for topic, freq in top_topics
            ],
            "key_insights": all_insights[:10],
            "action_items": all_action_items[:10],
            "sentiment_summary": (
                "positive"
                if avg_sentiment > 0.1
                else "negative"
                if avg_sentiment < -0.1
                else "neutral"
            ),
        }


@dataclass
class LinearAnalysisAgent:
    """Enhanced Linear Analysis Agent for LangGraph integration"""

    name: str = "linear_analysis_agent"
    description: str = "Analyzes Linear project health and development insights"

    # Service integrations
    cortex_service: Optional[SnowflakeCortexService] = None
    ai_memory: Optional[EnhancedAiMemoryMCPServer] = None
    linear_agent: Optional[LinearProjectHealthAgent] = None

    initialized: bool = False

    async def initialize(self) -> None:
        """Initialize Linear Analysis Agent"""
        if self.initialized:
            return

        try:
            from backend.agents.specialized.linear_project_health_agent import (
                LinearProjectHealthAgent,
            )

            self.cortex_service = SnowflakeCortexService()
            self.ai_memory = EnhancedAiMemoryMCPServer()
            self.linear_agent = LinearProjectHealthAgent()

            await self.ai_memory.initialize()
            await self.linear_agent.initialize()

            self.initialized = True
            logger.info("✅ Linear Analysis Agent initialized for LangGraph")

        except Exception as e:
            logger.error(f"Failed to initialize Linear Analysis Agent: {e}")
            raise

    async def analyze_project_health(self, state: WorkflowState) -> WorkflowState:
        """Analyze Linear project health for the workflow"""
        if not self.initialized:
            await self.initialize()

        try:
            # Get Linear data from query or parameters
            project_query = state.get("query", "")

            # Query Linear projects and issues (placeholder - would query STG_LINEAR_ISSUES)
            projects = await self._get_relevant_linear_projects(project_query)

            if not projects:
                state["linear_health"] = {
                    "message": "No relevant Linear projects found",
                    "projects_analyzed": 0,
                }
                return state

            # Analyze project health
            health_reports = []
            for project in projects[:3]:  # Limit to top 3 projects
                try:
                    issues = await self._get_project_issues(project["project_id"])
                    health_report = await self.linear_agent.assess_project_health(
                        project_id=project["project_id"],
                        project_name=project["project_name"],
                        issues=issues,
                    )
                    health_reports.append(health_report)
                except Exception as e:
                    logger.warning(
                        f"Failed to analyze project {project['project_id']}: {e}"
                    )

            # Aggregate health insights
            linear_insights = self._aggregate_linear_insights(health_reports)

            state["linear_data"] = {
                "projects": [p["project_id"] for p in projects],
                "total_analyzed": len(health_reports),
            }
            state["linear_health"] = linear_insights

            logger.info(f"Analyzed {len(health_reports)} Linear projects")
            return state

        except Exception as e:
            logger.error(f"Error in Linear analysis: {e}")
            state["error"] = f"Linear analysis failed: {str(e)}"
            return state

    async def _get_relevant_linear_projects(self, query: str) -> List[Dict[str, Any]]:
        """Get relevant Linear projects (placeholder implementation)"""
        # In production, this would query STG_LINEAR_PROJECTS table
        return []

    async def _get_project_issues(self, project_id: str) -> List[Any]:
        """Get issues for a Linear project (placeholder implementation)"""
        # In production, this would query STG_LINEAR_ISSUES table
        return []

    def _aggregate_linear_insights(self, reports: List[Any]) -> Dict[str, Any]:
        """Aggregate insights from multiple Linear project health reports"""
        if not reports:
            return {"message": "No health reports to aggregate"}

        # Calculate aggregate metrics
        total_projects = len(reports)
        avg_health_score = sum(r.health_score for r in reports) / total_projects

        # Count health statuses
        status_counts = {}
        for report in reports:
            status = report.health_status.value
            status_counts[status] = status_counts.get(status, 0) + 1

        # Collect all risks and recommendations
        all_risks = []
        all_recommendations = []

        for report in reports:
            all_risks.extend(
                [
                    {"type": r.risk_type.value, "severity": r.severity}
                    for r in report.risks
                ]
            )
            all_recommendations.extend(report.recommendations)

        # Count risk types
        risk_counts = {}
        for risk in all_risks:
            risk_type = risk["type"]
            risk_counts[risk_type] = risk_counts.get(risk_type, 0) + 1

        return {
            "projects_analyzed": total_projects,
            "average_health_score": avg_health_score,
            "health_status_distribution": status_counts,
            "common_risks": sorted(
                risk_counts.items(), key=lambda x: x[1], reverse=True
            )[:5],
            "top_recommendations": list(set(all_recommendations))[:10],
            "overall_health": (
                "healthy"
                if avg_health_score > 0.7
                else "at_risk"
                if avg_health_score > 0.5
                else "critical"
            ),
        }


@dataclass
class KnowledgeCuratorAgent:
    """Enhanced Knowledge Curator Agent for LangGraph integration"""

    name: str = "knowledge_curator_agent"
    description: str = "Manages and curates knowledge base content"

    # Service integrations
    cortex_service: Optional[SnowflakeCortexService] = None
    ai_memory: Optional[EnhancedAiMemoryMCPServer] = None
    kb_service: Optional[KBManagementService] = None

    initialized: bool = False

    async def initialize(self) -> None:
        """Initialize Knowledge Curator Agent"""
        if self.initialized:
            return

        try:
            self.cortex_service = SnowflakeCortexService()
            self.ai_memory = EnhancedAiMemoryMCPServer()
            self.kb_service = KBManagementService()

            await self.ai_memory.initialize()
            await self.kb_service.initialize()

            self.initialized = True
            logger.info("✅ Knowledge Curator Agent initialized for LangGraph")

        except Exception as e:
            logger.error(f"Failed to initialize Knowledge Curator Agent: {e}")
            raise

    async def curate_knowledge(self, state: WorkflowState) -> WorkflowState:
        """Curate knowledge from multiple sources for the workflow"""
        if not self.initialized:
            await self.initialize()

        try:
            # Extract knowledge from all available sources
            knowledge_sources = []

            # Extract from Slack insights
            if state.get("slack_insights"):
                slack_knowledge = self._extract_slack_knowledge(state["slack_insights"])
                knowledge_sources.append(
                    {"source": "slack", "knowledge": slack_knowledge}
                )

            # Extract from Linear insights
            if state.get("linear_health"):
                linear_knowledge = self._extract_linear_knowledge(
                    state["linear_health"]
                )
                knowledge_sources.append(
                    {"source": "linear", "knowledge": linear_knowledge}
                )

            # Extract from HubSpot/Gong data
            if state.get("deal_insights"):
                deal_knowledge = self._extract_deal_knowledge(state["deal_insights"])
                knowledge_sources.append(
                    {"source": "deals", "knowledge": deal_knowledge}
                )

            # Synthesize cross-source insights
            synthesized_knowledge = await self._synthesize_knowledge(knowledge_sources)

            state["kb_insights"] = {
                "sources_processed": len(knowledge_sources),
                "synthesized_insights": synthesized_knowledge,
                "knowledge_gaps": await self._identify_knowledge_gaps(
                    knowledge_sources
                ),
                "recommended_actions": await self._recommend_knowledge_actions(
                    synthesized_knowledge
                ),
            }

            logger.info(f"Curated knowledge from {len(knowledge_sources)} sources")
            return state

        except Exception as e:
            logger.error(f"Error in knowledge curation: {e}")
            state["error"] = f"Knowledge curation failed: {str(e)}"
            return state

    def _extract_slack_knowledge(self, slack_insights: Dict[str, Any]) -> List[str]:
        """Extract knowledge items from Slack insights"""
        knowledge = []

        # Extract from top topics
        for topic_data in slack_insights.get("top_topics", []):
            knowledge.append(f"Team frequently discusses: {topic_data['topic']}")

        # Extract from key insights
        knowledge.extend(slack_insights.get("key_insights", [])[:5])

        return knowledge

    def _extract_linear_knowledge(self, linear_health: Dict[str, Any]) -> List[str]:
        """Extract knowledge items from Linear health insights"""
        knowledge = []

        # Extract from common risks
        for risk_type, count in linear_health.get("common_risks", []):
            knowledge.append(
                f"Common project risk: {risk_type} (affects {count} projects)"
            )

        # Extract from recommendations
        knowledge.extend(linear_health.get("top_recommendations", [])[:5])

        return knowledge

    def _extract_deal_knowledge(self, deal_insights: Dict[str, Any]) -> List[str]:
        """Extract knowledge items from deal insights"""
        knowledge = []

        # Extract deal patterns and insights
        if "patterns" in deal_insights:
            knowledge.extend(deal_insights["patterns"][:3])

        if "insights" in deal_insights:
            knowledge.extend(deal_insights["insights"][:3])

        return knowledge

    async def _synthesize_knowledge(
        self, knowledge_sources: List[Dict[str, Any]]
    ) -> List[str]:
        """Synthesize knowledge across sources using AI"""
        try:
            if not knowledge_sources:
                return []

            # Combine all knowledge items
            all_knowledge = []
            for source in knowledge_sources:
                all_knowledge.extend(source["knowledge"])

            if not all_knowledge:
                return []

            async with self.cortex_service as cortex:
                synthesis_prompt = f"""
                Synthesize these business insights from multiple sources into key strategic knowledge:
                
                Knowledge Items:
                {chr(10).join([f"- {item}" for item in all_knowledge[:20]])}
                
                Generate 3-5 synthesized insights that connect patterns across sources.
                Focus on actionable business intelligence.
                """

                synthesized = await cortex.complete_text_with_cortex(
                    prompt=synthesis_prompt, max_tokens=300
                )

                # Parse synthesized insights
                insights = [
                    line.strip() for line in synthesized.split("\n") if line.strip()
                ]
                return insights[:5]

        except Exception as e:
            logger.error(f"Error synthesizing knowledge: {e}")
            return ["Knowledge synthesis failed"]

    async def _identify_knowledge_gaps(
        self, knowledge_sources: List[Dict[str, Any]]
    ) -> List[str]:
        """Identify gaps in knowledge coverage"""
        gaps = []

        source_types = set(source["source"] for source in knowledge_sources)

        if "slack" not in source_types:
            gaps.append("Missing team communication insights")

        if "linear" not in source_types:
            gaps.append("Missing development project insights")

        if "deals" not in source_types:
            gaps.append("Missing sales and deal insights")

        return gaps

    async def _recommend_knowledge_actions(
        self, synthesized_knowledge: List[str]
    ) -> List[str]:
        """Recommend actions based on synthesized knowledge"""
        actions = []

        if synthesized_knowledge:
            actions.append("Document key insights in knowledge base")
            actions.append("Share insights with relevant teams")
            actions.append("Create action items for identified opportunities")

        actions.append("Schedule regular knowledge synthesis sessions")

        return actions


# New Marketing and Sales Intelligence Agents for LangGraph Integration


@dataclass
class MarketingAnalysisLangGraphAgent:
    """Enhanced Marketing Analysis Agent for LangGraph integration"""

    name: str = "marketing_analysis_agent"
    description: str = (
        "Analyzes marketing campaigns and generates content recommendations"
    )

    # Service integrations
    cortex_service: Optional[SnowflakeCortexService] = None
    ai_memory: Optional[EnhancedAiMemoryMCPServer] = None
    marketing_agent: Optional[MarketingAnalysisAgent] = None
    smart_ai_service: Optional[SmartAIService] = None

    initialized: bool = False

    async def initialize(self) -> None:
        """Initialize Marketing Analysis Agent"""
        if self.initialized:
            return

        try:
            self.cortex_service = SnowflakeCortexService()
            self.ai_memory = EnhancedAiMemoryMCPServer()
            self.marketing_agent = MarketingAnalysisAgent()
            self.smart_ai_service = SmartAIService()

            await self.ai_memory.initialize()
            await self.marketing_agent.initialize()
            await self.smart_ai_service.initialize()

            self.initialized = True
            logger.info("✅ Marketing Analysis Agent initialized for LangGraph")

        except Exception as e:
            logger.error(f"Failed to initialize Marketing Analysis Agent: {e}")
            raise

    async def analyze_marketing_performance(
        self, state: WorkflowState
    ) -> WorkflowState:
        """Analyze marketing performance for the workflow"""
        if not self.initialized:
            await self.initialize()

        try:
            query = state.get("query", "")

            # Determine analysis type from query
            analysis_type = self._determine_marketing_analysis_type(query)

            # Get marketing data based on analysis type
            marketing_data = await self._get_marketing_data(analysis_type, query)

            if not marketing_data:
                state["marketing_insights"] = {
                    "message": "No relevant marketing data found",
                    "analysis_type": analysis_type,
                }
                return state

            # Perform marketing analysis using the specialized agent
            insights = await self.marketing_agent.comprehensive_marketing_analysis(
                campaigns=marketing_data.get("campaigns", []),
                customers=marketing_data.get("customers", []),
                competitors=marketing_data.get("competitors", []),
            )

            # Store marketing data and insights
            state["marketing_data"] = marketing_data
            state["marketing_insights"] = insights

            # Generate content recommendations if requested
            if "content" in query.lower() or analysis_type == "content_generation":
                content_recs = await self._generate_content_recommendations(
                    insights, query
                )
                state["content_recommendations"] = content_recs

            # Perform competitive analysis if requested
            if (
                "competitor" in query.lower()
                or analysis_type == "competitive_intelligence"
            ):
                competitive_analysis = await self._perform_competitive_analysis(
                    insights, query
                )
                state["competitive_analysis"] = competitive_analysis

            logger.info(f"Completed marketing analysis: {analysis_type}")
            return state

        except Exception as e:
            logger.error(f"Error in marketing analysis: {e}")
            state["error"] = f"Marketing analysis failed: {str(e)}"
            return state

    def _determine_marketing_analysis_type(self, query: str) -> str:
        """Determine the type of marketing analysis needed"""
        query_lower = query.lower()

        if "campaign" in query_lower and "performance" in query_lower:
            return "campaign_performance"
        elif "content" in query_lower or "generate" in query_lower:
            return "content_generation"
        elif "audience" in query_lower or "segment" in query_lower:
            return "audience_segmentation"
        elif "competitor" in query_lower or "competitive" in query_lower:
            return "competitive_intelligence"
        else:
            return "comprehensive"

    async def _get_marketing_data(
        self, analysis_type: str, query: str
    ) -> Dict[str, Any]:
        """Get relevant marketing data based on analysis type"""
        # In production, this would query STG_MARKETING_CAMPAIGNS, STG_CUSTOMER_DATA, etc.
        # For now, return structured data based on analysis needs

        data = {
            "campaigns": [],
            "customers": [],
            "competitors": [],
            "content_type": "email",
            "audience": "general",
            "context": {},
            "criteria": ["behavior", "demographics"],
            "focus": ["positioning", "messaging"],
        }

        # Parse specific parameters from query
        if "email" in query.lower():
            data["content_type"] = "email"
        elif "social" in query.lower():
            data["content_type"] = "social_media"
        elif "blog" in query.lower():
            data["content_type"] = "blog_post"

        return data

    async def _generate_content_recommendations(
        self, insights: Dict[str, Any], query: str
    ) -> Dict[str, Any]:
        """Generate content recommendations based on insights"""
        try:
            # Use SmartAIService for advanced content generation
            content_prompt = f"""
            Based on these marketing insights, generate content recommendations:
            
            Insights: {json.dumps(insights, indent=2)}
            Query: {query}
            
            Provide:
            1. Content themes and topics
            2. Messaging recommendations
            3. Channel-specific adaptations
            4. Call-to-action suggestions
            5. Performance optimization tips
            """

            content_recommendations = await self.smart_ai_service.generate_response(
                {
                    "prompt": content_prompt,
                    "task_type": "content_generation",
                    "model_preference": "balanced",
                    "max_tokens": 500,
                }
            )

            return {
                "recommendations": content_recommendations.get("response", ""),
                "generated_at": datetime.now().isoformat(),
                "source": "smart_ai_service",
            }

        except Exception as e:
            logger.error(f"Error generating content recommendations: {e}")
            return {"error": f"Content generation failed: {str(e)}"}

    async def _perform_competitive_analysis(
        self, insights: Dict[str, Any], query: str
    ) -> Dict[str, Any]:
        """Perform competitive analysis based on insights"""
        try:
            # Use SmartAIService for competitive intelligence
            competitive_prompt = f"""
            Analyze competitive landscape based on these insights:
            
            Marketing Insights: {json.dumps(insights, indent=2)}
            Query: {query}
            
            Provide:
            1. Competitive positioning analysis
            2. Market differentiation opportunities
            3. Messaging gap analysis
            4. Strategic recommendations
            5. Tactical next steps
            """

            competitive_analysis = await self.smart_ai_service.generate_response(
                {
                    "prompt": competitive_prompt,
                    "task_type": "competitive_analysis",
                    "model_preference": "performance",
                    "max_tokens": 600,
                }
            )

            return {
                "analysis": competitive_analysis.get("response", ""),
                "generated_at": datetime.now().isoformat(),
                "source": "smart_ai_service",
            }

        except Exception as e:
            logger.error(f"Error in competitive analysis: {e}")
            return {"error": f"Competitive analysis failed: {str(e)}"}


@dataclass
class SalesIntelligenceLangGraphAgent:
    """Enhanced Sales Intelligence Agent for LangGraph integration"""

    name: str = "sales_intelligence_agent"
    description: str = "Provides advanced sales intelligence and deal risk assessment"

    # Service integrations
    cortex_service: Optional[SnowflakeCortexService] = None
    ai_memory: Optional[EnhancedAiMemoryMCPServer] = None
    sales_agent: Optional[SalesIntelligenceAgent] = None
    smart_ai_service: Optional[SmartAIService] = None
    hubspot_connector: Optional[SnowflakeHubSpotConnector] = None
    gong_connector: Optional[SnowflakeGongConnector] = None

    initialized: bool = False

    async def initialize(self) -> None:
        """Initialize Sales Intelligence Agent"""
        if self.initialized:
            return

        try:
            self.cortex_service = SnowflakeCortexService()
            self.ai_memory = EnhancedAiMemoryMCPServer()
            self.sales_agent = SalesIntelligenceAgent()
            self.smart_ai_service = SmartAIService()
            self.hubspot_connector = SnowflakeHubSpotConnector()
            self.gong_connector = SnowflakeGongConnector()

            await self.ai_memory.initialize()
            await self.sales_agent.initialize()
            await self.smart_ai_service.initialize()

            self.initialized = True
            logger.info("✅ Sales Intelligence Agent initialized for LangGraph")

        except Exception as e:
            logger.error(f"Failed to initialize Sales Intelligence Agent: {e}")
            raise

    async def analyze_sales_intelligence(self, state: WorkflowState) -> WorkflowState:
        """Analyze sales intelligence for the workflow"""
        if not self.initialized:
            await self.initialize()

        try:
            query = state.get("query", "")
            deal_id = state.get("deal_id")

            # Determine analysis type from query
            analysis_type = self._determine_sales_analysis_type(query)

            # Get sales data
            sales_data = await self._get_sales_data(analysis_type, deal_id, query)

            if not sales_data:
                state["sales_intelligence"] = {
                    "message": "No relevant sales data found",
                    "analysis_type": analysis_type,
                }
                return state

            # Perform sales intelligence analysis using the specialized agent
            insights = await self.sales_agent.comprehensive_sales_analysis(
                deal_id=deal_id,
                include_risk_assessment=True,
                include_competitive_analysis=True,
                include_forecasting=True,
            )

            # Store sales data and insights
            state["sales_data"] = sales_data
            state["sales_intelligence"] = insights

            # Generate specific analysis based on type
            if analysis_type == "deal_risk_assessment":
                risk_assessment = await self.sales_agent.assess_deal_risk(
                    deal_id=deal_id or sales_data.get("deal_id"),
                    hubspot_data=sales_data.get("hubspot_data", {}),
                    gong_data=sales_data.get("gong_data", {}),
                    risk_factors=sales_data.get("risk_factors", []),
                )
                state["deal_risk_assessment"] = risk_assessment

            # Generate competitive analysis if needed
            if (
                "competitor" in query.lower()
                or analysis_type == "competitor_talking_points"
            ):
                competitive_analysis = await self._enhance_competitive_analysis(
                    insights, query
                )
                state["competitive_analysis"] = competitive_analysis

            logger.info(f"Completed sales intelligence analysis: {analysis_type}")
            return state

        except Exception as e:
            logger.error(f"Error in sales intelligence analysis: {e}")
            state["error"] = f"Sales intelligence analysis failed: {str(e)}"
            return state

    def _determine_sales_analysis_type(self, query: str) -> str:
        """Determine the type of sales analysis needed"""
        query_lower = query.lower()

        if "risk" in query_lower and "deal" in query_lower:
            return "deal_risk_assessment"
        elif "email" in query_lower or "follow" in query_lower:
            return "sales_email_generation"
        elif "competitor" in query_lower or "talking points" in query_lower:
            return "competitor_talking_points"
        elif "pipeline" in query_lower or "forecast" in query_lower:
            return "pipeline_health"
        else:
            return "comprehensive"

    async def _get_sales_data(
        self, analysis_type: str, deal_id: Optional[str], query: str
    ) -> Dict[str, Any]:
        """Get relevant sales data based on analysis type"""
        try:
            data = {}

            # Get HubSpot data if deal_id provided
            if deal_id:
                async with self.hubspot_connector as connector:
                    hubspot_deals = await connector.query_hubspot_deals(limit=10)
                    if not hubspot_deals.empty:
                        deal_data = hubspot_deals[hubspot_deals["DEAL_ID"] == deal_id]
                        if not deal_data.empty:
                            data["hubspot_data"] = deal_data.iloc[0].to_dict()
                            data["deal_id"] = deal_id
                            data["deal_context"] = {
                                "deal_name": data["hubspot_data"].get("DEAL_NAME"),
                                "company_name": data["hubspot_data"].get(
                                    "COMPANY_NAME"
                                ),
                                "deal_stage": data["hubspot_data"].get("DEAL_STAGE"),
                                "amount": data["hubspot_data"].get("AMOUNT"),
                            }

            # Get Gong data if available
            async with self.gong_connector as connector:
                recent_calls = await connector.get_calls_for_coaching(
                    date_range_days=30, limit=5
                )
                data["gong_data"] = recent_calls
                data["recent_calls"] = recent_calls[:3]  # Most recent 3 calls

            # Set default values based on analysis type
            if analysis_type == "deal_risk_assessment":
                data["risk_factors"] = [
                    "timeline",
                    "budget",
                    "decision_maker",
                    "competition",
                ]
            elif analysis_type == "sales_email_generation":
                data["email_type"] = "follow_up"
                data["personalization"] = {}
            elif analysis_type == "competitor_talking_points":
                # Extract competitor from query
                competitors = ["salesforce", "hubspot", "pipedrive", "zoho"]
                data["competitor"] = next(
                    (c for c in competitors if c in query.lower()), "unknown"
                )
                data["competitive_situation"] = {}
            elif analysis_type == "pipeline_health":
                data["time_period"] = "current_quarter"
                data["segments"] = {}
                data["forecast_horizon"] = 90

            return data

        except Exception as e:
            logger.error(f"Error getting sales data: {e}")
            return {}

    async def _enhance_competitive_analysis(
        self, insights: Dict[str, Any], query: str
    ) -> Dict[str, Any]:
        """Enhance competitive analysis using SmartAIService"""
        try:
            competitive_prompt = f"""
            Enhance this competitive analysis with strategic insights:
            
            Sales Intelligence: {json.dumps(insights, indent=2)}
            Query: {query}
            
            Provide:
            1. Competitive positioning strategy
            2. Differentiation talking points
            3. Objection handling techniques
            4. Win/loss analysis insights
            5. Strategic recommendations
            """

            enhanced_analysis = await self.smart_ai_service.generate_response(
                {
                    "prompt": competitive_prompt,
                    "task_type": "competitive_analysis",
                    "model_preference": "performance",
                    "max_tokens": 700,
                }
            )

            return {
                "enhanced_analysis": enhanced_analysis.get("response", ""),
                "generated_at": datetime.now().isoformat(),
                "source": "smart_ai_service",
            }

        except Exception as e:
            logger.error(f"Error enhancing competitive analysis: {e}")
            return {"error": f"Competitive analysis enhancement failed: {str(e)}"}


@dataclass
class SupervisorAgent:
    """Enhanced Supervisor Agent for orchestrating multi-source workflows"""

    name: str = "supervisor_agent"
    description: str = "Orchestrates enhanced multi-source analysis workflows"

    # Service integrations
    cortex_service: Optional[SnowflakeCortexService] = None
    hubspot_connector: Optional[SnowflakeHubSpotConnector] = None
    gong_connector: Optional[SnowflakeGongConnector] = None
    ai_memory: Optional[EnhancedAiMemoryMCPServer] = None

    initialized: bool = False

    async def initialize(self) -> None:
        """Initialize services"""
        if self.initialized:
            return

        try:
            self.cortex_service = SnowflakeCortexService()
            self.hubspot_connector = SnowflakeHubSpotConnector()
            self.gong_connector = SnowflakeGongConnector()
            self.ai_memory = EnhancedAiMemoryMCPServer()

            await self.ai_memory.initialize()

            self.initialized = True
            logger.info("✅ Enhanced Supervisor Agent initialized")

        except Exception as e:
            logger.error(f"Failed to initialize Enhanced Supervisor Agent: {e}")
            raise

    async def route_workflow(self, state: WorkflowState) -> WorkflowState:
        """Route workflow to appropriate agents based on query and type"""
        if not self.initialized:
            await self.initialize()

        workflow_type = state.get("workflow_type", "")
        query = state.get("query", "")

        # Determine next action based on workflow type and available data
        if workflow_type == WorkflowType.CROSS_SOURCE_ANALYSIS.value:
            # Multi-source analysis workflow
            if not state.get("slack_data") and "slack" in query.lower():
                state["next_action"] = "analyze_slack"
            elif not state.get("linear_data") and (
                "linear" in query.lower() or "project" in query.lower()
            ):
                state["next_action"] = "analyze_linear"
            elif not state.get("hubspot_data") and (
                "deal" in query.lower() or "customer" in query.lower()
            ):
                state["next_action"] = "analyze_hubspot"
            elif not state.get("gong_data") and (
                "call" in query.lower() or "conversation" in query.lower()
            ):
                state["next_action"] = "analyze_gong"
            else:
                state["next_action"] = "synthesize_insights"

        elif workflow_type == WorkflowType.SLACK_INTELLIGENCE.value:
            state["next_action"] = "analyze_slack"

        elif workflow_type == WorkflowType.PROJECT_HEALTH_MONITORING.value:
            state["next_action"] = "analyze_linear"

        elif workflow_type == WorkflowType.KNOWLEDGE_SYNTHESIS.value:
            state["next_action"] = "curate_knowledge"

        # New Marketing Intelligence workflows
        elif workflow_type == WorkflowType.MARKETING_INTELLIGENCE.value:
            state["next_action"] = "analyze_marketing"

        elif workflow_type == WorkflowType.CAMPAIGN_OPTIMIZATION.value:
            state["next_action"] = "analyze_marketing"

        elif workflow_type == WorkflowType.COMPETITIVE_ANALYSIS.value:
            # Determine if this is marketing or sales focused
            if any(
                keyword in query.lower()
                for keyword in ["campaign", "content", "audience", "brand"]
            ):
                state["next_action"] = "analyze_marketing"
            elif any(
                keyword in query.lower()
                for keyword in ["deal", "sales", "pipeline", "revenue"]
            ):
                state["next_action"] = "analyze_sales"
            else:
                # Default to both for comprehensive competitive analysis
                state["next_action"] = (
                    "analyze_marketing"  # Start with marketing, then sales
                )

        # New Sales Intelligence workflows
        elif workflow_type == WorkflowType.SALES_INTELLIGENCE.value:
            state["next_action"] = "analyze_sales"

        elif workflow_type == WorkflowType.DEAL_RISK_ASSESSMENT.value:
            state["next_action"] = "analyze_sales"

        elif workflow_type == WorkflowType.REVENUE_INTELLIGENCE.value:
            state["next_action"] = "analyze_sales"

        else:
            # Default to existing deal analysis workflow
            state["next_action"] = "analyze_hubspot"

        return state

    async def synthesize_final_insights(self, state: WorkflowState) -> WorkflowState:
        """Synthesize final insights from all analyzed sources"""
        if not self.initialized:
            await self.initialize()

        try:
            # Collect insights from all sources
            all_insights = []
            data_sources_used = []

            if state.get("slack_insights"):
                all_insights.append(
                    {"source": "slack", "insights": state["slack_insights"]}
                )
                data_sources_used.append("slack")

            if state.get("linear_health"):
                all_insights.append(
                    {"source": "linear", "insights": state["linear_health"]}
                )
                data_sources_used.append("linear")

            if state.get("deal_insights"):
                all_insights.append(
                    {"source": "hubspot", "insights": state["deal_insights"]}
                )
                data_sources_used.append("hubspot")

            if state.get("call_analysis"):
                all_insights.append(
                    {"source": "gong", "insights": state["call_analysis"]}
                )
                data_sources_used.append("gong")

            if state.get("kb_insights"):
                all_insights.append(
                    {"source": "knowledge_base", "insights": state["kb_insights"]}
                )
                data_sources_used.append("knowledge_base")

            # Generate comprehensive synthesis
            synthesis = await self._generate_comprehensive_synthesis(
                all_insights, state["query"]
            )

            state["final_response"] = {
                "query": state["query"],
                "workflow_type": state["workflow_type"],
                "synthesis": synthesis,
                "data_sources_used": data_sources_used,
                "total_sources": len(all_insights),
                "confidence_score": self._calculate_confidence_score(all_insights),
            }

            state["next_action"] = "complete"

            return state

        except Exception as e:
            logger.error(f"Error synthesizing final insights: {e}")
            state["error"] = f"Synthesis failed: {str(e)}"
            return state

    async def _generate_comprehensive_synthesis(
        self, all_insights: List[Dict[str, Any]], query: str
    ) -> Dict[str, Any]:
        """Generate comprehensive synthesis using AI"""
        try:
            if not all_insights:
                return {"message": "No insights available for synthesis"}

            # Prepare insights summary for AI
            insights_summary = []
            for insight_data in all_insights:
                source = insight_data["source"]
                insights = insight_data["insights"]

                if isinstance(insights, dict):
                    summary = f"{source.title()} insights: "
                    key_points = []

                    for key, value in insights.items():
                        if isinstance(value, (str, int, float)):
                            key_points.append(f"{key}: {value}")
                        elif isinstance(value, list) and value:
                            key_points.append(
                                f"{key}: {', '.join(map(str, value[:3]))}"
                            )

                    summary += "; ".join(key_points[:5])
                    insights_summary.append(summary)

            async with self.cortex_service as cortex:
                synthesis_prompt = f"""
                Generate a comprehensive business intelligence synthesis for this query:
                
                Query: {query}
                
                Available insights from multiple sources:
                {chr(10).join(insights_summary)}
                
                Provide:
                1. Executive summary
                2. Key cross-source patterns
                3. Strategic recommendations
                4. Risk assessment
                5. Next steps
                
                Focus on actionable business intelligence.
                """

                synthesis_result = await cortex.complete_text_with_cortex(
                    prompt=synthesis_prompt, max_tokens=600
                )

                return {
                    "executive_summary": synthesis_result,
                    "cross_source_patterns": self._identify_cross_source_patterns(
                        all_insights
                    ),
                    "strategic_recommendations": self._generate_strategic_recommendations(
                        all_insights
                    ),
                    "confidence_assessment": "High confidence with multi-source validation",
                }

        except Exception as e:
            logger.error(f"Error generating synthesis: {e}")
            return {"error": f"Synthesis generation failed: {str(e)}"}

    def _identify_cross_source_patterns(
        self, all_insights: List[Dict[str, Any]]
    ) -> List[str]:
        """Identify patterns across different data sources"""
        patterns = []

        # Look for common themes across sources
        source_keywords = {}
        for insight_data in all_insights:
            source = insight_data["source"]
            insights = insight_data["insights"]

            # Extract keywords from insights
            if isinstance(insights, dict):
                text_content = str(insights)
                # Simple keyword extraction (could be enhanced with NLP)
                keywords = [
                    "customer",
                    "project",
                    "team",
                    "risk",
                    "opportunity",
                    "performance",
                ]
                found_keywords = [kw for kw in keywords if kw in text_content.lower()]
                source_keywords[source] = found_keywords

        # Find common keywords across sources
        all_keywords = set()
        for keywords in source_keywords.values():
            all_keywords.update(keywords)

        for keyword in all_keywords:
            sources_with_keyword = [
                source
                for source, keywords in source_keywords.items()
                if keyword in keywords
            ]
            if len(sources_with_keyword) > 1:
                patterns.append(
                    f"{keyword.title()} mentioned across {', '.join(sources_with_keyword)}"
                )

        return patterns[:5]

    def _generate_strategic_recommendations(
        self, all_insights: List[Dict[str, Any]]
    ) -> List[str]:
        """Generate strategic recommendations based on all insights"""
        recommendations = []

        # Extract recommendations from each source
        for insight_data in all_insights:
            insights = insight_data["insights"]
            if isinstance(insights, dict):
                # Look for recommendation fields
                if "recommendations" in insights:
                    recs = insights["recommendations"]
                    if isinstance(recs, list):
                        recommendations.extend(recs[:2])
                elif "recommended_actions" in insights:
                    actions = insights["recommended_actions"]
                    if isinstance(actions, list):
                        recommendations.extend(actions[:2])

        # Add cross-source recommendations
        if len(all_insights) > 1:
            recommendations.append(
                "Establish regular cross-functional data review meetings"
            )
            recommendations.append(
                "Create integrated dashboard for holistic business monitoring"
            )

        return list(set(recommendations))[:5]  # Remove duplicates and limit

    def _calculate_confidence_score(self, all_insights: List[Dict[str, Any]]) -> float:
        """Calculate confidence score based on number and quality of insights"""
        if not all_insights:
            return 0.0

        base_score = min(
            len(all_insights) * 0.2, 0.8
        )  # More sources = higher confidence

        # Bonus for having diverse source types
        source_types = set(insight["source"] for insight in all_insights)
        diversity_bonus = len(source_types) * 0.05

        return min(base_score + diversity_bonus, 1.0)


class EnhancedLangGraphWorkflowOrchestrator:
    """Enhanced LangGraph Workflow Orchestrator with multi-source intelligence"""

    def __init__(self):
        self.supervisor = SupervisorAgent()
        self.call_analysis_agent = CallAnalysisAgent()
        self.sales_coach_agent = SalesCoachAgent()
        self.slack_analysis_agent = SlackAnalysisAgent()
        self.linear_analysis_agent = LinearAnalysisAgent()
        self.knowledge_curator_agent = KnowledgeCuratorAgent()

        # New Marketing and Sales Intelligence agents
        self.marketing_analysis_agent = MarketingAnalysisLangGraphAgent()
        self.sales_intelligence_agent = SalesIntelligenceLangGraphAgent()

        self.workflow_graph = None
        self.initialized = False

    async def initialize(self) -> None:
        """Initialize the enhanced orchestrator"""
        if self.initialized:
            return

        try:
            # Initialize all agents
            await self.supervisor.initialize()
            await self.call_analysis_agent.initialize()
            await self.sales_coach_agent.initialize()
            await self.slack_analysis_agent.initialize()
            await self.linear_analysis_agent.initialize()
            await self.knowledge_curator_agent.initialize()

            # Initialize new Marketing and Sales Intelligence agents
            await self.marketing_analysis_agent.initialize()
            await self.sales_intelligence_agent.initialize()

            # Create enhanced workflow graph
            if LANGGRAPH_AVAILABLE:
                self.workflow_graph = self._create_enhanced_workflow_graph()

            self.initialized = True
            logger.info(
                "✅ Enhanced LangGraph Workflow Orchestrator initialized with Marketing and Sales Intelligence"
            )

        except Exception as e:
            logger.error(f"Failed to initialize Enhanced LangGraph Orchestrator: {e}")
            raise

    def _create_enhanced_workflow_graph(self) -> StateGraph:
        """Create enhanced workflow graph with new agents"""
        workflow = StateGraph(WorkflowState)

        # Add all agent nodes
        workflow.add_node("supervisor", self.supervisor.route_workflow)
        workflow.add_node("analyze_hubspot", self._analyze_hubspot_wrapper)
        workflow.add_node("analyze_gong", self.call_analysis_agent.analyze_calls)
        workflow.add_node(
            "analyze_slack", self.slack_analysis_agent.analyze_slack_conversations
        )
        workflow.add_node(
            "analyze_linear", self.linear_analysis_agent.analyze_project_health
        )
        workflow.add_node(
            "curate_knowledge", self.knowledge_curator_agent.curate_knowledge
        )
        workflow.add_node("generate_coaching", self.sales_coach_agent.generate_coaching)

        # Add new Marketing and Sales Intelligence nodes
        workflow.add_node(
            "analyze_marketing",
            self.marketing_analysis_agent.analyze_marketing_performance,
        )
        workflow.add_node(
            "analyze_sales", self.sales_intelligence_agent.analyze_sales_intelligence
        )

        workflow.add_node(
            "synthesize_insights", self.supervisor.synthesize_final_insights
        )

        # Set entry point
        workflow.set_entry_point("supervisor")

        # Add conditional edges based on next_action
        workflow.add_conditional_edges(
            "supervisor",
            self._route_next_action,
            {
                "analyze_hubspot": "analyze_hubspot",
                "analyze_gong": "analyze_gong",
                "analyze_slack": "analyze_slack",
                "analyze_linear": "analyze_linear",
                "curate_knowledge": "curate_knowledge",
                "analyze_marketing": "analyze_marketing",  # New marketing routing
                "analyze_sales": "analyze_sales",  # New sales routing
                "synthesize_insights": "synthesize_insights",
                "complete": END,
            },
        )

        # Add edges back to supervisor for continued routing
        workflow.add_edge("analyze_hubspot", "supervisor")
        workflow.add_edge("analyze_gong", "supervisor")
        workflow.add_edge("analyze_slack", "supervisor")
        workflow.add_edge("analyze_linear", "supervisor")
        workflow.add_edge("curate_knowledge", "supervisor")
        workflow.add_edge("generate_coaching", "supervisor")
        workflow.add_edge("analyze_marketing", "supervisor")  # New marketing edge
        workflow.add_edge("analyze_sales", "supervisor")  # New sales edge
        workflow.add_edge("synthesize_insights", END)

        return workflow.compile()

    def _route_next_action(self, state: WorkflowState) -> str:
        """Route to next action based on state"""
        return state.get("next_action", "complete")

    async def _analyze_hubspot_wrapper(self, state: WorkflowState) -> WorkflowState:
        """Wrapper for HubSpot analysis to maintain compatibility"""
        # This would integrate with existing HubSpot analysis logic
        # For now, set a placeholder
        state["hubspot_data"] = {"analyzed": True}
        state["deal_insights"] = {"message": "HubSpot analysis completed"}
        return state

    async def execute_workflow(self, request: WorkflowRequest) -> WorkflowResult:
        """Execute enhanced workflow"""
        if not self.initialized:
            await self.initialize()

        start_time = asyncio.get_event_loop().time()

        try:
            # Create initial state
            initial_state = WorkflowState(
                query=request.query,
                workflow_type=request.workflow_type.value,
                deal_id=request.parameters.get("deal_id"),
                hubspot_data=None,
                gong_data=None,
                slack_data=None,
                linear_data=None,
                kb_data=None,
                call_analysis=None,
                deal_insights=None,
                coaching_recommendations=None,
                slack_insights=None,
                linear_health=None,
                kb_insights=None,
                next_action="start",
                error=None,
                final_response=None,
            )

            # Execute workflow
            if self.workflow_graph:
                final_state = await self.workflow_graph.ainvoke(initial_state)
            else:
                # Fallback execution without LangGraph
                final_state = await self._execute_fallback_workflow(
                    initial_state, request
                )

            # Calculate processing time
            processing_time = asyncio.get_event_loop().time() - start_time

            # Create result
            if final_state.get("error"):
                return WorkflowResult(
                    success=False,
                    workflow_type=request.workflow_type,
                    insights={},
                    recommendations=[],
                    data_sources_used=[],
                    processing_time=processing_time,
                    confidence_score=0.0,
                    error=final_state["error"],
                )

            final_response = final_state.get("final_response", {})

            return WorkflowResult(
                success=True,
                workflow_type=request.workflow_type,
                insights=final_response.get("synthesis", {}),
                recommendations=final_response.get("synthesis", {}).get(
                    "strategic_recommendations", []
                ),
                data_sources_used=final_response.get("data_sources_used", []),
                processing_time=processing_time,
                confidence_score=final_response.get("confidence_score", 0.5),
            )

        except Exception as e:
            processing_time = asyncio.get_event_loop().time() - start_time
            logger.error(f"Workflow execution failed: {e}")

            return WorkflowResult(
                success=False,
                workflow_type=request.workflow_type,
                insights={},
                recommendations=[],
                data_sources_used=[],
                processing_time=processing_time,
                confidence_score=0.0,
                error=str(e),
            )

    async def _execute_fallback_workflow(
        self, initial_state: WorkflowState, request: WorkflowRequest
    ) -> WorkflowState:
        """Execute workflow without LangGraph as fallback"""
        state = initial_state

        try:
            # Route workflow
            state = await self.supervisor.route_workflow(state)

            # Execute based on workflow type
            if request.workflow_type == WorkflowType.SLACK_INTELLIGENCE:
                state = await self.slack_analysis_agent.analyze_slack_conversations(
                    state
                )
            elif request.workflow_type == WorkflowType.PROJECT_HEALTH_MONITORING:
                state = await self.linear_analysis_agent.analyze_project_health(state)
            elif request.workflow_type == WorkflowType.KNOWLEDGE_SYNTHESIS:
                state = await self.knowledge_curator_agent.curate_knowledge(state)
            elif request.workflow_type == WorkflowType.CROSS_SOURCE_ANALYSIS:
                # Execute multiple agents
                state = await self.slack_analysis_agent.analyze_slack_conversations(
                    state
                )
                state = await self.linear_analysis_agent.analyze_project_health(state)
                state = await self.knowledge_curator_agent.curate_knowledge(state)

            # Synthesize final insights
            state = await self.supervisor.synthesize_final_insights(state)

            return state

        except Exception as e:
            state["error"] = str(e)
            return state


# Example usage and testing
async def test_enhanced_workflow():
    """Test enhanced workflow orchestration"""
    orchestrator = EnhancedLangGraphWorkflowOrchestrator()
    await orchestrator.initialize()

    # Test cross-source analysis
    request = WorkflowRequest(
        query="What are the risks for our current projects based on team communication and development progress?",
        workflow_type=WorkflowType.CROSS_SOURCE_ANALYSIS,
        parameters={"include_slack": True, "include_linear": True},
    )

    result = await orchestrator.execute_workflow(request)

    print(f"Workflow Success: {result.success}")
    print(f"Data Sources Used: {result.data_sources_used}")
    print(f"Processing Time: {result.processing_time:.2f}s")
    print(f"Confidence Score: {result.confidence_score:.2f}")

    if result.success:
        print(f"Insights: {json.dumps(result.insights, indent=2)}")
        print(f"Recommendations: {result.recommendations}")
    else:
        print(f"Error: {result.error}")


if __name__ == "__main__":
    asyncio.run(test_enhanced_workflow())
