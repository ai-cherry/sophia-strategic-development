#!/usr/bin/env python3
"""
Sophia AI - Advanced Gateway with Latest High-Performance Models
Implements June 2025 state-of-the-art models with sophisticated routing
Based on comprehensive Portkey + OpenRouter + MCP analysis
"""

import asyncio
import json
import logging
import time
from typing import Dict, Any, Optional, List, Literal
import aiohttp
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# Import our clean ESC configuration
from backend.core.clean_esc_config import config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Enhanced Pydantic models
class ModelTier(BaseModel):
    tier: Literal["premium", "specialized", "balanced", "cost_effective"]
    description: str
    use_cases: List[str]

class AdvancedChatRequest(BaseModel):
    message: str
    task_type: Optional[str] = "general"  # reasoning, coding, analysis, general
    complexity: Optional[Literal["simple", "medium", "complex", "expert"]] = "medium"
    user_tier: Optional[Literal["free", "pro", "enterprise"]] = "pro"
    max_tokens: Optional[int] = 4000
    temperature: Optional[float] = 0.7
    priority: Optional[Literal["cost", "performance", "balanced"]] = "performance"
    enable_thinking: Optional[bool] = True  # For reasoning models
    mcp_tools: Optional[List[str]] = []  # MCP tools to enable

class ModelPerformanceMetrics(BaseModel):
    model_id: str
    name: str
    provider: str
    tier: ModelTier
    cost_per_1k_input: float
    cost_per_1k_output: float
    context_window: int
    latency_ms: float
    quality_score: float
    reasoning_score: Optional[float] = None
    coding_score: Optional[float] = None
    specializations: List[str]

class AdvancedChatResponse(BaseModel):
    response: str
    model_used: str
    tier: str
    reasoning_trace: Optional[str] = None  # For o3 Pro and other reasoning models
    cost: float
    tokens_used: Dict[str, int]
    response_time_ms: float
    routing_decision: Dict[str, Any]
    mcp_tools_used: List[str]
    fallback_triggered: bool = False

class AdvancedAIGateway:
    """Advanced AI Gateway with Latest High-Performance Models (June 2025)"""
    
    def __init__(self):
        self.app = FastAPI(
            title="Sophia AI - Advanced Gateway",
            description="Latest high-performance models with sophisticated routing",
            version="3.0.0"
        )
        self._setup_middleware()
        self._setup_routes()
        self.session: Optional[aiohttp.ClientSession] = None
        
        # Latest High-Performance Models (June 2025)
        self.model_catalog = {
            # Premium Reasoning Models
            "openai/o3-pro": ModelPerformanceMetrics(
                model_id="openai/o3-pro",
                name="OpenAI o3 Pro",
                provider="openai",
                tier=ModelTier(
                    tier="premium",
                    description="Premium reasoning model with extensive compute",
                    use_cases=["complex_reasoning", "multi_step_planning", "agentic_workflows"]
                ),
                cost_per_1k_input=20.0,
                cost_per_1k_output=80.0,
                context_window=200000,
                latency_ms=3000,
                quality_score=0.98,
                reasoning_score=0.99,
                coding_score=0.95,
                specializations=["mathematical_reasoning", "complex_problem_solving", "chain_of_thought"]
            ),
            
            "google/gemini-2.5-pro": ModelPerformanceMetrics(
                model_id="google/gemini-2.5-pro",
                name="Google Gemini 2.5 Pro",
                provider="google",
                tier=ModelTier(
                    tier="premium",
                    description="LMArena #1 performer with built-in thinking",
                    use_cases=["advanced_reasoning", "scientific_tasks", "mathematics"]
                ),
                cost_per_1k_input=1.25,
                cost_per_1k_output=10.0,
                context_window=1050000,
                latency_ms=2500,
                quality_score=0.97,
                reasoning_score=0.96,
                coding_score=0.94,
                specializations=["scientific_reasoning", "mathematics", "built_in_thinking"]
            ),
            
            "kimi/dev-72b": ModelPerformanceMetrics(
                model_id="kimi/dev-72b",
                name="Kimi Dev 72B",
                provider="kimi",
                tier=ModelTier(
                    tier="specialized",
                    description="Software engineering specialist",
                    use_cases=["software_engineering", "bug_fixing", "code_reasoning"]
                ),
                cost_per_1k_input=0.0,  # Free
                cost_per_1k_output=0.0,
                context_window=96000,
                latency_ms=1800,
                quality_score=0.90,
                reasoning_score=0.85,
                coding_score=0.96,
                specializations=["software_engineering", "swe_bench", "bug_fixing", "code_reasoning"]
            ),
            
            "google/gemini-2.5-flash": ModelPerformanceMetrics(
                model_id="google/gemini-2.5-flash",
                name="Google Gemini 2.5 Flash",
                provider="google",
                tier=ModelTier(
                    tier="balanced",
                    description="Google's workhorse with advanced reasoning",
                    use_cases=["general_tasks", "fast_reasoning", "balanced_performance"]
                ),
                cost_per_1k_input=0.30,
                cost_per_1k_output=2.50,
                context_window=1000000,
                latency_ms=1200,
                quality_score=0.92,
                reasoning_score=0.89,
                coding_score=0.87,
                specializations=["fast_reasoning", "balanced_performance", "workhorse"]
            )
        }
        
        # Performance analytics
        self.analytics = {
            "total_requests": 0,
            "model_usage": {},
            "cost_savings": 0.0,
            "avg_response_time": 0.0,
            "fallback_events": 0
        }
    
    def _setup_middleware(self):
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
    
    def _setup_routes(self):
        self.app.add_event_handler("startup", self.startup)
        self.app.add_event_handler("shutdown", self.shutdown)
        
        self.app.add_api_route("/health", self.health_check, methods=["GET"])
        self.app.add_api_route("/models", self.get_model_catalog, methods=["GET"])
        self.app.add_api_route("/ai/advanced-chat", self.advanced_chat, methods=["POST"])
        self.app.add_api_route("/analytics/advanced", self.get_advanced_analytics, methods=["GET"])
    
    async def startup(self):
        """Initialize Advanced AI Gateway"""
        logger.info("ðŸš€ Starting Advanced AI Gateway with Latest Models")
        self.session = aiohttp.ClientSession()
        logger.info("âœ… Advanced Gateway initialized")
    
    async def shutdown(self):
        """Cleanup resources"""
        if self.session:
            await self.session.close()
        logger.info("ðŸ”„ Advanced AI Gateway shutdown complete")
    
    async def health_check(self):
        """Enhanced health check with latest model status"""
        return {
            "status": "healthy",
            "service": "Sophia AI Advanced Gateway",
            "version": "3.0.0",
            "architecture": "june_2025_sota_models",
            "total_models": len(self.model_catalog),
            "premium_models": len([m for m in self.model_catalog.values() if m.tier.tier == "premium"]),
            "features": [
                "OpenAI o3 Pro Reasoning",
                "Gemini 2.5 Pro (LMArena #1)",
                "Kimi Dev 72B (Free SWE Specialist)",
                "Conditional Routing",
                "Advanced Analytics"
            ]
        }
    
    async def get_model_catalog(self):
        """Get comprehensive model catalog with latest models"""
        return {
            "models": [
                {
                    "id": model.model_id,
                    "name": model.name,
                    "provider": model.provider,
                    "tier": model.tier.tier,
                    "description": model.tier.description,
                    "cost_per_1k": {
                        "input": model.cost_per_1k_input,
                        "output": model.cost_per_1k_output
                    },
                    "context_window": model.context_window,
                    "scores": {
                        "quality": model.quality_score,
                        "reasoning": model.reasoning_score,
                        "coding": model.coding_score
                    },
                    "specializations": model.specializations,
                    "use_cases": model.tier.use_cases
                }
                for model in self.model_catalog.values()
            ]
        }
    
    async def advanced_chat(self, request: AdvancedChatRequest):
        """Advanced chat with sophisticated routing"""
        start_time = time.time()
        self.analytics["total_requests"] += 1
        
        # Choose model based on task type and complexity
        if request.task_type == "coding":
            selected_model = self.model_catalog["kimi/dev-72b"]
        elif request.task_type == "reasoning" and request.complexity == "expert":
            selected_model = self.model_catalog["openai/o3-pro"]
        elif request.complexity in ["complex", "expert"]:
            selected_model = self.model_catalog["google/gemini-2.5-pro"]
        else:
            selected_model = self.model_catalog["google/gemini-2.5-flash"]
        
        # Simulate model response
        response = await self._simulate_model_response(request, selected_model)
        
        end_time = time.time()
        response_time = (end_time - start_time) * 1000
        
        return AdvancedChatResponse(
            response=response["content"],
            model_used=selected_model.model_id,
            tier=selected_model.tier.tier,
            reasoning_trace=response.get("reasoning_trace"),
            cost=response.get("cost", 0.0),
            tokens_used=response.get("tokens", {"input": 0, "output": 0}),
            response_time_ms=response_time,
            routing_decision={
                "strategy": "task_optimized",
                "reason": f"Selected {selected_model.name} for {request.task_type} task",
                "complexity": request.complexity
            },
            mcp_tools_used=request.mcp_tools or [],
            fallback_triggered=False
        )
    
    async def get_advanced_analytics(self):
        """Get comprehensive analytics"""
        return {
            "overview": {
                "total_requests": self.analytics["total_requests"],
                "avg_response_time": self.analytics["avg_response_time"],
                "total_cost_savings": self.analytics["cost_savings"]
            },
            "model_performance": {
                "premium_tier": {
                    "models": ["openai/o3-pro", "google/gemini-2.5-pro"],
                    "avg_quality": 0.975,
                    "use_case": "Expert-level reasoning and complex analysis"
                },
                "specialized_tier": {
                    "models": ["kimi/dev-72b"],
                    "avg_quality": 0.90,
                    "use_case": "Software engineering (FREE!)"
                },
                "balanced_tier": {
                    "models": ["google/gemini-2.5-flash"],
                    "avg_quality": 0.92,
                    "use_case": "General high-performance tasks"
                }
            }
        }
    
    async def _simulate_model_response(self, request: AdvancedChatRequest, model: ModelPerformanceMetrics) -> Dict[str, Any]:
        """Simulate model response"""
        await asyncio.sleep(model.latency_ms / 1000)
        
        cost = self._calculate_cost(request.message, model)
        
        response = {
            "content": f"[{model.name}] Advanced AI response to: {request.message}",
            "cost": cost,
            "tokens": {
                "input": len(request.message.split()),
                "output": 50
            }
        }
        
        if model.reasoning_score and model.reasoning_score > 0.9 and request.enable_thinking:
            response["reasoning_trace"] = f"[Thinking] Step-by-step reasoning for {request.task_type}..."
        
        return response
    
    def _calculate_cost(self, message: str, model: ModelPerformanceMetrics) -> float:
        """Calculate estimated cost"""
        input_tokens = len(message.split())
        output_tokens = 50
        
        input_cost = (input_tokens / 1000) * model.cost_per_1k_input
        output_cost = (output_tokens / 1000) * model.cost_per_1k_output
        
        return input_cost + output_cost

# Global gateway instance
advanced_gateway = AdvancedAIGateway()
app = advanced_gateway.app

if __name__ == "__main__":
    logger.info("ðŸš€ Starting Sophia AI - Advanced Gateway (June 2025 SOTA Models)")
    uvicorn.run(
        "backend.advanced_ai_gateway:app",
        host="0.0.0.0",
        port=8003,
        reload=False,
        log_level="info"
    ) 