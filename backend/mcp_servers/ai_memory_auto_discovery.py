"""
Enhanced AI Memory Auto-Discovery System
Intelligent context detection, pattern recognition, and automated memory storage
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 1106 lines

Recommended decomposition:
- ai_memory_auto_discovery_core.py - Core functionality
- ai_memory_auto_discovery_utils.py - Utility functions
- ai_memory_auto_discovery_models.py - Data models
- ai_memory_auto_discovery_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

from backend.core.config_manager import get_config_value
from backend.mcp_servers.enhanced_ai_memory_mcp_server import (
    EnhancedAiMemoryMCPServer,
    MemoryCategory,
)

logger = logging.getLogger(__name__)


class ContextType(Enum):
    """Types of development context"""

    ARCHITECTURE_DECISION = "architecture_decision"
    BUG_ANALYSIS = "bug_analysis"
    CODE_PATTERN = "code_pattern"
    PERFORMANCE_INSIGHT = "performance_insight"
    SECURITY_CONSIDERATION = "security_consideration"
    WORKFLOW_OPTIMIZATION = "workflow_optimization"
    BUSINESS_LOGIC = "business_logic"
    INTEGRATION_PATTERN = "integration_pattern"
    ERROR_RESOLUTION = "error_resolution"
    DEPLOYMENT_STRATEGY = "deployment_strategy"


class ConfidenceLevel(Enum):
    """Confidence levels for auto-detected patterns"""

    LOW = 0.3
    MEDIUM = 0.6
    HIGH = 0.8
    VERY_HIGH = 0.9


@dataclass
class ContextPattern:
    """Pattern for detecting specific types of development context"""

    context_type: ContextType
    keywords: list[str]
    phrases: list[str]
    code_indicators: list[str]
    importance_weight: float
    confidence_threshold: float = 0.6


@dataclass
class DetectedContext:
    """Detected development context with metadata"""

    context_type: ContextType
    confidence: float
    key_points: list[str]
    suggested_category: str
    suggested_tags: list[str]
    importance_score: float
    reasoning: str
    extracted_code: str | None = None
    related_files: list[str] = field(default_factory=list)
    dependencies: list[str] = field(default_factory=list)


class IntelligentContextDetector:
    """
    Advanced context detection system that identifies development patterns,
    architectural decisions, and important insights from conversations
    """

    def __init__(self):
        self.patterns = self._initialize_patterns()
        self.openai_client = None
        self.initialized = False

    async def initialize(self):
        """Initialize OpenAI client for advanced analysis"""
        if self.initialized:
            return

        try:
            import openai

            api_key = await get_config_value("openai_api_key")
            if api_key:
                self.openai_client = openai.AsyncOpenAI(api_key=api_key)
                logger.info("âœ… Intelligent Context Detector initialized with OpenAI")
            else:
                logger.warning(
                    "OpenAI API key not available, using pattern-based detection only"
                )

            self.initialized = True

        except Exception as e:
            logger.error(f"Failed to initialize Intelligent Context Detector: {e}")
            self.initialized = True  # Continue with pattern-based detection

    def _initialize_patterns(self) -> list[ContextPattern]:
        """Initialize detection patterns for different context types"""
        return [
            ContextPattern(
                context_type=ContextType.ARCHITECTURE_DECISION,
                keywords=[
                    "architecture",
                    "design",
                    "structure",
                    "pattern",
                    "approach",
                    "strategy",
                ],
                phrases=[
                    "we decided to",
                    "architectural decision",
                    "design choice",
                    "we chose",
                    "the approach is",
                    "pattern we're using",
                    "architectural pattern",
                    "design decision",
                    "we're implementing",
                    "the structure will be",
                ],
                code_indicators=[
                    "class",
                    "interface",
                    "abstract",
                    "@dataclass",
                    "def __init__",
                ],
                importance_weight=0.9,
                confidence_threshold=0.7,
            ),
            ContextPattern(
                context_type=ContextType.BUG_ANALYSIS,
                keywords=[
                    "bug",
                    "error",
                    "issue",
                    "problem",
                    "fix",
                    "solution",
                    "debug",
                ],
                phrases=[
                    "the bug was",
                    "error occurs",
                    "issue is",
                    "problem with",
                    "fixed by",
                    "solution is",
                    "root cause",
                    "debugging showed",
                    "the fix is",
                    "resolved by",
                    "error happens when",
                ],
                code_indicators=[
                    "try:",
                    "except:",
                    "raise",
                    "assert",
                    "if __debug__",
                    "logging",
                ],
                importance_weight=0.8,
                confidence_threshold=0.6,
            ),
            ContextPattern(
                context_type=ContextType.CODE_PATTERN,
                keywords=[
                    "pattern",
                    "implementation",
                    "method",
                    "function",
                    "class",
                    "algorithm",
                ],
                phrases=[
                    "code pattern",
                    "implementation approach",
                    "we implement",
                    "the method is",
                    "algorithm we use",
                    "coding standard",
                    "best practice",
                    "pattern for",
                    "way to implement",
                ],
                code_indicators=[
                    "def ",
                    "class ",
                    "async def",
                    "@decorator",
                    "lambda",
                    "yield",
                ],
                importance_weight=0.7,
                confidence_threshold=0.6,
            ),
            ContextPattern(
                context_type=ContextType.PERFORMANCE_INSIGHT,
                keywords=[
                    "performance",
                    "optimization",
                    "speed",
                    "memory",
                    "efficiency",
                    "bottleneck",
                ],
                phrases=[
                    "performance improvement",
                    "optimization technique",
                    "bottleneck is",
                    "memory usage",
                    "speed up",
                    "efficient way",
                    "performance issue",
                    "optimize by",
                    "faster approach",
                    "reduce latency",
                ],
                code_indicators=[
                    "async",
                    "await",
                    "cache",
                    "pool",
                    "batch",
                    "concurrent",
                ],
                importance_weight=0.8,
                confidence_threshold=0.7,
            ),
            ContextPattern(
                context_type=ContextType.SECURITY_CONSIDERATION,
                keywords=[
                    "security",
                    "authentication",
                    "authorization",
                    "encryption",
                    "vulnerability",
                ],
                phrases=[
                    "security consideration",
                    "authentication method",
                    "secure way",
                    "vulnerability in",
                    "encryption approach",
                    "security risk",
                    "authorization pattern",
                    "secure implementation",
                    "security best practice",
                ],
                code_indicators=[
                    "auth",
                    "encrypt",
                    "hash",
                    "token",
                    "secret",
                    "validate",
                ],
                importance_weight=0.9,
                confidence_threshold=0.8,
            ),
            ContextPattern(
                context_type=ContextType.INTEGRATION_PATTERN,
                keywords=[
                    "integration",
                    "api",
                    "service",
                    "connector",
                    "client",
                    "interface",
                ],
                phrases=[
                    "integration with",
                    "api client",
                    "service connection",
                    "external service",
                    "integration pattern",
                    "connector for",
                    "interface to",
                    "api wrapper",
                    "service integration",
                    "external api",
                    "third party",
                ],
                code_indicators=[
                    "requests",
                    "aiohttp",
                    "client",
                    "api",
                    "endpoint",
                    "async def",
                ],
                importance_weight=0.8,
                confidence_threshold=0.6,
            ),
            ContextPattern(
                context_type=ContextType.BUSINESS_LOGIC,
                keywords=[
                    "business",
                    "logic",
                    "rule",
                    "process",
                    "workflow",
                    "requirement",
                ],
                phrases=[
                    "business logic",
                    "business rule",
                    "process flow",
                    "workflow step",
                    "business requirement",
                    "logic for",
                    "business process",
                    "rule is",
                    "requirement is",
                    "business case",
                ],
                code_indicators=["if", "elif", "else", "match", "case", "validate"],
                importance_weight=0.8,
                confidence_threshold=0.7,
            ),
            ContextPattern(
                context_type=ContextType.ERROR_RESOLUTION,
                keywords=[
                    "error",
                    "exception",
                    "failure",
                    "resolve",
                    "handle",
                    "recovery",
                ],
                phrases=[
                    "error handling",
                    "exception occurs",
                    "failure mode",
                    "error recovery",
                    "handle the error",
                    "exception handling",
                    "error case",
                    "failure scenario",
                    "error resolution",
                    "exception strategy",
                ],
                code_indicators=[
                    "try:",
                    "except",
                    "finally:",
                    "raise",
                    "Exception",
                    "Error",
                ],
                importance_weight=0.8,
                confidence_threshold=0.6,
            ),
            ContextPattern(
                context_type=ContextType.DEPLOYMENT_STRATEGY,
                keywords=[
                    "deployment",
                    "deploy",
                    "production",
                    "staging",
                    "environment",
                    "release",
                ],
                phrases=[
                    "deployment strategy",
                    "deploy to",
                    "production deployment",
                    "staging environment",
                    "release process",
                    "deployment approach",
                    "environment setup",
                    "deploy with",
                    "production ready",
                ],
                code_indicators=[
                    "docker",
                    "kubernetes",
                    "config",
                    "env",
                    "production",
                    "staging",
                ],
                importance_weight=0.8,
                confidence_threshold=0.7,
            ),
        ]

    async def detect_context(
        self, conversation: str, file_context: dict[str, Any] | None = None
    ) -> list[DetectedContext]:
        """
        Detect development context from conversation with advanced AI analysis

        Args:
            conversation: The conversation text to analyze
            file_context: Optional context about files being discussed

        Returns:
            List of detected contexts with confidence scores
        """
        if not self.initialized:
            await self.initialize()

        detected_contexts = []

        # Pattern-based detection
        pattern_contexts = self._detect_with_patterns(conversation, file_context)
        detected_contexts.extend(pattern_contexts)

        # AI-enhanced detection if OpenAI is available
        if self.openai_client:
            ai_contexts = await self._detect_with_ai(conversation, file_context)
            detected_contexts.extend(ai_contexts)

        # Merge and deduplicate contexts
        merged_contexts = self._merge_contexts(detected_contexts)

        # Filter by confidence threshold
        high_confidence_contexts = [
            ctx
            for ctx in merged_contexts
            if ctx.confidence
            >= (ctx.context_type.value if hasattr(ctx.context_type, "value") else 0.6)
        ]

        return high_confidence_contexts

    def _detect_with_patterns(
        self, conversation: str, file_context: dict[str, Any] | None = None
    ) -> list[DetectedContext]:
        """Pattern-based context detection"""
        detected = []
        conversation_lower = conversation.lower()

        for pattern in self.patterns:
            confidence = 0.0
            matched_keywords = []
            matched_phrases = []
            matched_code = []

            # Check keywords
            for keyword in pattern.keywords:
                if keyword.lower() in conversation_lower:
                    confidence += 0.1
                    matched_keywords.append(keyword)

            # Check phrases
            for phrase in pattern.phrases:
                if phrase.lower() in conversation_lower:
                    confidence += 0.2
                    matched_phrases.append(phrase)

            # Check code indicators
            for indicator in pattern.code_indicators:
                if indicator in conversation:
                    confidence += 0.15
                    matched_code.append(indicator)

            # Adjust confidence based on context
            if file_context:
                if file_context.get("file_type") in ["py", "js", "ts", "java", "cpp"]:
                    confidence += 0.1
                if file_context.get("is_new_file"):
                    confidence += 0.05

            # Apply importance weight
            confidence *= pattern.importance_weight

            if confidence >= pattern.confidence_threshold:
                # Extract key points
                key_points = []
                key_points.extend([f"Keyword: {kw}" for kw in matched_keywords[:3]])
                key_points.extend(
                    [f"Phrase: {phrase}" for phrase in matched_phrases[:2]]
                )
                key_points.extend([f"Code: {code}" for code in matched_code[:2]])

                # Generate tags
                tags = [pattern.context_type.value]
                tags.extend(matched_keywords[:3])
                if file_context and file_context.get("file_name"):
                    tags.append(f"file_{file_context['file_name'].replace('.', '_')}")

                detected.append(
                    DetectedContext(
                        context_type=pattern.context_type,
                        confidence=min(confidence, 1.0),
                        key_points=key_points,
                        suggested_category=self._map_context_to_category(
                            pattern.context_type
                        ),
                        suggested_tags=tags,
                        importance_score=pattern.importance_weight,
                        reasoning=f"Pattern-based detection: {len(matched_keywords)} keywords, {len(matched_phrases)} phrases, {len(matched_code)} code indicators",
                        extracted_code=self._extract_code_blocks(conversation),
                        related_files=(
                            [file_context.get("file_name")]
                            if file_context and file_context.get("file_name")
                            else []
                        ),
                    )
                )

        return detected

    async def _detect_with_ai(
        self, conversation: str, file_context: dict[str, Any] | None = None
    ) -> list[DetectedContext]:
        """AI-enhanced context detection using OpenAI"""
        try:
            # Prepare context for AI analysis
            context_info = ""
            if file_context:
                context_info = f"\nFile context: {json.dumps(file_context, indent=2)}"

            prompt = f"""
            Analyze this development conversation and identify important contexts that should be remembered for future AI assistance.

            Conversation:
            {conversation[:4000]}  # Limit to avoid token limits
            {context_info}

            Identify and extract:
            1. Architectural decisions and reasoning
            2. Bug fixes and their root causes
            3. Code patterns and best practices
            4. Performance optimizations
            5. Security considerations
            6. Integration approaches
            7. Business logic rules
            8. Error handling strategies
            9. Deployment strategies
            10. Important insights or lessons learned

            For each identified context, provide:
            - Type of context
            - Confidence level (0.0-1.0)
            - Key points (bullet list)
            - Suggested tags
            - Importance score (0.0-1.0)
            - Brief reasoning

            Return as JSON array with this structure:
            [{{
                "context_type": "architecture_decision|bug_analysis|code_pattern|performance_insight|security_consideration|integration_pattern|business_logic|error_resolution|deployment_strategy",
                "confidence": 0.8,
                "key_points": ["point1", "point2"],
                "suggested_tags": ["tag1", "tag2"],
                "importance_score": 0.7,
                "reasoning": "explanation"
            }}]
            """

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1500,
            )

            ai_analysis = response.choices[0].message.content

            # Parse AI response
            try:
                contexts_data = json.loads(ai_analysis)
                ai_contexts = []

                for ctx_data in contexts_data:
                    context_type = self._parse_context_type(
                        ctx_data.get("context_type", "")
                    )
                    if context_type:
                        ai_contexts.append(
                            DetectedContext(
                                context_type=context_type,
                                confidence=float(ctx_data.get("confidence", 0.5)),
                                key_points=ctx_data.get("key_points", []),
                                suggested_category=self._map_context_to_category(
                                    context_type
                                ),
                                suggested_tags=ctx_data.get("suggested_tags", []),
                                importance_score=float(
                                    ctx_data.get("importance_score", 0.5)
                                ),
                                reasoning=f"AI analysis: {ctx_data.get('reasoning', 'AI-detected pattern')}",
                                extracted_code=self._extract_code_blocks(conversation),
                                related_files=(
                                    [file_context.get("file_name")]
                                    if file_context and file_context.get("file_name")
                                    else []
                                ),
                            )
                        )

                return ai_contexts

            except json.JSONDecodeError:
                logger.warning("Failed to parse AI context analysis response")
                return []

        except Exception as e:
            logger.error(f"AI context detection failed: {e}")
            return []

    def _parse_context_type(self, type_str: str) -> ContextType | None:
        """Parse context type string to enum"""
        type_mapping = {
            "architecture_decision": ContextType.ARCHITECTURE_DECISION,
            "bug_analysis": ContextType.BUG_ANALYSIS,
            "code_pattern": ContextType.CODE_PATTERN,
            "performance_insight": ContextType.PERFORMANCE_INSIGHT,
            "security_consideration": ContextType.SECURITY_CONSIDERATION,
            "integration_pattern": ContextType.INTEGRATION_PATTERN,
            "business_logic": ContextType.BUSINESS_LOGIC,
            "error_resolution": ContextType.ERROR_RESOLUTION,
            "deployment_strategy": ContextType.DEPLOYMENT_STRATEGY,
            "workflow_optimization": ContextType.WORKFLOW_OPTIMIZATION,
        }
        return type_mapping.get(type_str.lower())

    def _map_context_to_category(self, context_type: ContextType) -> str:
        """Map context type to memory category"""
        mapping = {
            ContextType.ARCHITECTURE_DECISION: MemoryCategory.ARCHITECTURE,
            ContextType.BUG_ANALYSIS: MemoryCategory.BUG_SOLUTION,
            ContextType.CODE_PATTERN: MemoryCategory.AI_CODING_PATTERN,
            ContextType.PERFORMANCE_INSIGHT: MemoryCategory.PERFORMANCE_TIP,
            ContextType.SECURITY_CONSIDERATION: MemoryCategory.SECURITY_PATTERN,
            ContextType.INTEGRATION_PATTERN: MemoryCategory.AI_CODING_PATTERN,
            ContextType.BUSINESS_LOGIC: MemoryCategory.CODE_DECISION,
            ContextType.ERROR_RESOLUTION: MemoryCategory.BUG_SOLUTION,
            ContextType.DEPLOYMENT_STRATEGY: MemoryCategory.WORKFLOW,
            ContextType.WORKFLOW_OPTIMIZATION: MemoryCategory.WORKFLOW,
        }
        return mapping.get(context_type, MemoryCategory.CODE_DECISION)

    def _extract_code_blocks(self, text: str) -> str | None:
        """Extract code blocks from text"""
        # Match code blocks (```...``` or `...`)
        code_patterns = [r"```[\w]*\n(.*?)\n```", r"`([^`\n]+)`"]

        code_blocks = []
        for pattern in code_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            code_blocks.extend(matches)

        if code_blocks:
            return "\n\n".join(code_blocks[:3])  # Limit to first 3 blocks
        return None

    def _merge_contexts(self, contexts: list[DetectedContext]) -> list[DetectedContext]:
        """Merge similar contexts and remove duplicates"""
        if not contexts:
            return []

        # Group by context type
        grouped = {}
        for ctx in contexts:
            key = ctx.context_type
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(ctx)

        # Merge contexts of the same type
        merged = []
        for context_type, ctx_list in grouped.items():
            if len(ctx_list) == 1:
                merged.append(ctx_list[0])
            else:
                # Merge multiple contexts of the same type
                best_ctx = max(ctx_list, key=lambda x: x.confidence)

                # Combine key points and tags
                all_key_points = []
                all_tags = set()
                all_files = set()

                for ctx in ctx_list:
                    all_key_points.extend(ctx.key_points)
                    all_tags.update(ctx.suggested_tags)
                    all_files.update(ctx.related_files)

                # Create merged context
                merged_ctx = DetectedContext(
                    context_type=context_type,
                    confidence=max(ctx.confidence for ctx in ctx_list),
                    key_points=list(set(all_key_points))[:10],  # Limit to 10 key points
                    suggested_category=best_ctx.suggested_category,
                    suggested_tags=list(all_tags)[:8],  # Limit to 8 tags
                    importance_score=max(ctx.importance_score for ctx in ctx_list),
                    reasoning=f"Merged from {len(ctx_list)} detections: "
                    + "; ".join(ctx.reasoning for ctx in ctx_list)[:200],
                    extracted_code=best_ctx.extracted_code,
                    related_files=list(all_files),
                )
                merged.append(merged_ctx)

        return merged


class AutoDiscoveryOrchestrator:
    """
    Orchestrates the automatic discovery and storage of development context
    """

    def __init__(self):
        self.context_detector = IntelligentContextDetector()
        self.ai_memory = None
        self.initialized = False
        self.discovery_stats = {
            "total_conversations_analyzed": 0,
            "contexts_detected": 0,
            "memories_stored": 0,
            "last_analysis": None,
        }

    async def initialize(self):
        """Initialize the auto-discovery system"""
        if self.initialized:
            return

        await self.context_detector.initialize()
        self.ai_memory = EnhancedAiMemoryMCPServer()
        await self.ai_memory.initialize()

        self.initialized = True
        logger.info("âœ… Auto-Discovery Orchestrator initialized")

    async def analyze_and_store_conversation(
        self,
        conversation: str,
        participants: list[str] = None,
        file_context: dict[str, Any] | None = None,
        auto_store: bool = True,
    ) -> dict[str, Any]:
        """
        Analyze conversation for important context and automatically store memories

        Args:
            conversation: The conversation text
            participants: List of participants
            file_context: Context about files being discussed
            auto_store: Whether to automatically store detected contexts

        Returns:
            Analysis results and storage status
        """
        if not self.initialized:
            await self.initialize()

        try:
            # Detect contexts
            detected_contexts = await self.context_detector.detect_context(
                conversation, file_context
            )

            self.discovery_stats["total_conversations_analyzed"] += 1
            self.discovery_stats["contexts_detected"] += len(detected_contexts)
            self.discovery_stats["last_analysis"] = datetime.now().isoformat()

            if not detected_contexts:
                return {
                    "status": "no_context_detected",
                    "message": "No significant development context detected",
                    "conversation_length": len(conversation),
                    "stats": self.discovery_stats,
                }

            # Store high-confidence contexts
            stored_memories = []

            if auto_store:
                for context in detected_contexts:
                    if context.confidence >= 0.7:  # High confidence threshold
                        try:
                            # Create comprehensive memory content
                            memory_content = self._create_memory_content(
                                conversation, context, participants, file_context
                            )

                            # Store in AI Memory
                            storage_result = await self.ai_memory.store_memory(
                                content=memory_content,
                                category=context.suggested_category,
                                tags=context.suggested_tags,
                                importance_score=context.importance_score,
                                auto_detected=True,
                            )

                            stored_memories.append(
                                {
                                    "context_type": context.context_type.value,
                                    "confidence": context.confidence,
                                    "storage_result": storage_result,
                                    "memory_id": storage_result.get("id"),
                                }
                            )

                            self.discovery_stats["memories_stored"] += 1

                        except Exception as e:
                            logger.error(f"Failed to store context memory: {e}")
                            stored_memories.append(
                                {
                                    "context_type": context.context_type.value,
                                    "confidence": context.confidence,
                                    "storage_result": {
                                        "status": "error",
                                        "error": str(e),
                                    },
                                    "memory_id": None,
                                }
                            )

            return {
                "status": "analysis_complete",
                "detected_contexts": len(detected_contexts),
                "high_confidence_contexts": len(
                    [ctx for ctx in detected_contexts if ctx.confidence >= 0.7]
                ),
                "stored_memories": len(stored_memories),
                "contexts": [
                    {
                        "type": ctx.context_type.value,
                        "confidence": ctx.confidence,
                        "key_points": ctx.key_points,
                        "importance": ctx.importance_score,
                        "reasoning": ctx.reasoning,
                    }
                    for ctx in detected_contexts
                ],
                "storage_results": stored_memories,
                "stats": self.discovery_stats,
            }

        except Exception as e:
            logger.error(f"Auto-discovery analysis failed: {e}")
            return {"status": "error", "error": str(e), "stats": self.discovery_stats}

    def _create_memory_content(
        self,
        conversation: str,
        context: DetectedContext,
        participants: list[str] = None,
        file_context: dict[str, Any] | None = None,
    ) -> str:
        """Create comprehensive memory content from detected context"""

        # Header with context information
        content_parts = [
            f"# {context.context_type.value.replace('_', ' ').title()}",
            f"**Confidence:** {context.confidence:.2f}",
            f"**Importance:** {context.importance_score:.2f}",
            f"**Detected:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        ]

        if participants:
            content_parts.append(f"**Participants:** {', '.join(participants)}")

        if file_context:
            content_parts.append(
                f"**File Context:** {file_context.get('file_name', 'Unknown')}"
            )

        # Key points
        if context.key_points:
            content_parts.append("\n## Key Points:")
            for point in context.key_points:
                content_parts.append(f"- {point}")

        # Extracted code if available
        if context.extracted_code:
            content_parts.append("\n## Code Examples:")
            content_parts.append(f"```\n{context.extracted_code}\n```")

        # Related files
        if context.related_files:
            content_parts.append("\n## Related Files:")
            for file in context.related_files:
                content_parts.append(f"- {file}")

        # Context and reasoning
        content_parts.append("\n## Context:")
        content_parts.append(context.reasoning)

        # Original conversation excerpt (limited)
        conversation_excerpt = (
            conversation[:1000] + "..." if len(conversation) > 1000 else conversation
        )
        content_parts.append("\n## Conversation Excerpt:")
        content_parts.append(f"```\n{conversation_excerpt}\n```")

        return "\n".join(content_parts)

    async def get_discovery_stats(self) -> dict[str, Any]:
        """Get auto-discovery statistics"""
        return {
            **self.discovery_stats,
            "initialized": self.initialized,
            "detector_available": self.context_detector.initialized,
            "ai_memory_available": (
                self.ai_memory.initialized if self.ai_memory else False
            ),
        }

    async def smart_recall(
        self,
        query: str,
        context_hint: str | None = None,
        file_context: dict[str, Any] | None = None,
        limit: int = 5,
    ) -> list[dict[str, Any]]:
        """
        Smart recall that considers current context and file awareness

        Args:
            query: Search query
            context_hint: Hint about the type of context being sought
            file_context: Current file context for relevance
            limit: Maximum results

        Returns:
            Contextually relevant memories
        """
        if not self.initialized:
            await self.initialize()

        try:
            # Enhance query with context
            enhanced_query = query

            if context_hint:
                enhanced_query += f" {context_hint}"

            if file_context and file_context.get("file_name"):
                file_name = file_context["file_name"]
                file_ext = file_name.split(".")[-1] if "." in file_name else ""
                enhanced_query += f" {file_name} {file_ext}"

            # Get memories from AI Memory system
            memories = await self.ai_memory.recall_memory(
                query=enhanced_query,
                limit=limit * 2,  # Get more to filter
            )

            # Filter and rank based on current context
            contextual_memories = []

            for memory in memories:
                relevance_score = memory.get("relevance_score", 0.5)

                # Boost relevance for file-related memories
                if file_context and file_context.get("file_name"):
                    file_name = file_context["file_name"]
                    memory_content = memory.get("content", "").lower()

                    if file_name.lower() in memory_content:
                        relevance_score += 0.2

                    # Check for similar file extensions
                    if "." in file_name:
                        file_ext = file_name.split(".")[-1]
                        if file_ext in memory_content:
                            relevance_score += 0.1

                # Boost relevance for auto-detected memories
                if memory.get("auto_detected"):
                    relevance_score += 0.1

                contextual_memories.append(
                    {**memory, "contextual_relevance": min(relevance_score, 1.0)}
                )

            # Sort by contextual relevance and limit results
            contextual_memories.sort(
                key=lambda x: x["contextual_relevance"], reverse=True
            )

            return contextual_memories[:limit]

        except Exception as e:
            logger.error(f"Smart recall failed: {e}")
            return []


# Global auto-discovery instance
auto_discovery = AutoDiscoveryOrchestrator()


async def analyze_conversation_auto(
    conversation: str,
    participants: list[str] = None,
    file_context: dict[str, Any] | None = None,
    auto_store: bool = True,
) -> dict[str, Any]:
    """
    Convenience function for automatic conversation analysis

    Args:
        conversation: Conversation text to analyze
        participants: List of participants
        file_context: File context information
        auto_store: Whether to automatically store detected contexts

    Returns:
        Analysis and storage results
    """
    return await auto_discovery.analyze_and_store_conversation(
        conversation=conversation,
        participants=participants,
        file_context=file_context,
        auto_store=auto_store,
    )


async def smart_memory_recall(
    query: str,
    context_hint: str | None = None,
    file_context: dict[str, Any] | None = None,
    limit: int = 5,
) -> list[dict[str, Any]]:
    """
    Convenience function for smart memory recall

    Args:
        query: Search query
        context_hint: Context hint for better results
        file_context: Current file context
        limit: Maximum results

    Returns:
        Contextually relevant memories
    """
    return await auto_discovery.smart_recall(
        query=query, context_hint=context_hint, file_context=file_context, limit=limit
    )


# Example usage
if __name__ == "__main__":

    async def test_auto_discovery():
        """Test the auto-discovery system"""

        test_conversation = """
        We decided to use Snowflake Cortex for our vector embeddings because it provides
        native integration with our data warehouse. The architecture decision was based on:

        1. Performance - native vector operations are faster
        2. Cost - reduces external dependencies on Pinecone for business data
        3. Security - data stays within our enterprise data warehouse

        Here's the implementation pattern we're using:

        ```python
        async def store_embedding_in_business_table(self, table_name: str, record_id: str):
            embedding = await cortex.embed_text(text_content)
            await self.update_record(table_name, record_id, embedding)
        ```

        This approach solved the SQL injection vulnerability we had before by using
        parameterized queries and whitelist validation.
        """

        file_context = {
            "file_name": "snowflake_cortex_service.py",
            "file_type": "py",
            "is_new_file": False,
        }

        result = await analyze_conversation_auto(
            conversation=test_conversation,
            participants=["developer", "architect"],
            file_context=file_context,
            auto_store=True,
        )

        print(json.dumps(result, indent=2, default=str))

        # Test smart recall
        recall_result = await smart_memory_recall(
            query="vector embedding architecture decision",
            context_hint="architecture",
            file_context=file_context,
            limit=3,
        )

        print("\n--- Smart Recall Results ---")
        print(json.dumps(recall_result, indent=2, default=str))

    asyncio.run(test_auto_discovery())
