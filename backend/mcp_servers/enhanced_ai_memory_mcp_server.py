"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 1519 lines

Recommended decomposition:
- enhanced_ai_memory_mcp_server_core.py - Core functionality
- enhanced_ai_memory_mcp_server_utils.py - Utility functions
- enhanced_ai_memory_mcp_server_models.py - Data models
- enhanced_ai_memory_mcp_server_handlers.py - Request handlers

TODO: Implement file decomposition
"""

from datetime import UTC, datetime

#!/usr/bin/env python3
"""
Enhanced AI Memory MCP Server
Extended with Slack, Linear, and Foundational Knowledge Base integration
"""

import asyncio
import json
import logging
from dataclasses import dataclass
from datetime import timedelta
from enum import Enum
from typing import Any

import pandas as pd

# MCP imports
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import TextContent, Tool

# Enhanced imports

logger = logging.getLogger(__name__)


class EnhancedMemoryCategory(Enum):
    """Extended memory categories for new data sources"""

    # Existing categories
    GONG_CALL_SUMMARY = "gong_call_summary"
    GONG_CALL_INSIGHT = "gong_call_insight"
    HUBSPOT_DEAL_ANALYSIS = "hubspot_deal_analysis"
    SALES_COACHING = "sales_coaching"
    BUSINESS_INTELLIGENCE = "business_intelligence"

    # New Slack categories
    SLACK_CONVERSATION = "slack_conversation"
    SLACK_INSIGHT = "slack_insight"
    SLACK_DECISION = "slack_decision"
    SLACK_ACTION_ITEM = "slack_action_item"
    SLACK_KNOWLEDGE_SHARE = "slack_knowledge_share"
    SLACK_TEAM_UPDATE = "slack_team_update"
    SLACK_CUSTOMER_FEEDBACK = "slack_customer_feedback"

    # New Linear categories
    LINEAR_ISSUE = "linear_issue"
    LINEAR_PROJECT = "linear_project"
    LINEAR_MILESTONE = "linear_milestone"
    LINEAR_ROADMAP = "linear_roadmap"
    LINEAR_TEAM_VELOCITY = "linear_team_velocity"
    LINEAR_SPRINT_SUMMARY = "linear_sprint_summary"
    LINEAR_FEATURE_REQUEST = "linear_feature_request"

    # New Foundational KB categories
    FOUNDATIONAL_EMPLOYEE = "foundational_employee"
    FOUNDATIONAL_CUSTOMER = "foundational_customer"
    FOUNDATIONAL_PRODUCT = "foundational_product"
    FOUNDATIONAL_COMPETITOR = "foundational_competitor"
    FOUNDATIONAL_PROCESS = "foundational_process"
    FOUNDATIONAL_VALUE = "foundational_value"

    # New Knowledge Base categories
    KB_ARTICLE = "kb_article"
    KB_ENTITY = "kb_entity"
    KB_DOCUMENT = "kb_document"
    KB_INSIGHT = "kb_insight"
    KB_FAQ = "kb_faq"
    KB_BEST_PRACTICE = "kb_best_practice"


@dataclass
class EnhancedMemoryMetadata:
    """Enhanced metadata for new memory types"""

    # Common fields
    source_type: str
    source_id: str
    confidence_score: float = 0.8
    importance_score: float = 0.5
    business_value_score: float = 0.5

    # Slack-specific fields
    slack_channel_id: str | None = None
    slack_channel_name: str | None = None
    slack_user_id: str | None = None
    slack_thread_ts: str | None = None
    slack_participants: list[str] | None = None

    # Linear-specific fields
    linear_project_id: str | None = None
    linear_project_name: str | None = None
    linear_team_id: str | None = None
    linear_assignee_id: str | None = None
    linear_priority: str | None = None
    linear_status: str | None = None

    # Foundational-specific fields
    foundational_type: str | None = None
    foundational_department: str | None = None
    foundational_tier: str | None = None

    # Knowledge base-specific fields
    kb_category: str | None = None
    kb_visibility: str | None = None
    kb_author: str | None = None
    kb_keywords: list[str] | None = None


class EnhancedAiMemoryMCPServer:
    """Enhanced AI Memory MCP Server with new data source integration"""

    def __init__(self):
        self.server_name = "enhanced-ai-memory"

        # Initialize basic attributes
        self.initialized = False
        self.cortex_service = None

        # Enhanced category exclusions
        self.category_exclusions = {
            # Exclude test and temporary categories
            "test_category",
            "temp_data",
            "debug_info",
        }

    async def store_slack_conversation_memory(
        self,
        conversation_id: str,
        conversation_title: str,
        conversation_summary: str,
        channel_name: str,
        participants: list[str],
        key_topics: list[str],
        decisions_made: list[str],
        action_items: list[str],
        business_value_score: float = 0.6,
    ) -> str:
        """Store Slack conversation as memory"""

        # Determine category based on content
        category = self._classify_slack_conversation(
            conversation_summary, decisions_made, action_items
        )

        # Create enhanced content
        content = f"""
        Slack Conversation: {conversation_title}
        Channel: #{channel_name}
        Participants: {", ".join(participants)}

        Summary: {conversation_summary}

        Key Topics: {", ".join(key_topics)}

        Decisions Made:
        {chr(10).join(f"- {decision}" for decision in decisions_made)}

        Action Items:
        {chr(10).join(f"- {item}" for item in action_items)}
        """

        # Enhanced metadata
        metadata = EnhancedMemoryMetadata(
            source_type="slack",
            source_id=conversation_id,
            business_value_score=business_value_score,
            slack_channel_name=channel_name,
            slack_participants=participants,
        )

        # Store memory
        memory_id = await self.store_memory(
            content=content,
            category=category.value,
            tags=["slack", "conversation", channel_name] + key_topics,
            metadata=metadata.__dict__,
            importance_score=business_value_score,
        )

        logger.info(f"Stored Slack conversation memory: {memory_id}")
        return memory_id

    async def store_linear_issue_memory(
        self,
        issue_id: str,
        issue_title: str,
        issue_description: str,
        project_name: str,
        assignee: str,
        priority: str,
        status: str,
        labels: list[str],
        importance_score: float = 0.5,
    ) -> str:
        """Store Linear issue as memory"""

        # Determine category based on labels and priority
        category = self._classify_linear_issue(labels, priority, status)

        # Create content
        content = f"""
        Linear Issue: {issue_title}
        Project: {project_name}
        Assignee: {assignee}
        Priority: {priority}
        Status: {status}

        Description: {issue_description}

        Labels: {", ".join(labels)}
        """

        # Enhanced metadata
        metadata = EnhancedMemoryMetadata(
            source_type="linear",
            source_id=issue_id,
            importance_score=importance_score,
            linear_project_name=project_name,
            linear_assignee_id=assignee,
            linear_priority=priority,
            linear_status=status,
        )

        # Store memory
        memory_id = await self.store_memory(
            content=content,
            category=category.value,
            tags=["linear", "issue", project_name, priority] + labels,
            metadata=metadata.__dict__,
            importance_score=importance_score,
        )

        logger.info(f"Stored Linear issue memory: {memory_id}")
        return memory_id

    async def store_foundational_knowledge_memory(
        self,
        record_id: str,
        record_type: str,
        title: str,
        description: str,
        category: str,
        department: str | None = None,
        importance_score: float = 0.7,
    ) -> str:
        """Store foundational knowledge as memory"""

        # Map record type to memory category
        category_map = {
            "employee": EnhancedMemoryCategory.FOUNDATIONAL_EMPLOYEE,
            "customer": EnhancedMemoryCategory.FOUNDATIONAL_CUSTOMER,
            "product": EnhancedMemoryCategory.FOUNDATIONAL_PRODUCT,
            "competitor": EnhancedMemoryCategory.FOUNDATIONAL_COMPETITOR,
            "business_process": EnhancedMemoryCategory.FOUNDATIONAL_PROCESS,
            "organizational_value": EnhancedMemoryCategory.FOUNDATIONAL_VALUE,
        }

        memory_category = category_map.get(
            record_type, EnhancedMemoryCategory.FOUNDATIONAL_EMPLOYEE
        )

        # Create content
        content = f"""
        Foundational Knowledge: {title}
        Type: {record_type.replace("_", " ").title()}
        Category: {category}
        {f"Department: {department}" if department else ""}

        Description: {description}
        """

        # Enhanced metadata
        metadata = EnhancedMemoryMetadata(
            source_type="foundational",
            source_id=record_id,
            importance_score=importance_score,
            foundational_type=record_type,
            foundational_department=department,
        )

        # Store memory
        memory_id = await self.store_memory(
            content=content,
            category=memory_category.value,
            tags=["foundational", record_type, category]
            + ([department] if department else []),
            metadata=metadata.__dict__,
            importance_score=importance_score,
        )

        logger.info(f"Stored foundational knowledge memory: {memory_id}")
        return memory_id

    async def store_kb_article_memory(
        self,
        article_id: str,
        title: str,
        content: str,
        category: str,
        author: str,
        keywords: list[str],
        visibility: str = "internal",
        importance_score: float = 0.6,
    ) -> str:
        """Store knowledge base article as memory"""

        # Create memory content
        memory_content = f"""
        Knowledge Base Article: {title}
        Category: {category}
        Author: {author}
        Visibility: {visibility}

        Content: {content[:2000]}{"..." if len(content) > 2000 else ""}

        Keywords: {", ".join(keywords)}
        """

        # Enhanced metadata
        metadata = EnhancedMemoryMetadata(
            source_type="knowledge_base",
            source_id=article_id,
            importance_score=importance_score,
            kb_category=category,
            kb_visibility=visibility,
            kb_author=author,
            kb_keywords=keywords,
        )

        # Store memory
        memory_id = await self.store_memory(
            content=memory_content,
            category=EnhancedMemoryCategory.KB_ARTICLE.value,
            tags=["knowledge_base", "article", category] + keywords,
            metadata=metadata.__dict__,
            importance_score=importance_score,
        )

        logger.info(f"Stored KB article memory: {memory_id}")
        return memory_id

    async def recall_slack_insights(
        self,
        query: str,
        channel_name: str | None = None,
        date_range_days: int = 30,
        limit: int = 10,
    ) -> list[dict[str, Any]]:
        """Recall Slack conversation insights"""

        # Build search filters
        filters = {
            "categories": [
                cat.value
                for cat in [
                    EnhancedMemoryCategory.SLACK_CONVERSATION,
                    EnhancedMemoryCategory.SLACK_INSIGHT,
                    EnhancedMemoryCategory.SLACK_DECISION,
                    EnhancedMemoryCategory.SLACK_ACTION_ITEM,
                ]
            ]
        }

        if channel_name:
            filters["tags"] = [channel_name]

        # Add date filter
        cutoff_date = datetime.now() - timedelta(days=date_range_days)
        filters["created_after"] = cutoff_date.isoformat()

        # Perform search
        results = await self.recall_memories(query=query, filters=filters, limit=limit)

        logger.info(f"Found {len(results)} Slack insights for query: {query}")
        return results

    async def recall_linear_issue_details(
        self,
        query: str,
        project_name: str | None = None,
        priority: str | None = None,
        status: str | None = None,
        limit: int = 10,
    ) -> list[dict[str, Any]]:
        """Recall Linear issue details"""

        # Build search filters
        filters = {
            "categories": [
                cat.value
                for cat in [
                    EnhancedMemoryCategory.LINEAR_ISSUE,
                    EnhancedMemoryCategory.LINEAR_PROJECT,
                    EnhancedMemoryCategory.LINEAR_FEATURE_REQUEST,
                ]
            ]
        }

        tags = ["linear"]
        if project_name:
            tags.append(project_name)
        if priority:
            tags.append(priority)
        if status:
            tags.append(status)

        filters["tags"] = tags

        # Perform search
        results = await self.recall_memories(query=query, filters=filters, limit=limit)

        logger.info(f"Found {len(results)} Linear issues for query: {query}")
        return results

    async def recall_foundational_knowledge(
        self,
        query: str,
        knowledge_type: str | None = None,
        department: str | None = None,
        limit: int = 10,
    ) -> list[dict[str, Any]]:
        """Recall foundational knowledge"""

        # Build search filters
        foundational_categories = [
            cat.value
            for cat in [
                EnhancedMemoryCategory.FOUNDATIONAL_EMPLOYEE,
                EnhancedMemoryCategory.FOUNDATIONAL_CUSTOMER,
                EnhancedMemoryCategory.FOUNDATIONAL_PRODUCT,
                EnhancedMemoryCategory.FOUNDATIONAL_COMPETITOR,
                EnhancedMemoryCategory.FOUNDATIONAL_PROCESS,
                EnhancedMemoryCategory.FOUNDATIONAL_VALUE,
            ]
        ]

        filters = {"categories": foundational_categories}

        tags = ["foundational"]
        if knowledge_type:
            tags.append(knowledge_type)
        if department:
            tags.append(department)

        filters["tags"] = tags

        # Perform search
        results = await self.recall_memories(query=query, filters=filters, limit=limit)

        logger.info(
            f"Found {len(results)} foundational knowledge items for query: {query}"
        )
        return results

    async def recall_kb_articles(
        self,
        query: str,
        category: str | None = None,
        author: str | None = None,
        limit: int = 10,
    ) -> list[dict[str, Any]]:
        """Recall knowledge base articles"""

        # Build search filters
        filters = {
            "categories": [
                cat.value
                for cat in [
                    EnhancedMemoryCategory.KB_ARTICLE,
                    EnhancedMemoryCategory.KB_INSIGHT,
                    EnhancedMemoryCategory.KB_FAQ,
                    EnhancedMemoryCategory.KB_BEST_PRACTICE,
                ]
            ]
        }

        tags = ["knowledge_base"]
        if category:
            tags.append(category)
        if author:
            tags.append(author)

        filters["tags"] = tags

        # Perform search
        results = await self.recall_memories(query=query, filters=filters, limit=limit)

        logger.info(f"Found {len(results)} KB articles for query: {query}")
        return results

    def _classify_slack_conversation(
        self, summary: str, decisions: list[str], action_items: list[str]
    ) -> EnhancedMemoryCategory:
        """Classify Slack conversation based on content"""

        summary_lower = summary.lower()

        if decisions:
            return EnhancedMemoryCategory.SLACK_DECISION
        elif action_items:
            return EnhancedMemoryCategory.SLACK_ACTION_ITEM
        elif any(
            keyword in summary_lower for keyword in ["customer", "client", "feedback"]
        ):
            return EnhancedMemoryCategory.SLACK_CUSTOMER_FEEDBACK
        elif any(
            keyword in summary_lower for keyword in ["update", "status", "progress"]
        ):
            return EnhancedMemoryCategory.SLACK_TEAM_UPDATE
        elif any(
            keyword in summary_lower
            for keyword in ["knowledge", "share", "tip", "learn"]
        ):
            return EnhancedMemoryCategory.SLACK_KNOWLEDGE_SHARE
        else:
            return EnhancedMemoryCategory.SLACK_CONVERSATION

    def _classify_linear_issue(
        self, labels: list[str], priority: str, status: str
    ) -> EnhancedMemoryCategory:
        """Classify Linear issue based on attributes"""

        labels_lower = [label.lower() for label in labels]

        if "feature" in labels_lower or "enhancement" in labels_lower:
            return EnhancedMemoryCategory.LINEAR_FEATURE_REQUEST
        elif "milestone" in labels_lower or "epic" in labels_lower:
            return EnhancedMemoryCategory.LINEAR_MILESTONE
        elif priority.lower() in ["urgent", "high"]:
            return EnhancedMemoryCategory.LINEAR_ISSUE
        else:
            return EnhancedMemoryCategory.LINEAR_ISSUE

    async def store_gong_call_insight(
        self,
        call_id: str,
        participant_data: dict[str, Any],
        transcript_data: dict[str, Any],
        call_metadata: dict[str, Any],
        analysis_data: dict[str, Any],
        user_id: str,
        importance_score: float = 0.8,
        tags: list[str] = None,
        custom_metadata: dict[str, Any] = None,
        correlation_id: str = None,
    ) -> dict[str, Any]:
        """Store Gong call insight with comprehensive validation and processing"""
        try:
            # Validate input data
            validation_result = await self._validate_gong_insight_data(
                call_id, participant_data, transcript_data, call_metadata, analysis_data
            )
            if not validation_result["valid"]:
                return {"success": False, "error": validation_result["error"]}

            # Process and enrich data
            enriched_data = await self._process_gong_insight_data(
                participant_data, transcript_data, call_metadata, analysis_data
            )

            # Generate memory content
            memory_content = await self._generate_gong_memory_content(
                call_id, enriched_data, analysis_data
            )

            # Store in AI Memory
            memory_id = await self._store_gong_memory(
                memory_content,
                call_id,
                user_id,
                importance_score,
                tags,
                custom_metadata,
            )

            # Update analytics and correlations
            await self._update_gong_analytics(call_id, memory_id, correlation_id)

            return self._format_gong_insight_response(memory_id, call_id, enriched_data)

        except Exception as e:
            return self._handle_gong_insight_error(e, call_id)

    async def _validate_gong_insight_data(
        self,
        call_id: str,
        participant_data: dict,
        transcript_data: dict,
        call_metadata: dict,
        analysis_data: dict,
    ) -> dict[str, Any]:
        """Validate Gong insight data for completeness and format"""
        if not call_id or not isinstance(call_id, str):
            return {"valid": False, "error": "Invalid call_id"}

        if not participant_data or not isinstance(participant_data, dict):
            return {"valid": False, "error": "Invalid participant_data"}

        if not transcript_data or not isinstance(transcript_data, dict):
            return {"valid": False, "error": "Invalid transcript_data"}

        # Additional validation logic here
        return {"valid": True}

    async def _process_gong_insight_data(
        self,
        participant_data: dict,
        transcript_data: dict,
        call_metadata: dict,
        analysis_data: dict,
    ) -> dict[str, Any]:
        """Process and enrich Gong insight data"""
        enriched_data = {
            "participants": self._process_participants(participant_data),
            "transcript": self._process_transcript(transcript_data),
            "metadata": self._process_metadata(call_metadata),
            "analysis": self._process_analysis(analysis_data),
        }
        return enriched_data

    async def _generate_gong_memory_content(
        self, call_id: str, enriched_data: dict, analysis_data: dict
    ) -> str:
        """Generate structured memory content from Gong data"""
        content_parts = [
            f"Gong Call Analysis - ID: {call_id}",
            f"Participants: {len(enriched_data['participants'])}",
            f"Key Topics: {', '.join(analysis_data.get('topics', []))}",
            f"Sentiment: {analysis_data.get('sentiment', 'neutral')}",
            f"Action Items: {len(analysis_data.get('action_items', []))}",
        ]
        return "\n".join(content_parts)

    async def _store_gong_memory(
        self,
        content: str,
        call_id: str,
        user_id: str,
        importance_score: float,
        tags: list[str],
        custom_metadata: dict,
    ) -> str:
        """Store processed Gong data in AI Memory"""
        memory_tags = ["gong_call", "sales_insight"] + (tags or [])

        metadata = {
            "call_id": call_id,
            "source": "gong",
            "type": "call_insight",
            **(custom_metadata or {}),
        }

        return await self.store_memory(
            content=content,
            category="SALES_CALL_INSIGHT",
            tags=memory_tags,
            importance_score=importance_score,
            metadata=metadata,
        )

    async def _update_gong_analytics(
        self, call_id: str, memory_id: str, correlation_id: str
    ):
        """Update analytics and correlation tracking"""
        # Analytics update logic here
        pass

    def _format_gong_insight_response(
        self, memory_id: str, call_id: str, enriched_data: dict
    ) -> dict[str, Any]:
        """Format the final response for Gong insight storage"""
        return {
            "success": True,
            "memory_id": memory_id,
            "call_id": call_id,
            "participants_processed": len(enriched_data["participants"]),
            "transcript_length": len(enriched_data["transcript"]),
            "stored_at": datetime.now(UTC).isoformat(),
        }

    def _handle_gong_insight_error(
        self, error: Exception, call_id: str
    ) -> dict[str, Any]:
        """Handle errors in Gong insight processing"""
        logger.error(f"Error storing Gong insight for call {call_id}: {error}")
        return {"success": False, "error": str(error), "call_id": call_id}

    def _process_participants(self, participant_data: dict) -> list[dict]:
        """Process participant data"""
        return participant_data.get("participants", [])

    def _process_transcript(self, transcript_data: dict) -> str:
        """Process transcript data"""
        return transcript_data.get("transcript", "")

    def _process_metadata(self, call_metadata: dict) -> dict:
        """Process call metadata"""
        return call_metadata

    def _process_analysis(self, analysis_data: dict) -> dict:
        """Process analysis data"""
        return analysis_data

    async def recall_gong_call_insights(
        self,
        query: str,
        call_id: str | None = None,
        sentiment_filter: str | None = None,
        date_range_days: int | None = None,
        limit: int = 5,
        use_cortex_search: bool = True,
    ) -> list[dict[str, Any]]:
        """
        Enhanced Gong call insights recall with STG_GONG_CALLS integration

        Args:
            query: Search query for insights
            call_id: Filter by specific call ID
            sentiment_filter: Filter by sentiment ('positive', 'negative', 'neutral')
            date_range_days: Filter calls from last N days
            limit: Maximum number of results
            use_cortex_search: Whether to use Snowflake Cortex for vector search

        Returns:
            List of relevant Gong call insights with enhanced metadata
        """
        if not self.initialized:
            await self.initialize()

        try:
            # Use Snowflake Cortex vector search for enhanced results
            if use_cortex_search and self.cortex_service:
                cortex_results = (
                    await self.cortex_service.search_gong_calls_with_ai_memory(
                        query_text=query,
                        top_k=limit,
                        similarity_threshold=0.7,
                        sentiment_filter=sentiment_filter,
                        date_range_days=date_range_days,
                    )
                )

                if cortex_results:
                    # Convert Cortex results to enhanced format
                    enhanced_results = []
                    for result in cortex_results:
                        enhanced_result = {
                            "id": result["call_id"],
                            "content": result["ai_insights"]["call_summary"]
                            or result["call_title"],
                            "category": EnhancedMemoryCategory.GONG_CALL_INSIGHT,
                            "relevance_score": result["search_metadata"][
                                "similarity_score"
                            ],
                            "call_id": result["call_id"],
                            "call_title": result["call_title"],
                            "call_datetime": result["call_datetime"],
                            "account_name": result["account_info"]["account_name"],
                            "deal_stage": result["account_info"]["deal_stage"],
                            "deal_value": result["account_info"]["deal_value"],
                            "sentiment_score": result["ai_insights"]["sentiment_score"],
                            "key_topics": result["ai_insights"]["key_topics"],
                            "risk_indicators": result["ai_insights"]["risk_indicators"],
                            "next_steps": result["ai_insights"]["next_steps"],
                            "primary_user": result["primary_user"],
                            "call_direction": result["call_direction"],
                            "call_duration_seconds": result["call_duration_seconds"],
                            "talk_ratio": result["ai_insights"]["talk_ratio"],
                            "created_at": result["search_metadata"][
                                "ai_memory_updated_at"
                            ],
                            "embedding_source": "snowflake_cortex",
                            "storage_location": "STG_GONG_CALLS",
                            "tags": [],
                        }
                        enhanced_results.append(enhanced_result)

                    logger.info(
                        f"Found {len(enhanced_results)} Gong call insights via Snowflake Cortex search"
                    )
                    return enhanced_results

            # Fallback to Pinecone search for non-Cortex categories or if Cortex unavailable
            embedding = await self.get_embedding(query)
            results = []

            if self.pinecone_index and embedding:
                try:
                    # Build filter for Gong-specific categories
                    filter_dict = {
                        "category": {
                            "$in": [
                                EnhancedMemoryCategory.GONG_CALL_INSIGHT.value,
                                EnhancedMemoryCategory.GONG_CALL_SUMMARY.value,
                                EnhancedMemoryCategory.GONG_COACHING_RECOMMENDATION.value,
                                EnhancedMemoryCategory.GONG_SENTIMENT_ANALYSIS.value,
                                EnhancedMemoryCategory.GONG_TOPIC_ANALYSIS.value,
                            ]
                        }
                    }

                    # Add additional filters
                    if call_id:
                        filter_dict["call_id"] = call_id

                    query_response = self.pinecone_index.query(
                        vector=embedding,
                        filter=filter_dict,
                        top_k=limit * 2,  # Get more to allow for filtering
                        include_metadata=True,
                    )

                    for match in query_response.matches:
                        if match.score >= 0.7:  # Similarity threshold
                            metadata = match.metadata

                            # Apply sentiment filter if specified
                            if sentiment_filter:
                                sentiment_score = metadata.get("sentiment_score", 0)
                                if (
                                    (
                                        sentiment_filter.lower() == "positive"
                                        and sentiment_score <= 0.3
                                    )
                                    or (
                                        sentiment_filter.lower() == "negative"
                                        and sentiment_score >= -0.3
                                    )
                                    or sentiment_filter.lower() == "neutral"
                                    and not (-0.3 <= sentiment_score <= 0.3)
                                ):
                                    continue

                            result = {
                                "id": match.id,
                                "content": metadata.get("content", ""),
                                "category": metadata.get("category", ""),
                                "relevance_score": match.score,
                                "call_id": metadata.get("call_id", ""),
                                "sentiment_score": metadata.get("sentiment_score"),
                                "key_topics": metadata.get("key_topics", []),
                                "risk_indicators": metadata.get("risk_indicators", []),
                                "next_steps": metadata.get("next_steps", []),
                                "call_summary": metadata.get("call_summary", ""),
                                "embedding_source": metadata.get(
                                    "embedding_source", "pinecone"
                                ),
                                "storage_location": metadata.get(
                                    "storage_location", "pinecone"
                                ),
                                "created_at": metadata.get("created_at", ""),
                                "tags": metadata.get("tags", []),
                            }
                            results.append(result)

                    # Sort by relevance and limit results
                    results.sort(key=lambda x: x["relevance_score"], reverse=True)
                    results = results[:limit]

                    logger.info(
                        f"Found {len(results)} Gong call insights via Pinecone search"
                    )

                except Exception as e:
                    logger.error(f"Error querying Pinecone for Gong insights: {e}")

            return results

        except Exception as e:
            logger.error(f"Error recalling Gong call insights: {e}")
            return []

    async def store_gong_transcript_insight(
        self,
        transcript_id: str,
        call_id: str,
        speaker_name: str,
        insight_content: str,
        transcript_text: str = None,
        segment_sentiment: float | None = None,
        extracted_entities: list[str] = None,
        key_phrases: list[str] = None,
        tags: list[str] = None,
    ) -> dict[str, Any]:
        """
        Store Gong transcript segment insight with STG_GONG_CALL_TRANSCRIPTS integration

        Args:
            transcript_id: Unique transcript segment ID
            call_id: Associated call ID
            speaker_name: Name of the speaker
            insight_content: AI-generated insight about this segment
            transcript_text: Raw transcript text
            segment_sentiment: Sentiment score for this segment
            extracted_entities: List of extracted entities
            key_phrases: List of key phrases
            tags: Additional tags

        Returns:
            Storage result with embedding and metadata
        """
        if not self.initialized:
            await self.initialize()

        try:
            # Generate content for embedding
            content_parts = [insight_content]

            if transcript_text:
                content_parts.append(f"Transcript: {transcript_text}")

            if extracted_entities:
                content_parts.append(f"Entities: {', '.join(extracted_entities)}")

            if key_phrases:
                content_parts.append(f"Key Phrases: {', '.join(key_phrases)}")

            full_content = "\n".join(content_parts)

            # Generate embedding
            embedding = await self.get_embedding(full_content)

            # Enhanced metadata
            metadata = {
                "transcript_id": transcript_id,
                "call_id": call_id,
                "speaker_name": speaker_name,
                "insight_type": "gong_transcript_analysis",
                "segment_sentiment": segment_sentiment,
                "extracted_entities": extracted_entities or [],
                "key_phrases": key_phrases or [],
                "transcript_text": transcript_text,
                "embedding_source": "openai",
                "storage_location": "STG_GONG_CALL_TRANSCRIPTS",
                "tags": tags or [],
                "created_at": datetime.now().isoformat(),
            }

            # Store in Pinecone
            memory_id = (
                f"gong_transcript_{transcript_id}_{int(datetime.now().timestamp())}"
            )

            if self.pinecone_index and embedding:
                try:
                    self.pinecone_index.upsert(
                        [
                            {
                                "id": memory_id,
                                "values": embedding,
                                "metadata": {
                                    **metadata,
                                    "category": EnhancedMemoryCategory.GONG_CALL_INSIGHT.value,
                                    "content": full_content[:1000],
                                },
                            }
                        ]
                    )

                    logger.info(
                        f"Stored Gong transcript insight in Pinecone: {memory_id}"
                    )
                except Exception as e:
                    logger.error(f"Failed to store transcript insight in Pinecone: {e}")

            # Update STG_GONG_CALL_TRANSCRIPTS table
            if self.cortex_service:
                try:
                    async with self.cortex_service as cortex:
                        update_sql = f"""
                        UPDATE STG_TRANSFORMED.STG_GONG_CALL_TRANSCRIPTS
                        SET
                            AI_MEMORY_EMBEDDING = PARSE_JSON('{json.dumps(embedding)}'),
                            AI_MEMORY_METADATA = PARSE_JSON('{json.dumps(metadata)}'),
                            AI_MEMORY_UPDATED_AT = CURRENT_TIMESTAMP(),
                            SEGMENT_SENTIMENT = COALESCE(SEGMENT_SENTIMENT, {segment_sentiment or "NULL"}),
                            EXTRACTED_ENTITIES = COALESCE(EXTRACTED_ENTITIES, PARSE_JSON('{json.dumps(extracted_entities or [])}')),
                            KEY_PHRASES = COALESCE(KEY_PHRASES, PARSE_JSON('{json.dumps(key_phrases or [])}'))
                        WHERE TRANSCRIPT_ID = '{transcript_id}'
                        """

                        await cortex.execute_query(update_sql)
                        logger.info(
                            f"Updated STG_GONG_CALL_TRANSCRIPTS with AI Memory data: {transcript_id}"
                        )

                except Exception as e:
                    logger.warning(f"Failed to update STG_GONG_CALL_TRANSCRIPTS: {e}")

            return {
                "success": True,
                "memory_id": memory_id,
                "transcript_id": transcript_id,
                "call_id": call_id,
                "embedding_dimensions": len(embedding) if embedding else 0,
                "metadata": metadata,
                "content_length": len(full_content),
            }

        except Exception as e:
            logger.error(f"Error storing Gong transcript insight: {e}")
            return {
                "success": False,
                "error": str(e),
                "transcript_id": transcript_id,
            }

    async def search_gong_insights_by_account(
        self,
        account_name: str,
        limit: int = 10,
        include_transcripts: bool = False,
    ) -> list[dict[str, Any]]:
        """
        Search for all Gong insights related to a specific account

        Args:
            account_name: Name of the account to search for
            limit: Maximum number of results
            include_transcripts: Whether to include transcript-level insights

        Returns:
            List of Gong insights for the specified account
        """
        if not self.initialized:
            await self.initialize()

        try:
            results = []

            # Search call-level insights
            call_insights = await self.recall_gong_call_insights(
                query=f"account {account_name}", limit=limit, use_cortex_search=True
            )

            # Filter by exact account name match
            for insight in call_insights:
                if insight.get("account_name", "").lower() == account_name.lower():
                    insight["insight_level"] = "call"
                    results.append(insight)

            # Search transcript-level insights if requested
            if include_transcripts and self.cortex_service:
                try:
                    transcript_results = await self.cortex_service.search_gong_transcripts_with_ai_memory(
                        query_text=account_name, top_k=limit, similarity_threshold=0.7
                    )

                    for transcript in transcript_results:
                        if (
                            transcript.get("call_context", {})
                            .get("account_name", "")
                            .lower()
                            == account_name.lower()
                        ):
                            # Convert transcript format to standard insight format
                            insight = {
                                "id": transcript["transcript_id"],
                                "content": transcript["content"]["segment_summary"]
                                or transcript["content"]["transcript_text"],
                                "category": EnhancedMemoryCategory.GONG_CALL_INSIGHT,
                                "relevance_score": transcript["ai_insights"][
                                    "similarity_score"
                                ],
                                "call_id": transcript["call_id"],
                                "account_name": transcript["call_context"][
                                    "account_name"
                                ],
                                "speaker": transcript["speaker"],
                                "timing": transcript["timing"],
                                "segment_sentiment": transcript["ai_insights"][
                                    "segment_sentiment"
                                ],
                                "insight_level": "transcript",
                                "extracted_entities": transcript["content"][
                                    "extracted_entities"
                                ],
                                "key_phrases": transcript["content"]["key_phrases"],
                            }
                            results.append(insight)

                except Exception as e:
                    logger.warning(f"Failed to search transcript insights: {e}")

            # Sort by relevance score and limit results
            results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
            results = results[:limit]

            logger.info(
                f"Found {len(results)} Gong insights for account: {account_name}"
            )
            return results

        except Exception as e:
            logger.error(f"Error searching Gong insights by account: {e}")
            return []

    async def get_gong_memory_statistics(self) -> dict[str, Any]:
        """
        Get comprehensive statistics about Gong data in AI Memory

        Returns:
            Dictionary with statistics about Gong memory storage
        """
        if not self.initialized:
            await self.initialize()

        try:
            stats = {
                "pinecone_stats": {},
                "snowflake_stats": {},
                "ai_memory_stats": {},
                "generated_at": datetime.now().isoformat(),
            }

            # Get Pinecone statistics
            if self.pinecone_index:
                try:
                    # Query for Gong-related memories in Pinecone
                    gong_categories = [
                        EnhancedMemoryCategory.GONG_CALL_INSIGHT.value,
                        EnhancedMemoryCategory.GONG_CALL_SUMMARY.value,
                        EnhancedMemoryCategory.GONG_COACHING_RECOMMENDATION.value,
                        EnhancedMemoryCategory.GONG_SENTIMENT_ANALYSIS.value,
                        EnhancedMemoryCategory.GONG_TOPIC_ANALYSIS.value,
                    ]

                    # Note: Pinecone doesn't have a direct count API, so we estimate
                    stats["pinecone_stats"] = {
                        "gong_categories_tracked": len(gong_categories),
                        "status": "connected",
                    }

                except Exception as e:
                    stats["pinecone_stats"] = {"error": str(e)}

            # Get Snowflake statistics
            if self.cortex_service:
                try:
                    async with self.cortex_service as cortex:
                        # STG_GONG_CALLS statistics
                        calls_stats_sql = """
                        SELECT
                            COUNT(*) as total_calls,
                            COUNT(CASE WHEN AI_MEMORY_EMBEDDING IS NOT NULL THEN 1 END) as calls_with_embeddings,
                            COUNT(CASE WHEN CALL_SUMMARY IS NOT NULL THEN 1 END) as calls_with_summaries,
                            COUNT(CASE WHEN KEY_TOPICS IS NOT NULL THEN 1 END) as calls_with_topics,
                            AVG(SENTIMENT_SCORE) as avg_sentiment_score,
                            MAX(AI_MEMORY_UPDATED_AT) as latest_ai_memory_update
                        FROM STG_TRANSFORMED.STG_GONG_CALLS
                        """

                        calls_result = await cortex.execute_query(calls_stats_sql)

                        if not calls_result.empty:
                            row = calls_result.iloc[0]
                            stats["snowflake_stats"]["stg_gong_calls"] = {
                                "total_calls": int(row["TOTAL_CALLS"]),
                                "calls_with_embeddings": int(
                                    row["CALLS_WITH_EMBEDDINGS"]
                                ),
                                "calls_with_summaries": int(
                                    row["CALLS_WITH_SUMMARIES"]
                                ),
                                "calls_with_topics": int(row["CALLS_WITH_TOPICS"]),
                                "avg_sentiment_score": (
                                    float(row["AVG_SENTIMENT_SCORE"])
                                    if pd.notna(row["AVG_SENTIMENT_SCORE"])
                                    else None
                                ),
                                "latest_ai_memory_update": (
                                    row["LATEST_AI_MEMORY_UPDATE"].isoformat()
                                    if pd.notna(row["LATEST_AI_MEMORY_UPDATE"])
                                    else None
                                ),
                                "embedding_coverage_percent": round(
                                    (
                                        (
                                            row["CALLS_WITH_EMBEDDINGS"]
                                            / row["TOTAL_CALLS"]
                                            * 100
                                        )
                                        if row["TOTAL_CALLS"] > 0
                                        else 0
                                    ),
                                    1,
                                ),
                            }

                        # STG_GONG_CALL_TRANSCRIPTS statistics
                        transcripts_stats_sql = """
                        SELECT
                            COUNT(*) as total_transcripts,
                            COUNT(CASE WHEN AI_MEMORY_EMBEDDING IS NOT NULL THEN 1 END) as transcripts_with_embeddings,
                            COUNT(CASE WHEN SEGMENT_SENTIMENT IS NOT NULL THEN 1 END) as transcripts_with_sentiment,
                            COUNT(DISTINCT CALL_ID) as unique_calls_with_transcripts
                        FROM STG_TRANSFORMED.STG_GONG_CALL_TRANSCRIPTS
                        """

                        transcripts_result = await cortex.execute_query(
                            transcripts_stats_sql
                        )

                        if not transcripts_result.empty:
                            row = transcripts_result.iloc[0]
                            stats["snowflake_stats"]["stg_gong_call_transcripts"] = {
                                "total_transcripts": int(row["TOTAL_TRANSCRIPTS"]),
                                "transcripts_with_embeddings": int(
                                    row["TRANSCRIPTS_WITH_EMBEDDINGS"]
                                ),
                                "transcripts_with_sentiment": int(
                                    row["TRANSCRIPTS_WITH_SENTIMENT"]
                                ),
                                "unique_calls_with_transcripts": int(
                                    row["UNIQUE_CALLS_WITH_TRANSCRIPTS"]
                                ),
                                "embedding_coverage_percent": round(
                                    (
                                        (
                                            row["TRANSCRIPTS_WITH_EMBEDDINGS"]
                                            / row["TOTAL_TRANSCRIPTS"]
                                            * 100
                                        )
                                        if row["TOTAL_TRANSCRIPTS"] > 0
                                        else 0
                                    ),
                                    1,
                                ),
                            }

                        # AI_MEMORY.MEMORY_RECORDS statistics
                        memory_stats_sql = """
                        SELECT
                            COUNT(*) as total_gong_memories,
                            COUNT(CASE WHEN EMBEDDING IS NOT NULL THEN 1 END) as memories_with_embeddings,
                            COUNT(DISTINCT CATEGORY) as unique_categories,
                            MAX(CREATED_AT) as latest_memory_created,
                            MAX(UPDATED_AT) as latest_memory_updated
                        FROM AI_MEMORY.MEMORY_RECORDS
                        WHERE SOURCE_TYPE = 'gong'
                        """

                        memory_result = await cortex.execute_query(memory_stats_sql)

                        if not memory_result.empty:
                            row = memory_result.iloc[0]
                            stats["ai_memory_stats"] = {
                                "total_gong_memories": int(row["TOTAL_GONG_MEMORIES"]),
                                "memories_with_embeddings": int(
                                    row["MEMORIES_WITH_EMBEDDINGS"]
                                ),
                                "unique_categories": int(row["UNIQUE_CATEGORIES"]),
                                "latest_memory_created": (
                                    row["LATEST_MEMORY_CREATED"].isoformat()
                                    if pd.notna(row["LATEST_MEMORY_CREATED"])
                                    else None
                                ),
                                "latest_memory_updated": (
                                    row["LATEST_MEMORY_UPDATED"].isoformat()
                                    if pd.notna(row["LATEST_MEMORY_UPDATED"])
                                    else None
                                ),
                            }

                except Exception as e:
                    stats["snowflake_stats"] = {"error": str(e)}

            logger.info("Generated Gong memory statistics")
            return stats

        except Exception as e:
            logger.error(f"Error generating Gong memory statistics: {e}")
            return {"error": str(e), "generated_at": datetime.now().isoformat()}


# MCP Server setup
server = Server("enhanced-ai-memory")
enhanced_ai_memory = EnhancedAiMemoryMCPServer()


@server.list_tools()
async def list_tools() -> list[Tool]:
    """List available AI Memory tools"""
    return [
        Tool(
            name="store_slack_conversation",
            description="Store Slack conversation as memory",
            inputSchema={
                "type": "object",
                "properties": {
                    "conversation_id": {"type": "string"},
                    "conversation_title": {"type": "string"},
                    "conversation_summary": {"type": "string"},
                    "channel_name": {"type": "string"},
                    "participants": {"type": "array", "items": {"type": "string"}},
                    "key_topics": {"type": "array", "items": {"type": "string"}},
                    "decisions_made": {"type": "array", "items": {"type": "string"}},
                    "action_items": {"type": "array", "items": {"type": "string"}},
                    "business_value_score": {"type": "number", "default": 0.6},
                },
                "required": [
                    "conversation_id",
                    "conversation_title",
                    "conversation_summary",
                    "channel_name",
                ],
            },
        ),
        Tool(
            name="store_linear_issue",
            description="Store Linear issue as memory",
            inputSchema={
                "type": "object",
                "properties": {
                    "issue_id": {"type": "string"},
                    "issue_title": {"type": "string"},
                    "issue_description": {"type": "string"},
                    "project_name": {"type": "string"},
                    "assignee": {"type": "string"},
                    "priority": {"type": "string"},
                    "status": {"type": "string"},
                    "labels": {"type": "array", "items": {"type": "string"}},
                    "importance_score": {"type": "number", "default": 0.5},
                },
                "required": [
                    "issue_id",
                    "issue_title",
                    "issue_description",
                    "project_name",
                ],
            },
        ),
        Tool(
            name="store_foundational_knowledge",
            description="Store foundational knowledge as memory",
            inputSchema={
                "type": "object",
                "properties": {
                    "record_id": {"type": "string"},
                    "record_type": {"type": "string"},
                    "title": {"type": "string"},
                    "description": {"type": "string"},
                    "category": {"type": "string"},
                    "department": {"type": "string"},
                    "importance_score": {"type": "number", "default": 0.7},
                },
                "required": [
                    "record_id",
                    "record_type",
                    "title",
                    "description",
                    "category",
                ],
            },
        ),
        Tool(
            name="store_kb_article",
            description="Store knowledge base article as memory",
            inputSchema={
                "type": "object",
                "properties": {
                    "article_id": {"type": "string"},
                    "title": {"type": "string"},
                    "content": {"type": "string"},
                    "category": {"type": "string"},
                    "author": {"type": "string"},
                    "keywords": {"type": "array", "items": {"type": "string"}},
                    "visibility": {"type": "string", "default": "internal"},
                    "importance_score": {"type": "number", "default": 0.6},
                },
                "required": ["article_id", "title", "content", "category", "author"],
            },
        ),
        Tool(
            name="recall_slack_insights",
            description="Recall Slack conversation insights",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "channel_name": {"type": "string"},
                    "date_range_days": {"type": "number", "default": 30},
                    "limit": {"type": "number", "default": 10},
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="recall_linear_issues",
            description="Recall Linear issue details",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "project_name": {"type": "string"},
                    "priority": {"type": "string"},
                    "status": {"type": "string"},
                    "limit": {"type": "number", "default": 10},
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="recall_foundational_knowledge",
            description="Recall foundational knowledge",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "knowledge_type": {"type": "string"},
                    "department": {"type": "string"},
                    "limit": {"type": "number", "default": 10},
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="recall_kb_articles",
            description="Recall knowledge base articles",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"},
                    "category": {"type": "string"},
                    "author": {"type": "string"},
                    "limit": {"type": "number", "default": 10},
                },
                "required": ["query"],
            },
        ),
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict[str, Any]) -> list[TextContent]:
    """Execute AI Memory tools"""
    try:
        if name == "store_slack_conversation":
            result = await enhanced_ai_memory.store_slack_conversation_memory(
                **arguments
            )
            return [
                TextContent(
                    type="text", text=f"Stored Slack conversation memory: {result}"
                )
            ]

        elif name == "store_linear_issue":
            result = await enhanced_ai_memory.store_linear_issue_memory(**arguments)
            return [
                TextContent(type="text", text=f"Stored Linear issue memory: {result}")
            ]

        elif name == "store_foundational_knowledge":
            result = await enhanced_ai_memory.store_foundational_knowledge_memory(
                **arguments
            )
            return [
                TextContent(
                    type="text", text=f"Stored foundational knowledge memory: {result}"
                )
            ]

        elif name == "store_kb_article":
            result = await enhanced_ai_memory.store_kb_article_memory(**arguments)
            return [
                TextContent(type="text", text=f"Stored KB article memory: {result}")
            ]

        elif name == "recall_slack_insights":
            results = await enhanced_ai_memory.recall_slack_insights(**arguments)
            return [TextContent(type="text", text=json.dumps(results, indent=2))]

        elif name == "recall_linear_issues":
            results = await enhanced_ai_memory.recall_linear_issue_details(**arguments)
            return [TextContent(type="text", text=json.dumps(results, indent=2))]

        elif name == "recall_foundational_knowledge":
            results = await enhanced_ai_memory.recall_foundational_knowledge(
                **arguments
            )
            return [TextContent(type="text", text=json.dumps(results, indent=2))]

        elif name == "recall_kb_articles":
            results = await enhanced_ai_memory.recall_kb_articles(**arguments)
            return [TextContent(type="text", text=json.dumps(results, indent=2))]

        else:
            return [TextContent(type="text", text=f"Unknown tool: {name}")]

    except Exception as e:
        logger.error(f"Tool execution failed: {e}")
        return [TextContent(type="text", text=f"Error: {str(e)}")]


async def main():
    """Run the Enhanced AI Memory MCP server"""
    try:
        await enhanced_ai_memory.initialize()
        logger.info(" Enhanced AI Memory MCP Server initialized")

        async with stdio_server() as (read_stream, write_stream):
            await server.run(
                read_stream, write_stream, server.create_initialization_options()
            )

    except KeyboardInterrupt:
        logger.info("Shutting down Enhanced AI Memory MCP Server")
    except Exception as e:
        logger.error(f"Server error: {e}")


if __name__ == "__main__":
    asyncio.run(main())
