"""
CoStar MCP Server for Sophia AI

Model Context Protocol server for processing CoStar real estate market data
Provides file watching, data ingestion, and market intelligence capabilities
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 623 lines

Recommended decomposition:
- costar_mcp_server_core.py - Core functionality
- costar_mcp_server_utils.py - Utility functions  
- costar_mcp_server_models.py - Data models
- costar_mcp_server_handlers.py - Request handlers

TODO: Implement file decomposition
"""

from __future__ import annotations

import asyncio
import hashlib
import logging
from datetime import datetime, date
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
import asyncpg
from pydantic import BaseModel

from backend.core.auto_esc_config import config
from backend.core.integration_registry import IntegrationRegistry

logger = logging.getLogger(__name__)


class CoStarDataRecord(BaseModel):
    """Represents a single CoStar market data record."""

    metro_area: str
    property_type: Optional[str] = None
    submarket: Optional[str] = None
    total_inventory: Optional[int] = None
    vacancy_rate: Optional[float] = None
    asking_rent_psf: Optional[float] = None
    effective_rent_psf: Optional[float] = None
    net_absorption: Optional[int] = None
    construction_deliveries: Optional[int] = None
    under_construction: Optional[int] = None
    construction_starts: Optional[int] = None
    cap_rate: Optional[float] = None
    price_per_sf: Optional[float] = None
    market_date: date
    quarter: Optional[str] = None
    data_source: str = "CoStar"


class CoStarImportResult(BaseModel):
    """Result of CoStar data import operation."""

    import_id: int
    filename: str
    records_processed: int = 0
    records_imported: int = 0
    records_failed: int = 0
    import_status: str = "pending"
    error_message: Optional[str] = None
    processing_time_seconds: float = 0.0


class CoStarMCPServer:
    """
    CoStar MCP Server for real estate market intelligence

    Features:
    - File watching for automatic data ingestion
    - CSV/Excel data processing with validation
    - Database integration with PostgreSQL
    - Market analysis and insights generation
    - API endpoints for data access
    """

    def __init__(self, watched_folder: str = "./watched_costar_files"):
        """Initialize CoStar MCP Server with watched folder."""
        self.watched_folder = Path(watched_folder)
        self.watched_folder.mkdir(exist_ok=True)
        self.integration_registry = IntegrationRegistry()
        self.db_pool: Optional[asyncpg.Pool] = None
        self._initialized = False

        # Column mapping for different CoStar data formats
        self.column_mappings = {
            "metro_area": ["metro_area", "market", "metro", "msa", "metropolitan_area"],
            "property_type": ["property_type", "prop_type", "type", "asset_type"],
            "submarket": ["submarket", "sub_market", "subarea"],
            "total_inventory": [
                "total_inventory",
                "inventory",
                "total_sf",
                "total_space",
            ],
            "vacancy_rate": ["vacancy_rate", "vacancy", "vac_rate", "vacant_pct"],
            "asking_rent_psf": ["asking_rent_psf", "asking_rent", "rent_psf", "rent"],
            "effective_rent_psf": ["effective_rent_psf", "effective_rent", "eff_rent"],
            "net_absorption": ["net_absorption", "absorption", "net_abs"],
            "construction_deliveries": [
                "construction_deliveries",
                "deliveries",
                "new_supply",
            ],
            "under_construction": ["under_construction", "under_const", "pipeline"],
            "construction_starts": ["construction_starts", "starts", "new_starts"],
            "cap_rate": ["cap_rate", "cap", "capitalization_rate"],
            "price_per_sf": ["price_per_sf", "price_psf", "sale_price_psf"],
            "market_date": ["market_date", "date", "period", "quarter_date"],
            "quarter": ["quarter", "qtr", "period"],
        }

    async def initialize(self) -> None:
        """Initialize the CoStar MCP server."""
        if self._initialized:
            return

        logger.info("Initializing CoStar MCP Server...")

        # Initialize database connection
        await self._initialize_database()

        # Register with integration registry
        await self.integration_registry.register("costar_mcp", self)

        # Start file watching
        asyncio.create_task(self._watch_files_continuously())

        self._initialized = True
        logger.info("CoStar MCP Server initialized successfully")

    async def _initialize_database(self) -> None:
        """Initialize database connection pool."""
        try:
            # Get database configuration from ESC
            db_config = {
                "host": config.get("postgres_host", "localhost"),
                "port": config.get("postgres_port", 5432),
                "database": config.get("postgres_database", "sophia_ai"),
                "user": config.get("postgres_user", "postgres"),
                "password": config.get("postgres_password", "password"),
            }

            self.db_pool = await asyncpg.create_pool(
                **db_config, min_size=2, max_size=10, command_timeout=60
            )

            logger.info("Database connection pool initialized")

        except Exception as e:
            logger.error(f"Failed to initialize database: {e}")
            raise

    async def _watch_files_continuously(self) -> None:
        """Continuously watch for new files in the watched folder."""
        while True:
            try:
                await self.watch_files()
                await asyncio.sleep(30)  # Check every 30 seconds
            except Exception as e:
                logger.error(f"Error in file watching loop: {e}")
                await asyncio.sleep(60)  # Wait longer on error

    async def watch_files(self) -> List[str]:
        """Watch for new CoStar files and process them."""
        processed_files = []

        try:
            # Get all files in watched folder
            files = [f for f in self.watched_folder.iterdir() if f.is_file()]

            for file_path in files:
                # Check if file is a supported format
                if file_path.suffix.lower() in [".csv", ".xlsx", ".xls"]:
                    # Check if already processed (simple check based on filename)
                    if not await self._is_file_already_processed(file_path):
                        logger.info(f"Processing new CoStar file: {file_path.name}")
                        await self.process_file(file_path)
                        processed_files.append(file_path.name)

                        # Move processed file to archive folder
                        await self._archive_processed_file(file_path)

        except Exception as e:
            logger.error(f"Error watching files: {e}")

        return processed_files

    async def process_file(self, file_path: Path) -> CoStarImportResult:
        """Process a CoStar data file."""
        start_time = datetime.now()

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        # Calculate file checksum
        file_checksum = await self._calculate_file_checksum(file_path)
        file_size = file_path.stat().st_size

        # Create import log entry
        import_id = await self._create_import_log(
            filename=file_path.name,
            file_size_bytes=file_size,
            file_checksum=file_checksum,
        )

        result = CoStarImportResult(import_id=import_id, filename=file_path.name)

        try:
            # Update import status to processing
            await self._update_import_log(
                import_id, "processing", processing_start_time=datetime.now()
            )

            # Read and process the file
            data_records = await self._read_and_validate_file(file_path)
            result.records_processed = len(data_records)

            # Import data to database
            imported_count, failed_count = await self._import_data_to_database(
                data_records
            )
            result.records_imported = imported_count
            result.records_failed = failed_count

            # Determine final status
            if failed_count == 0:
                result.import_status = "success"
            elif imported_count > 0:
                result.import_status = "partial"
            else:
                result.import_status = "failed"
                result.error_message = "No records were successfully imported"

            # Update import log with final results
            await self._update_import_log(
                import_id,
                result.import_status,
                processing_end_time=datetime.now(),
                records_processed=result.records_processed,
                records_imported=result.records_imported,
                records_failed=result.records_failed,
                error_message=result.error_message,
            )

        except Exception as e:
            logger.error(f"Error processing file {file_path.name}: {e}")
            result.import_status = "failed"
            result.error_message = str(e)

            await self._update_import_log(
                import_id,
                "failed",
                processing_end_time=datetime.now(),
                error_message=str(e),
            )

        finally:
            result.processing_time_seconds = (
                datetime.now() - start_time
            ).total_seconds()

        return result

    async def _read_and_validate_file(self, file_path: Path) -> List[CoStarDataRecord]:
        """Read and validate CoStar data file."""
        try:
            # Read file based on extension
            if file_path.suffix.lower() == ".csv":
                df = pd.read_csv(file_path)
            else:  # Excel files
                df = pd.read_excel(file_path)

            # Map columns to standard names
            df = self._map_columns(df)

            # Convert to CoStar data records
            records = []
            for _, row in df.iterrows():
                try:
                    # Parse market date
                    market_date = self._parse_date(row.get("market_date"))
                    if not market_date:
                        continue  # Skip records without valid dates

                    # Create record
                    record = CoStarDataRecord(
                        metro_area=str(row.get("metro_area", "")).strip(),
                        property_type=self._clean_string(row.get("property_type")),
                        submarket=self._clean_string(row.get("submarket")),
                        total_inventory=self._parse_int(row.get("total_inventory")),
                        vacancy_rate=self._parse_float(row.get("vacancy_rate")),
                        asking_rent_psf=self._parse_float(row.get("asking_rent_psf")),
                        effective_rent_psf=self._parse_float(
                            row.get("effective_rent_psf")
                        ),
                        net_absorption=self._parse_int(row.get("net_absorption")),
                        construction_deliveries=self._parse_int(
                            row.get("construction_deliveries")
                        ),
                        under_construction=self._parse_int(
                            row.get("under_construction")
                        ),
                        construction_starts=self._parse_int(
                            row.get("construction_starts")
                        ),
                        cap_rate=self._parse_float(row.get("cap_rate")),
                        price_per_sf=self._parse_float(row.get("price_per_sf")),
                        market_date=market_date,
                        quarter=self._clean_string(row.get("quarter")),
                    )

                    # Only add records with required fields
                    if record.metro_area:
                        records.append(record)

                except Exception as e:
                    logger.warning(f"Skipping invalid record: {e}")
                    continue

            return records

        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            raise

    def _map_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Map DataFrame columns to standard CoStar field names."""
        df_columns_lower = [col.lower().replace(" ", "_") for col in df.columns]
        column_mapping = {}

        for standard_name, possible_names in self.column_mappings.items():
            for possible_name in possible_names:
                if possible_name.lower() in df_columns_lower:
                    original_col = df.columns[
                        df_columns_lower.index(possible_name.lower())
                    ]
                    column_mapping[original_col] = standard_name
                    break

        return df.rename(columns=column_mapping)

    def _clean_string(self, value: Any) -> Optional[str]:
        """Clean and validate string values."""
        if pd.isna(value) or value == "":
            return None
        return str(value).strip()

    def _parse_int(self, value: Any) -> Optional[int]:
        """Parse integer values safely."""
        if pd.isna(value):
            return None
        try:
            return int(float(str(value).replace(",", "")))
        except (ValueError, TypeError):
            return None

    def _parse_float(self, value: Any) -> Optional[float]:
        """Parse float values safely."""
        if pd.isna(value):
            return None
        try:
            # Handle percentage strings
            str_val = str(value).replace(",", "").replace("%", "")
            return float(str_val)
        except (ValueError, TypeError):
            return None

    def _parse_date(self, value: Any) -> Optional[date]:
        """Parse date values safely."""
        if pd.isna(value):
            return None
        try:
            if isinstance(value, date):
                return value
            return pd.to_datetime(value).date()
        except (ValueError, TypeError):
            return None

    async def _import_data_to_database(
        self, records: List[CoStarDataRecord]
    ) -> tuple[int, int]:
        """Import CoStar data records to database."""
        imported_count = 0
        failed_count = 0

        async with self.db_pool.acquire() as conn:
            async with conn.transaction():
                for record in records:
                    try:
                        # Get or create market
                        market_id = await self._get_or_create_market(
                            conn, record.metro_area
                        )

                        # Insert market data
                        await conn.execute(
                            """
                            INSERT INTO costar_market_data (
                                market_id, property_type, submarket, total_inventory,
                                vacancy_rate, asking_rent_psf, effective_rent_psf,
                                net_absorption, construction_deliveries, under_construction,
                                construction_starts, cap_rate, price_per_sf,
                                market_date, quarter, data_source
                            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16)
                            ON CONFLICT (market_id, property_type, market_date, submarket) 
                            DO UPDATE SET
                                total_inventory = EXCLUDED.total_inventory,
                                vacancy_rate = EXCLUDED.vacancy_rate,
                                asking_rent_psf = EXCLUDED.asking_rent_psf,
                                effective_rent_psf = EXCLUDED.effective_rent_psf,
                                net_absorption = EXCLUDED.net_absorption,
                                construction_deliveries = EXCLUDED.construction_deliveries,
                                under_construction = EXCLUDED.under_construction,
                                construction_starts = EXCLUDED.construction_starts,
                                cap_rate = EXCLUDED.cap_rate,
                                price_per_sf = EXCLUDED.price_per_sf,
                                quarter = EXCLUDED.quarter,
                                updated_at = CURRENT_TIMESTAMP
                        """,
                            market_id,
                            record.property_type,
                            record.submarket,
                            record.total_inventory,
                            record.vacancy_rate,
                            record.asking_rent_psf,
                            record.effective_rent_psf,
                            record.net_absorption,
                            record.construction_deliveries,
                            record.under_construction,
                            record.construction_starts,
                            record.cap_rate,
                            record.price_per_sf,
                            record.market_date,
                            record.quarter,
                            record.data_source,
                        )

                        imported_count += 1

                    except Exception as e:
                        logger.error(
                            f"Failed to import record for {record.metro_area}: {e}"
                        )
                        failed_count += 1

        return imported_count, failed_count

    async def _get_or_create_market(
        self, conn: asyncpg.Connection, metro_area: str
    ) -> int:
        """Get existing market ID or create new market."""
        # Try to get existing market
        market_id = await conn.fetchval(
            "SELECT id FROM costar_markets WHERE metro_area = $1", metro_area
        )

        if market_id:
            return market_id

        # Create new market
        market_id = await conn.fetchval(
            """
            INSERT INTO costar_markets (metro_area) 
            VALUES ($1) 
            RETURNING id
        """,
            metro_area,
        )

        return market_id

    async def _create_import_log(
        self, filename: str, file_size_bytes: int, file_checksum: str
    ) -> int:
        """Create new import log entry."""
        async with self.db_pool.acquire() as conn:
            import_id = await conn.fetchval(
                """
                INSERT INTO costar_import_log (filename, file_size_bytes, file_checksum)
                VALUES ($1, $2, $3)
                RETURNING id
            """,
                filename,
                file_size_bytes,
                file_checksum,
            )

            return import_id

    async def _update_import_log(self, import_id: int, status: str, **kwargs) -> None:
        """Update import log entry."""
        set_clauses = ["import_status = $2"]
        values = [import_id, status]
        param_num = 3

        for key, value in kwargs.items():
            if value is not None:
                set_clauses.append(f"{key} = ${param_num}")
                values.append(value)
                param_num += 1

        query = f"UPDATE costar_import_log SET {', '.join(set_clauses)} WHERE id = $1"

        async with self.db_pool.acquire() as conn:
            await conn.execute(query, *values)

    async def _calculate_file_checksum(self, file_path: Path) -> str:
        """Calculate MD5 checksum of file."""
        hash_md5 = hashlib.md5(usedforsecurity=False)
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    async def _is_file_already_processed(self, file_path: Path) -> bool:
        """Check if file has already been processed."""
        file_checksum = await self._calculate_file_checksum(file_path)

        async with self.db_pool.acquire() as conn:
            result = await conn.fetchval(
                "SELECT id FROM costar_import_log WHERE file_checksum = $1 AND import_status = 'success'",
                file_checksum,
            )

            return result is not None

    async def _archive_processed_file(self, file_path: Path) -> None:
        """Move processed file to archive folder."""
        archive_folder = self.watched_folder / "processed"
        archive_folder.mkdir(exist_ok=True)

        archive_path = (
            archive_folder
            / f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file_path.name}"
        )
        file_path.rename(archive_path)

        logger.info(f"Archived processed file to: {archive_path}")

    async def get_markets(self) -> List[Dict[str, Any]]:
        """Get all available markets with record counts."""
        async with self.db_pool.acquire() as conn:
            results = await conn.fetch(
                """
                SELECT 
                    m.id,
                    m.metro_area,
                    m.state,
                    m.region,
                    m.market_tier,
                    COUNT(md.id) as record_count
                FROM costar_markets m
                LEFT JOIN costar_market_data md ON m.id = md.market_id
                GROUP BY m.id, m.metro_area, m.state, m.region, m.market_tier
                ORDER BY m.metro_area
            """
            )

            return [dict(row) for row in results]

    async def get_market_data(
        self, metro_area: str, limit: int = 100
    ) -> List[Dict[str, Any]]:
        """Get market data for specific metro area."""
        async with self.db_pool.acquire() as conn:
            results = await conn.fetch(
                """
                SELECT 
                    md.*,
                    m.metro_area,
                    m.state,
                    m.region
                FROM costar_market_data md
                JOIN costar_markets m ON md.market_id = m.id
                WHERE m.metro_area = $1
                ORDER BY md.market_date DESC, md.property_type
                LIMIT $2
            """,
                metro_area,
                limit,
            )

            return [dict(row) for row in results]

    async def get_import_history(self, limit: int = 50) -> List[Dict[str, Any]]:
        """Get import history."""
        async with self.db_pool.acquire() as conn:
            results = await conn.fetch(
                """
                SELECT *
                FROM costar_import_log
                ORDER BY created_at DESC
                LIMIT $1
            """,
                limit,
            )

            return [dict(row) for row in results]

    async def close(self) -> None:
        """Close database connections and cleanup."""
        if self.db_pool:
            await self.db_pool.close()
        logger.info("CoStar MCP Server closed")


# Global instance for MCP server
costar_server = CoStarMCPServer()


async def main():
    """Main function for running CoStar MCP server standalone."""
    await costar_server.initialize()

    # Keep server running
    try:
        while True:
            await asyncio.sleep(60)
    except KeyboardInterrupt:
        logger.info("Shutting down CoStar MCP Server")
        await costar_server.close()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
