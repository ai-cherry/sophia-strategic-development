#!/usr/bin/env python3
"""
Sophia AI FastAPI - Modern Deployment with Best Practices
Modern FastAPI with streaming chat support, proper lifespan management
"""

import asyncio
import logging
import os
from contextlib import asynccontextmanager
from datetime import datetime
from typing import AsyncGenerator

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from pydantic_settings import BaseSettings

# Settings Configuration
class Settings(BaseSettings):
    app_name: str = "Sophia AI Platform"
    app_version: str = "2.0.0"
    debug: bool = False
    environment: str = "production"
    
    class Config:
        env_prefix = "SOPHIA_"

settings = Settings()

# Setup logging
logging.basicConfig(
    level=logging.DEBUG if settings.debug else logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Lifespan management
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info(f"ðŸš€ {settings.app_name} v{settings.app_version} starting up...")
    logger.info(f"ðŸŒ Environment: {settings.environment}")
    logger.info(f"ðŸ”§ Debug mode: {settings.debug}")
    
    # Initialize services here if needed
    try:
        # Add service initialization here
        logger.info("âœ… Services initialized successfully")
    except Exception as e:
        logger.error(f"âŒ Service initialization failed: {e}")
        raise
    
    yield
    
    # Shutdown
    logger.info(f"ðŸ›‘ {settings.app_name} shutting down...")

# Create FastAPI app with lifespan
app = FastAPI(
    title=settings.app_name,
    description="AI-powered business intelligence with streaming chat",
    version=settings.app_version,
    lifespan=lifespan,
    debug=settings.debug
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class ChatRequest(BaseModel):
    message: str
    user_id: str = "user"
    stream: bool = False

class ChatResponse(BaseModel):
    content: str
    user_id: str

# Health check
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "Sophia AI Platform",
        "version": "2.0.0"
    }

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "Welcome to Sophia AI Platform",
        "version": "2.0.0",
        "docs_url": "/docs"
    }

# Mock AI response generator
async def generate_ai_response(message: str, user_id: str) -> AsyncGenerator[str, None]:
    response_parts = [
        f"Hello {user_id}! ",
        "I understand you said: ",
        f'"{message}". ',
        "This is a streaming response from Sophia AI. ",
        "I'm processing your request using advanced AI capabilities. ",
        "The system is working perfectly with FastAPI 2025 best practices. ",
        "Thank you for using Sophia AI!"
    ]
    
    for part in response_parts:
        await asyncio.sleep(0.2)  # Simulate processing
        yield part

# Streaming chat endpoint
@app.post("/api/v1/chat")
async def chat_endpoint(request: ChatRequest):
    try:
        if request.stream:
            # Return streaming response
            async def stream_response():
                async for token in generate_ai_response(request.message, request.user_id):
                    yield f"data: {token}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                stream_response(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                }
            )
        else:
            # Return complete response
            full_response = ""
            async for token in generate_ai_response(request.message, request.user_id):
                full_response += token
            
            return ChatResponse(
                content=full_response,
                user_id=request.user_id
            )
            
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}")
        raise HTTPException(status_code=500, detail=f"Chat processing failed: {str(e)}")

# Debug routes
@app.get("/debug/routes")
async def debug_routes():
    routes = []
    for route in app.routes:
        route_info = {
            "path": getattr(route, 'path', str(route)),
            "name": getattr(route, 'name', 'unknown')
        }
        # Safely get methods if available
        methods = getattr(route, 'methods', None)
        if methods:
            route_info["methods"] = list(methods)
        routes.append(route_info)
    return {"routes": routes}

# Enhanced health check
@app.get("/health/detailed")
async def detailed_health_check():
    return {
        "status": "healthy",
        "service": settings.app_name,
        "version": settings.app_version,
        "environment": settings.environment,
        "timestamp": datetime.utcnow().isoformat(),
        "debug": settings.debug
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

