#!/usr/bin/env python3
"""
Enhanced Gong Pipeline Test Suite

Comprehensive testing framework for the direct Python Gong data pipeline.
Tests the complete end-to-end flow powered by sophia_data_pipeline_ultimate.py.

Test Categories:
1. Connectivity Tests - API and database connections
2. Data Ingestion Tests - Raw data landing verification
3. Transformation Tests - STG table population
4. AI Enrichment Tests - Cortex integration
5. Integration Tests - End-to-end workflow
6. Performance Tests - Throughput and latency
7. Security Tests - PII masking and access control

Usage:
    python backend/scripts/enhanced_gong_pipeline_test_suite.py --test-suite all
    python backend/scripts/enhanced_gong_pipeline_test_suite.py --test-suite connectivity
    python backend/scripts/enhanced_gong_pipeline_test_suite.py --test-suite performance --environment dev
"""

import asyncio
import json
import logging
import sys
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
import time

from backend.core.auto_esc_config import get_config_value
from backend.scripts.sophia_data_pipeline_ultimate import (
    SophiaDataPipelineUltimate, 
    PipelineConfig, 
    PipelineMode,
    GongAPIClient,
    SnowflakeDataLoader
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestSuite(Enum):
    """Available test suites"""
    ALL = "all"
    CONNECTIVITY = "connectivity"
    INGESTION = "ingestion"
    TRANSFORMATION = "transformation"
    AI_ENRICHMENT = "ai-enrichment"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    SECURITY = "security"


class TestEnvironment(Enum):
    """Test environments"""
    DEV = "dev"
    STAGING = "staging"
    LOCAL = "local"


@dataclass
class TestResult:
    """Individual test result"""
    test_name: str
    test_suite: str
    status: str  # "passed", "failed", "skipped"
    duration_seconds: float
    message: str
    details: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


@dataclass
class TestSuiteResults:
    """Test suite execution results"""
    suite_name: str
    environment: str
    start_time: datetime
    end_time: Optional[datetime] = None
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    test_results: List[TestResult] = None
    
    def __post_init__(self):
        if self.test_results is None:
            self.test_results = []


class EnhancedGongPipelineTestSuite:
    """
    Comprehensive test suite for the direct Python Gong pipeline
    
    Tests all aspects of the sophia_data_pipeline_ultimate.py implementation
    including connectivity, data quality, performance, and security.
    """

    def __init__(self, environment: TestEnvironment = TestEnvironment.DEV):
        self.environment = environment
        self.database = "SOPHIA_AI_DEV" if environment == TestEnvironment.DEV else "SOPHIA_AI_STAGING"
        
        # Test configuration
        self.test_config = PipelineConfig(
            mode=PipelineMode.TEST,
            batch_size=10,  # Small batch for testing
            enable_ai_processing=True,
            dry_run=False  # We want to test actual operations
        )
        
        # Test results
        self.suite_results = TestSuiteResults(
            suite_name="enhanced_gong_pipeline",
            environment=environment.value,
            start_time=datetime.now()
        )
        
        logger.info(f"ðŸ§ª Enhanced Gong Pipeline Test Suite initialized for {environment.value}")

    async def run_test_suite(self, suite: TestSuite) -> TestSuiteResults:
        """Run the specified test suite"""
        logger.info("=" * 80)
        logger.info(f"ðŸš€ ENHANCED GONG PIPELINE TEST SUITE - {suite.value.upper()}")
        logger.info("=" * 80)
        
        try:
            if suite == TestSuite.ALL:
                await self._run_all_tests()
            elif suite == TestSuite.CONNECTIVITY:
                await self._run_connectivity_tests()
            elif suite == TestSuite.INGESTION:
                await self._run_ingestion_tests()
            elif suite == TestSuite.TRANSFORMATION:
                await self._run_transformation_tests()
            elif suite == TestSuite.AI_ENRICHMENT:
                await self._run_ai_enrichment_tests()
            elif suite == TestSuite.INTEGRATION:
                await self._run_integration_tests()
            elif suite == TestSuite.PERFORMANCE:
                await self._run_performance_tests()
            elif suite == TestSuite.SECURITY:
                await self._run_security_tests()
            
            self.suite_results.end_time = datetime.now()
            duration = (self.suite_results.end_time - self.suite_results.start_time).total_seconds()
            
            logger.info("=" * 80)
            logger.info("âœ… TEST SUITE COMPLETED")
            logger.info(f"â±ï¸  Duration: {duration:.2f} seconds")
            logger.info(f"ðŸ“Š Total: {self.suite_results.total_tests}")
            logger.info(f"âœ… Passed: {self.suite_results.passed_tests}")
            logger.info(f"âŒ Failed: {self.suite_results.failed_tests}")
            logger.info(f"â­ï¸ Skipped: {self.suite_results.skipped_tests}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"Test suite execution failed: {e}")
            
        return self.suite_results

    async def _run_test(self, test_name: str, test_suite: str, test_func) -> TestResult:
        """Run an individual test with timing and error handling"""
        start_time = time.time()
        
        try:
            logger.info(f"ðŸ§ª Running {test_name}...")
            result = await test_func()
            
            duration = time.time() - start_time
            
            test_result = TestResult(
                test_name=test_name,
                test_suite=test_suite,
                status="passed",
                duration_seconds=duration,
                message="Test passed successfully",
                details=result if isinstance(result, dict) else None
            )
            
            self.suite_results.passed_tests += 1
            logger.info(f"âœ… {test_name} - PASSED ({duration:.2f}s)")
            
        except Exception as e:
            duration = time.time() - start_time
            
            test_result = TestResult(
                test_name=test_name,
                test_suite=test_suite,
                status="failed",
                duration_seconds=duration,
                message=f"Test failed: {str(e)}",
                error=str(e)
            )
            
            self.suite_results.failed_tests += 1
            logger.error(f"âŒ {test_name} - FAILED ({duration:.2f}s): {str(e)}")
        
        self.suite_results.total_tests += 1
        self.suite_results.test_results.append(test_result)
        return test_result

    async def _run_all_tests(self):
        """Run all test suites"""
        await self._run_connectivity_tests()
        await self._run_ingestion_tests()
        await self._run_transformation_tests()
        await self._run_ai_enrichment_tests()
        await self._run_integration_tests()
        await self._run_performance_tests()
        await self._run_security_tests()

    async def _run_connectivity_tests(self):
        """Test connectivity to Gong API and Snowflake"""
        logger.info("ðŸ”Œ CONNECTIVITY TESTS - Starting")
        
        await self._run_test("test_gong_api_connection", "connectivity", self._test_gong_api_connection)
        await self._run_test("test_snowflake_connection", "connectivity", self._test_snowflake_connection)
        await self._run_test("test_pulumi_esc_secrets", "connectivity", self._test_pulumi_esc_secrets)
        await self._run_test("test_schema_availability", "connectivity", self._test_schema_availability)

    async def _test_gong_api_connection(self) -> Dict[str, Any]:
        """Test Gong API connectivity and authentication"""
        async with GongAPIClient(self.test_config) as gong:
            # Test basic API call
            from_date = datetime.now() - timedelta(hours=1)
            to_date = datetime.now()
            
            response = await gong.get_calls(from_date, to_date, limit=1)
            
            return {
                "api_accessible": True,
                "response_structure_valid": "calls" in response,
                "records_info_present": "records" in response
            }

    async def _test_snowflake_connection(self) -> Dict[str, Any]:
        """Test Snowflake connectivity and basic operations"""
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            # Test basic query
            cursor = snowflake.connection.cursor()
            try:
                cursor.execute("SELECT CURRENT_VERSION()")
                version = cursor.fetchone()[0]
                
                cursor.execute("SELECT CURRENT_DATABASE()")
                database = cursor.fetchone()[0]
                
                return {
                    "connection_successful": True,
                    "snowflake_version": version,
                    "current_database": database
                }
            finally:
                cursor.close()

    async def _test_pulumi_esc_secrets(self) -> Dict[str, Any]:
        """Test Pulumi ESC secret availability"""
        required_secrets = [
            "gong_access_key",
            "gong_access_key_secret",
            "snowflake_account",
            "snowflake_user",
            "snowflake_password"
        ]
        
        secret_status = {}
        for secret in required_secrets:
            try:
                value = get_config_value(secret)
                secret_status[secret] = "available" if value else "missing"
            except Exception as e:
                secret_status[secret] = f"error: {str(e)}"
        
        all_available = all(status == "available" for status in secret_status.values())
        
        return {
            "all_secrets_available": all_available,
            "secret_status": secret_status
        }

    async def _test_schema_availability(self) -> Dict[str, Any]:
        """Test required schema availability"""
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            cursor = snowflake.connection.cursor()
            try:
                required_schemas = ["RAW_ESTUARY", "STG_TRANSFORMED", "AI_MEMORY", "OPS_MONITORING"]
                schema_status = {}
                
                for schema in required_schemas:
                    cursor.execute(f"SHOW SCHEMAS LIKE '{schema}' IN DATABASE {self.database}")
                    result = cursor.fetchall()
                    schema_status[schema] = len(result) > 0
                
                return {
                    "all_schemas_available": all(schema_status.values()),
                    "schema_status": schema_status
                }
            finally:
                cursor.close()

    async def _run_ingestion_tests(self):
        """Test data ingestion functionality"""
        logger.info("ðŸ“¥ INGESTION TESTS - Starting")
        
        await self._run_test("test_raw_data_landing", "ingestion", self._test_raw_data_landing)
        await self._run_test("test_batch_processing", "ingestion", self._test_batch_processing)
        await self._run_test("test_error_handling", "ingestion", self._test_error_handling)

    async def _test_raw_data_landing(self) -> Dict[str, Any]:
        """Test raw data landing in RAW_ESTUARY schema"""
        # Run a small test pipeline
        pipeline = SophiaDataPipelineUltimate(self.test_config)
        
        # Override date range for test
        self.test_config.from_date = datetime.now() - timedelta(hours=1)
        self.test_config.to_date = datetime.now()
        
        results = await pipeline.run_pipeline()
        
        return {
            "pipeline_status": results["status"],
            "calls_processed": results["metrics"]["calls_processed"],
            "api_calls_made": results["metrics"]["total_api_calls"],
            "db_operations": results["metrics"]["total_db_operations"]
        }

    async def _test_batch_processing(self) -> Dict[str, Any]:
        """Test batch processing with different batch sizes"""
        test_results = {}
        
        for batch_size in [5, 10, 25]:
            config = PipelineConfig(
                mode=PipelineMode.TEST,
                batch_size=batch_size,
                from_date=datetime.now() - timedelta(hours=1),
                to_date=datetime.now()
            )
            
            start_time = time.time()
            pipeline = SophiaDataPipelineUltimate(config)
            results = await pipeline.run_pipeline()
            duration = time.time() - start_time
            
            test_results[f"batch_size_{batch_size}"] = {
                "status": results["status"],
                "duration": duration,
                "calls_processed": results["metrics"]["calls_processed"]
            }
        
        return test_results

    async def _test_error_handling(self) -> Dict[str, Any]:
        """Test error handling and recovery mechanisms"""
        # Test with invalid date range
        config = PipelineConfig(
            mode=PipelineMode.TEST,
            from_date=datetime.now(),  # Invalid: from_date after to_date
            to_date=datetime.now() - timedelta(hours=1)
        )
        
        pipeline = SophiaDataPipelineUltimate(config)
        results = await pipeline.run_pipeline()
        
        return {
            "handles_invalid_date_range": results["status"] == "failed",
            "error_logged": "error" in results
        }

    async def _run_transformation_tests(self):
        """Test data transformation functionality"""
        logger.info("ðŸ”„ TRANSFORMATION TESTS - Starting")
        
        await self._run_test("test_stg_table_population", "transformation", self._test_stg_table_population)
        await self._run_test("test_data_quality", "transformation", self._test_data_quality)

    async def _test_stg_table_population(self) -> Dict[str, Any]:
        """Test STG_TRANSFORMED table population"""
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            # Check if STG_GONG_CALLS table exists and has data
            cursor = snowflake.connection.cursor()
            try:
                cursor.execute(f"""
                    SELECT COUNT(*) as record_count,
                           COUNT(CASE WHEN CALL_ID IS NOT NULL THEN 1 END) as valid_call_ids,
                           COUNT(CASE WHEN CALL_DATETIME_UTC IS NOT NULL THEN 1 END) as valid_timestamps
                    FROM {self.database}.STG_TRANSFORMED.STG_GONG_CALLS
                    WHERE CREATED_AT >= CURRENT_DATE()
                """)
                
                result = cursor.fetchone()
                
                return {
                    "table_exists": True,
                    "record_count": result[0],
                    "valid_call_ids": result[1],
                    "valid_timestamps": result[2],
                    "data_quality_score": (result[1] / result[0] * 100) if result[0] > 0 else 0
                }
            finally:
                cursor.close()

    async def _test_data_quality(self) -> Dict[str, Any]:
        """Test data quality metrics"""
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            cursor = snowflake.connection.cursor()
            try:
                cursor.execute(f"""
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(CASE WHEN CALL_DURATION_SECONDS > 0 THEN 1 END) as valid_durations,
                        COUNT(CASE WHEN PRIMARY_USER_EMAIL IS NOT NULL THEN 1 END) as valid_emails,
                        AVG(CALL_DURATION_SECONDS) as avg_duration
                    FROM {self.database}.STG_TRANSFORMED.STG_GONG_CALLS
                    WHERE CREATED_AT >= CURRENT_DATE()
                """)
                
                result = cursor.fetchone()
                
                quality_score = 0
                if result[0] > 0:
                    duration_quality = result[1] / result[0]
                    email_quality = result[2] / result[0]
                    quality_score = (duration_quality + email_quality) / 2 * 100
                
                return {
                    "total_records": result[0],
                    "duration_quality_pct": (result[1] / result[0] * 100) if result[0] > 0 else 0,
                    "email_quality_pct": (result[2] / result[0] * 100) if result[0] > 0 else 0,
                    "overall_quality_score": quality_score,
                    "avg_call_duration": result[3]
                }
            finally:
                cursor.close()

    async def _run_ai_enrichment_tests(self):
        """Test AI enrichment functionality"""
        logger.info("ðŸ§  AI ENRICHMENT TESTS - Starting")
        
        await self._run_test("test_sentiment_analysis", "ai_enrichment", self._test_sentiment_analysis)
        await self._run_test("test_embedding_generation", "ai_enrichment", self._test_embedding_generation)

    async def _test_sentiment_analysis(self) -> Dict[str, Any]:
        """Test sentiment analysis using Snowflake Cortex"""
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            cursor = snowflake.connection.cursor()
            try:
                cursor.execute(f"""
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(CASE WHEN SENTIMENT_SCORE IS NOT NULL THEN 1 END) as sentiment_populated,
                        AVG(SENTIMENT_SCORE) as avg_sentiment,
                        MIN(SENTIMENT_SCORE) as min_sentiment,
                        MAX(SENTIMENT_SCORE) as max_sentiment
                    FROM {self.database}.STG_TRANSFORMED.STG_GONG_CALLS
                    WHERE CREATED_AT >= CURRENT_DATE()
                """)
                
                result = cursor.fetchone()
                
                return {
                    "total_records": result[0],
                    "sentiment_coverage_pct": (result[1] / result[0] * 100) if result[0] > 0 else 0,
                    "avg_sentiment": result[2],
                    "sentiment_range": [result[3], result[4]],
                    "sentiment_within_bounds": -1 <= (result[2] or 0) <= 1
                }
            finally:
                cursor.close()

    async def _test_embedding_generation(self) -> Dict[str, Any]:
        """Test embedding generation using Snowflake Cortex"""
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            cursor = snowflake.connection.cursor()
            try:
                cursor.execute(f"""
                    SELECT 
                        COUNT(*) as total_records,
                        COUNT(CASE WHEN AI_MEMORY_EMBEDDING IS NOT NULL THEN 1 END) as embeddings_populated
                    FROM {self.database}.STG_TRANSFORMED.STG_GONG_CALLS
                    WHERE CREATED_AT >= CURRENT_DATE()
                """)
                
                result = cursor.fetchone()
                
                return {
                    "total_records": result[0],
                    "embedding_coverage_pct": (result[1] / result[0] * 100) if result[0] > 0 else 0,
                    "embeddings_functional": result[1] > 0
                }
            finally:
                cursor.close()

    async def _run_integration_tests(self):
        """Test end-to-end integration"""
        logger.info("ðŸ”— INTEGRATION TESTS - Starting")
        
        await self._run_test("test_full_pipeline_flow", "integration", self._test_full_pipeline_flow)

    async def _test_full_pipeline_flow(self) -> Dict[str, Any]:
        """Test complete end-to-end pipeline flow"""
        config = PipelineConfig(
            mode=PipelineMode.TEST,
            from_date=datetime.now() - timedelta(hours=2),
            to_date=datetime.now(),
            batch_size=5,
            enable_ai_processing=True
        )
        
        pipeline = SophiaDataPipelineUltimate(config)
        results = await pipeline.run_pipeline()
        
        return {
            "pipeline_completed": results["status"] == "success",
            "all_phases_executed": results["metrics"]["total_db_operations"] >= 3,
            "data_processed": results["metrics"]["calls_processed"] > 0,
            "ai_enrichment_applied": results["metrics"]["ai_enrichments"] > 0
        }

    async def _run_performance_tests(self):
        """Test performance characteristics"""
        logger.info("âš¡ PERFORMANCE TESTS - Starting")
        
        await self._run_test("test_throughput", "performance", self._test_throughput)
        await self._run_test("test_latency", "performance", self._test_latency)

    async def _test_throughput(self) -> Dict[str, Any]:
        """Test data processing throughput"""
        config = PipelineConfig(
            mode=PipelineMode.TEST,
            from_date=datetime.now() - timedelta(hours=6),
            to_date=datetime.now(),
            batch_size=50
        )
        
        start_time = time.time()
        pipeline = SophiaDataPipelineUltimate(config)
        results = await pipeline.run_pipeline()
        duration = time.time() - start_time
        
        calls_per_second = results["metrics"]["calls_processed"] / duration if duration > 0 else 0
        
        return {
            "total_duration": duration,
            "calls_processed": results["metrics"]["calls_processed"],
            "calls_per_second": calls_per_second,
            "meets_performance_target": calls_per_second >= 10  # 10 calls/second minimum
        }

    async def _test_latency(self) -> Dict[str, Any]:
        """Test API and database latency"""
        
        # Test Gong API latency
        async with GongAPIClient(self.test_config) as gong:
            start_time = time.time()
            await gong.get_calls(
                datetime.now() - timedelta(hours=1),
                datetime.now(),
                limit=1
            )
            api_latency = time.time() - start_time
        
        # Test Snowflake query latency
        async with SnowflakeDataLoader(self.test_config) as snowflake:
            cursor = snowflake.connection.cursor()
            try:
                start_time = time.time()
                cursor.execute("SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES")
                cursor.fetchone()
                db_latency = time.time() - start_time
            finally:
                cursor.close()
        
        return {
            "gong_api_latency": api_latency,
            "snowflake_latency": db_latency,
            "api_latency_acceptable": api_latency < 5.0,  # 5 second max
            "db_latency_acceptable": db_latency < 2.0     # 2 second max
        }

    async def _run_security_tests(self):
        """Test security and compliance"""
        logger.info("ðŸ”’ SECURITY TESTS - Starting")
        
        await self._run_test("test_credential_security", "security", self._test_credential_security)

    async def _test_credential_security(self) -> Dict[str, Any]:
        """Test credential security and access control"""
        # Test that credentials are not exposed in logs or errors
        config = PipelineConfig(mode=PipelineMode.TEST, dry_run=True)
        
        # This should not expose credentials even in dry run mode
        pipeline = SophiaDataPipelineUltimate(config)
        results = await pipeline.run_pipeline()
        
        return {
            "credentials_not_exposed": True,  # Manual verification required
            "dry_run_functional": results["status"] == "success",
            "pulumi_esc_integration": get_config_value("gong_access_key") is not None
        }

    def generate_test_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        return {
            "test_suite": self.suite_results.suite_name,
            "environment": self.suite_results.environment,
            "execution_time": {
                "start_time": self.suite_results.start_time.isoformat(),
                "end_time": self.suite_results.end_time.isoformat() if self.suite_results.end_time else None,
                "duration_seconds": (
                    (self.suite_results.end_time - self.suite_results.start_time).total_seconds()
                    if self.suite_results.end_time else None
                )
            },
            "summary": {
                "total_tests": self.suite_results.total_tests,
                "passed": self.suite_results.passed_tests,
                "failed": self.suite_results.failed_tests,
                "skipped": self.suite_results.skipped_tests,
                "success_rate": (
                    (self.suite_results.passed_tests / self.suite_results.total_tests * 100)
                    if self.suite_results.total_tests > 0 else 0
                )
            },
            "test_results": [asdict(result) for result in self.suite_results.test_results]
        }


def parse_arguments() -> argparse.Namespace:
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Enhanced Gong Pipeline Test Suite"
    )
    
    parser.add_argument(
        "--test-suite",
        type=str,
        choices=[suite.value for suite in TestSuite],
        default=TestSuite.ALL.value,
        help="Test suite to run"
    )
    
    parser.add_argument(
        "--environment",
        type=str,
        choices=[env.value for env in TestEnvironment],
        default=TestEnvironment.DEV.value,
        help="Test environment"
    )
    
    parser.add_argument(
        "--output-file",
        type=str,
        help="Output file for test results (JSON format)"
    )
    
    return parser.parse_args()


async def main():
    """Main entry point"""
    args = parse_arguments()
    
    # Create test suite
    test_suite = EnhancedGongPipelineTestSuite(TestEnvironment(args.environment))
    
    # Run tests
    await test_suite.run_test_suite(TestSuite(args.test_suite))
    
    # Generate report
    report = test_suite.generate_test_report()
    
    # Output results
    if args.output_file:
        with open(args.output_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        logger.info(f"ðŸ“„ Test report saved to {args.output_file}")
    else:
        print(json.dumps(report, indent=2, default=str))
    
    # Exit with appropriate code
    success_rate = report["summary"]["success_rate"]
    sys.exit(0 if success_rate >= 80 else 1)  # 80% success rate required


if __name__ == "__main__":
    asyncio.run(main()) 
