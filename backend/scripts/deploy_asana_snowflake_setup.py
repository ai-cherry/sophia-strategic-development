#!/usr/bin/env python3
"""
Asana Snowflake Infrastructure Deployment Script

Deploys comprehensive Snowflake infrastructure for Asana integration including:
- Transformation stored procedures for raw Asana data
- AI enrichment procedures using Snowflake Cortex
- Scheduled tasks for automated processing
- Data quality monitoring and validation
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 916 lines

Recommended decomposition:
- deploy_asana_snowflake_setup_core.py - Core functionality
- deploy_asana_snowflake_setup_utils.py - Utility functions  
- deploy_asana_snowflake_setup_models.py - Data models
- deploy_asana_snowflake_setup_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

from backend.utils.snowflake_cortex_service import SnowflakeCortexService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class DeploymentResult:
    """Result of a deployment operation"""

    component: str
    status: str
    message: str
    execution_time: float
    details: Optional[Dict[str, Any]] = None


class AsanaSnowflakeDeployer:
    """Deploys comprehensive Asana Snowflake infrastructure"""

    def __init__(self, environment: str = "dev"):
        self.environment = environment
        self.cortex_service = None
        self.deployment_results: List[DeploymentResult] = []

    async def initialize(self) -> None:
        """Initialize the deployer"""
        try:
            self.cortex_service = SnowflakeCortexService()
            await self.cortex_service.initialize()
            logger.info("✅ Asana Snowflake Deployer initialized")
        except Exception as e:
            logger.error(f"❌ Failed to initialize deployer: {e}")
            raise

    async def execute_sql_with_result_tracking(
        self, sql: str, component: str
    ) -> DeploymentResult:
        """Execute SQL and track deployment result"""
        start_time = datetime.now()

        try:
            await self.cortex_service.execute_query(sql)
            execution_time = (datetime.now() - start_time).total_seconds()

            result = DeploymentResult(
                component=component,
                status="SUCCESS",
                message=f"{component} deployed successfully",
                execution_time=execution_time,
            )

            logger.info(f"✅ {component} deployed in {execution_time:.2f}s")

        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()

            result = DeploymentResult(
                component=component,
                status="FAILED",
                message=f"{component} deployment failed: {str(e)}",
                execution_time=execution_time,
            )

            logger.error(f"❌ {component} deployment failed: {e}")

        self.deployment_results.append(result)
        return result

    async def deploy_asana_transformation_procedures(self) -> List[DeploymentResult]:
        """Deploy Asana data transformation stored procedures"""
        logger.info("🚀 Deploying Asana transformation procedures")

        procedures = []

        # 1. Transform Asana Projects
        transform_projects_sql = """
        CREATE OR REPLACE PROCEDURE TRANSFORM_ASANA_PROJECTS()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        DECLARE
            processed_count NUMBER DEFAULT 0;
            error_count NUMBER DEFAULT 0;
        BEGIN
            
            -- Transform raw Asana projects to STG_ASANA_PROJECTS
            INSERT INTO STG_TRANSFORMED.STG_ASANA_PROJECTS (
                PROJECT_GID,
                PROJECT_NAME,
                PROJECT_DESCRIPTION,
                PROJECT_STATUS,
                PROJECT_COLOR,
                OWNER_GID,
                OWNER_NAME,
                TEAM_GID,
                TEAM_NAME,
                WORKSPACE_GID,
                WORKSPACE_NAME,
                IS_ARCHIVED,
                IS_PUBLIC,
                CREATED_AT,
                MODIFIED_AT,
                DUE_DATE,
                START_DATE,
                COMPLETION_PERCENTAGE,
                FOLLOWER_COUNT,
                MEMBER_COUNT,
                TASK_COUNT,
                COMPLETED_TASK_COUNT,
                INCOMPLETE_TASK_COUNT,
                PRIORITY_LEVEL,
                PROJECT_NOTES,
                CUSTOM_FIELDS,
                ASANA_PERMALINK_URL,
                LAST_UPDATED,
                DATA_SOURCE,
                CONFIDENCE_SCORE,
                AI_MEMORY_METADATA
            )
            SELECT 
                _ESTUARY_DATA:gid::VARCHAR AS PROJECT_GID,
                _ESTUARY_DATA:name::VARCHAR AS PROJECT_NAME,
                _ESTUARY_DATA:notes::VARCHAR AS PROJECT_DESCRIPTION,
                COALESCE(_ESTUARY_DATA:current_status:text::VARCHAR, 'ACTIVE') AS PROJECT_STATUS,
                _ESTUARY_DATA:color::VARCHAR AS PROJECT_COLOR,
                _ESTUARY_DATA:owner:gid::VARCHAR AS OWNER_GID,
                _ESTUARY_DATA:owner:name::VARCHAR AS OWNER_NAME,
                _ESTUARY_DATA:team:gid::VARCHAR AS TEAM_GID,
                _ESTUARY_DATA:team:name::VARCHAR AS TEAM_NAME,
                _ESTUARY_DATA:workspace:gid::VARCHAR AS WORKSPACE_GID,
                _ESTUARY_DATA:workspace:name::VARCHAR AS WORKSPACE_NAME,
                COALESCE(_ESTUARY_DATA:archived::BOOLEAN, FALSE) AS IS_ARCHIVED,
                COALESCE(_ESTUARY_DATA:public::BOOLEAN, FALSE) AS IS_PUBLIC,
                _ESTUARY_DATA:created_at::TIMESTAMP_LTZ AS CREATED_AT,
                _ESTUARY_DATA:modified_at::TIMESTAMP_LTZ AS MODIFIED_AT,
                _ESTUARY_DATA:due_date::DATE AS DUE_DATE,
                _ESTUARY_DATA:start_date::DATE AS START_DATE,
                COALESCE(_ESTUARY_DATA:completion_percentage::FLOAT, 0.0) AS COMPLETION_PERCENTAGE,
                COALESCE(ARRAY_SIZE(_ESTUARY_DATA:followers), 0) AS FOLLOWER_COUNT,
                COALESCE(ARRAY_SIZE(_ESTUARY_DATA:members), 0) AS MEMBER_COUNT,
                COALESCE(_ESTUARY_DATA:task_count::NUMBER, 0) AS TASK_COUNT,
                COALESCE(_ESTUARY_DATA:completed_task_count::NUMBER, 0) AS COMPLETED_TASK_COUNT,
                COALESCE(_ESTUARY_DATA:incomplete_task_count::NUMBER, 0) AS INCOMPLETE_TASK_COUNT,
                CASE 
                    WHEN _ESTUARY_DATA:due_date IS NOT NULL AND _ESTUARY_DATA:due_date::DATE < CURRENT_DATE THEN 'HIGH'
                    WHEN _ESTUARY_DATA:completion_percentage::FLOAT > 80 THEN 'MEDIUM'
                    ELSE 'LOW'
                END AS PRIORITY_LEVEL,
                _ESTUARY_DATA:notes::VARCHAR AS PROJECT_NOTES,
                _ESTUARY_DATA:custom_fields AS CUSTOM_FIELDS,
                _ESTUARY_DATA:permalink_url::VARCHAR AS ASANA_PERMALINK_URL,
                CURRENT_TIMESTAMP() AS LAST_UPDATED,
                'ASANA_API' AS DATA_SOURCE,
                CASE 
                    WHEN _ESTUARY_DATA:gid IS NOT NULL AND _ESTUARY_DATA:name IS NOT NULL THEN 0.95
                    WHEN _ESTUARY_DATA:gid IS NOT NULL THEN 0.75
                    ELSE 0.5
                END AS CONFIDENCE_SCORE,
                OBJECT_CONSTRUCT(
                    'source_table', 'RAW_ESTUARY._ESTUARY_RAW_ASANA_PROJECTS',
                    'transformation_date', CURRENT_TIMESTAMP()::STRING,
                    'asana_gid', _ESTUARY_DATA:gid::VARCHAR,
                    'has_description', CASE WHEN _ESTUARY_DATA:notes IS NOT NULL THEN TRUE ELSE FALSE END,
                    'has_due_date', CASE WHEN _ESTUARY_DATA:due_date IS NOT NULL THEN TRUE ELSE FALSE END,
                    'member_count', COALESCE(ARRAY_SIZE(_ESTUARY_DATA:members), 0),
                    'follower_count', COALESCE(ARRAY_SIZE(_ESTUARY_DATA:followers), 0)
                ) AS AI_MEMORY_METADATA
            FROM RAW_ESTUARY._ESTUARY_RAW_ASANA_PROJECTS
            WHERE _ESTUARY_DATA:gid IS NOT NULL
            AND NOT EXISTS (
                SELECT 1 FROM STG_TRANSFORMED.STG_ASANA_PROJECTS 
                WHERE PROJECT_GID = _ESTUARY_DATA:gid::VARCHAR
            );
            
            GET DIAGNOSTICS processed_count = ROW_COUNT;
            
            -- Log successful transformation
            INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                RECORDS_PROCESSED, START_TIME, END_TIME, DURATION_SECONDS
            ) VALUES (
                'TRANSFORM_ASANA_PROJECTS', 'STG_TRANSFORMED', 'STG_ASANA_PROJECTS', 'TRANSFORM', 'SUCCESS',
                processed_count, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 0
            );
            
            RETURN 'Successfully processed ' || processed_count || ' Asana projects';
            
        EXCEPTION
            WHEN OTHER THEN
                -- Log error
                INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                    JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                    ERROR_MESSAGE, START_TIME, END_TIME
                ) VALUES (
                    'TRANSFORM_ASANA_PROJECTS', 'STG_TRANSFORMED', 'STG_ASANA_PROJECTS', 'TRANSFORM', 'FAILED',
                    SQLERRM, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()
                );
                
                RETURN 'Error transforming Asana projects: ' || SQLERRM;
        END;
        $$;
        """

        procedures.append(
            await self.execute_sql_with_result_tracking(
                transform_projects_sql, "TRANSFORM_ASANA_PROJECTS"
            )
        )

        # 2. Transform Asana Tasks
        transform_tasks_sql = """
        CREATE OR REPLACE PROCEDURE TRANSFORM_ASANA_TASKS()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        DECLARE
            processed_count NUMBER DEFAULT 0;
        BEGIN
            
            -- Transform raw Asana tasks to STG_ASANA_TASKS
            INSERT INTO STG_TRANSFORMED.STG_ASANA_TASKS (
                TASK_GID,
                TASK_NAME,
                TASK_DESCRIPTION,
                TASK_STATUS,
                IS_COMPLETED,
                COMPLETED_AT,
                ASSIGNEE_GID,
                ASSIGNEE_NAME,
                ASSIGNEE_EMAIL,
                PROJECT_GID,
                PROJECT_NAME,
                PARENT_TASK_GID,
                WORKSPACE_GID,
                CREATED_AT,
                MODIFIED_AT,
                DUE_DATE,
                DUE_TIME,
                START_DATE,
                PRIORITY_LEVEL,
                TASK_NOTES,
                SUBTASK_COUNT,
                DEPENDENCY_COUNT,
                FOLLOWER_COUNT,
                ATTACHMENT_COUNT,
                STORY_COUNT,
                ESTIMATED_HOURS,
                ACTUAL_HOURS,
                TAGS,
                CUSTOM_FIELDS,
                ASANA_PERMALINK_URL,
                LAST_UPDATED,
                DATA_SOURCE,
                CONFIDENCE_SCORE,
                AI_MEMORY_METADATA
            )
            SELECT 
                _ESTUARY_DATA:gid::VARCHAR AS TASK_GID,
                _ESTUARY_DATA:name::VARCHAR AS TASK_NAME,
                _ESTUARY_DATA:notes::VARCHAR AS TASK_DESCRIPTION,
                CASE 
                    WHEN _ESTUARY_DATA:completed::BOOLEAN = TRUE THEN 'COMPLETED'
                    WHEN _ESTUARY_DATA:due_date IS NOT NULL AND _ESTUARY_DATA:due_date::DATE < CURRENT_DATE THEN 'OVERDUE'
                    WHEN _ESTUARY_DATA:due_date IS NOT NULL AND _ESTUARY_DATA:due_date::DATE <= CURRENT_DATE + 7 THEN 'DUE_SOON'
                    ELSE 'IN_PROGRESS'
                END AS TASK_STATUS,
                COALESCE(_ESTUARY_DATA:completed::BOOLEAN, FALSE) AS IS_COMPLETED,
                _ESTUARY_DATA:completed_at::TIMESTAMP_LTZ AS COMPLETED_AT,
                _ESTUARY_DATA:assignee:gid::VARCHAR AS ASSIGNEE_GID,
                _ESTUARY_DATA:assignee:name::VARCHAR AS ASSIGNEE_NAME,
                _ESTUARY_DATA:assignee:email::VARCHAR AS ASSIGNEE_EMAIL,
                CASE 
                    WHEN ARRAY_SIZE(_ESTUARY_DATA:projects) > 0 
                    THEN _ESTUARY_DATA:projects[0]:gid::VARCHAR
                    ELSE NULL
                END AS PROJECT_GID,
                CASE 
                    WHEN ARRAY_SIZE(_ESTUARY_DATA:projects) > 0 
                    THEN _ESTUARY_DATA:projects[0]:name::VARCHAR
                    ELSE NULL
                END AS PROJECT_NAME,
                _ESTUARY_DATA:parent:gid::VARCHAR AS PARENT_TASK_GID,
                _ESTUARY_DATA:workspace:gid::VARCHAR AS WORKSPACE_GID,
                _ESTUARY_DATA:created_at::TIMESTAMP_LTZ AS CREATED_AT,
                _ESTUARY_DATA:modified_at::TIMESTAMP_LTZ AS MODIFIED_AT,
                _ESTUARY_DATA:due_on::DATE AS DUE_DATE,
                _ESTUARY_DATA:due_at::TIMESTAMP_LTZ AS DUE_TIME,
                _ESTUARY_DATA:start_on::DATE AS START_DATE,
                CASE 
                    WHEN _ESTUARY_DATA:due_date IS NOT NULL AND _ESTUARY_DATA:due_date::DATE < CURRENT_DATE THEN 'HIGH'
                    WHEN _ESTUARY_DATA:due_date IS NOT NULL AND _ESTUARY_DATA:due_date::DATE <= CURRENT_DATE + 3 THEN 'MEDIUM'
                    ELSE 'LOW'
                END AS PRIORITY_LEVEL,
                _ESTUARY_DATA:notes::VARCHAR AS TASK_NOTES,
                COALESCE(ARRAY_SIZE(_ESTUARY_DATA:subtasks), 0) AS SUBTASK_COUNT,
                COALESCE(ARRAY_SIZE(_ESTUARY_DATA:dependencies), 0) AS DEPENDENCY_COUNT,
                COALESCE(ARRAY_SIZE(_ESTUARY_DATA:followers), 0) AS FOLLOWER_COUNT,
                COALESCE(ARRAY_SIZE(_ESTUARY_DATA:attachments), 0) AS ATTACHMENT_COUNT,
                COALESCE(_ESTUARY_DATA:num_hearts::NUMBER, 0) AS STORY_COUNT,
                _ESTUARY_DATA:estimated_hours::FLOAT AS ESTIMATED_HOURS,
                _ESTUARY_DATA:actual_hours::FLOAT AS ACTUAL_HOURS,
                _ESTUARY_DATA:tags AS TAGS,
                _ESTUARY_DATA:custom_fields AS CUSTOM_FIELDS,
                _ESTUARY_DATA:permalink_url::VARCHAR AS ASANA_PERMALINK_URL,
                CURRENT_TIMESTAMP() AS LAST_UPDATED,
                'ASANA_API' AS DATA_SOURCE,
                CASE 
                    WHEN _ESTUARY_DATA:gid IS NOT NULL AND _ESTUARY_DATA:name IS NOT NULL THEN 0.95
                    WHEN _ESTUARY_DATA:gid IS NOT NULL THEN 0.75
                    ELSE 0.5
                END AS CONFIDENCE_SCORE,
                OBJECT_CONSTRUCT(
                    'source_table', 'RAW_ESTUARY._ESTUARY_RAW_ASANA_TASKS',
                    'transformation_date', CURRENT_TIMESTAMP()::STRING,
                    'asana_gid', _ESTUARY_DATA:gid::VARCHAR,
                    'has_assignee', CASE WHEN _ESTUARY_DATA:assignee:gid IS NOT NULL THEN TRUE ELSE FALSE END,
                    'has_due_date', CASE WHEN _ESTUARY_DATA:due_on IS NOT NULL THEN TRUE ELSE FALSE END,
                    'has_description', CASE WHEN _ESTUARY_DATA:notes IS NOT NULL THEN TRUE ELSE FALSE END,
                    'project_count', COALESCE(ARRAY_SIZE(_ESTUARY_DATA:projects), 0),
                    'subtask_count', COALESCE(ARRAY_SIZE(_ESTUARY_DATA:subtasks), 0)
                ) AS AI_MEMORY_METADATA
            FROM RAW_ESTUARY._ESTUARY_RAW_ASANA_TASKS
            WHERE _ESTUARY_DATA:gid IS NOT NULL
            AND NOT EXISTS (
                SELECT 1 FROM STG_TRANSFORMED.STG_ASANA_TASKS 
                WHERE TASK_GID = _ESTUARY_DATA:gid::VARCHAR
            );
            
            GET DIAGNOSTICS processed_count = ROW_COUNT;
            
            -- Log successful transformation
            INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                RECORDS_PROCESSED, START_TIME, END_TIME, DURATION_SECONDS
            ) VALUES (
                'TRANSFORM_ASANA_TASKS', 'STG_TRANSFORMED', 'STG_ASANA_TASKS', 'TRANSFORM', 'SUCCESS',
                processed_count, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 0
            );
            
            RETURN 'Successfully processed ' || processed_count || ' Asana tasks';
            
        EXCEPTION
            WHEN OTHER THEN
                INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                    JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                    ERROR_MESSAGE, START_TIME, END_TIME
                ) VALUES (
                    'TRANSFORM_ASANA_TASKS', 'STG_TRANSFORMED', 'STG_ASANA_TASKS', 'TRANSFORM', 'FAILED',
                    SQLERRM, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()
                );
                
                RETURN 'Error transforming Asana tasks: ' || SQLERRM;
        END;
        $$;
        """

        procedures.append(
            await self.execute_sql_with_result_tracking(
                transform_tasks_sql, "TRANSFORM_ASANA_TASKS"
            )
        )

        # 3. Transform Asana Users
        transform_users_sql = """
        CREATE OR REPLACE PROCEDURE TRANSFORM_ASANA_USERS()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        DECLARE
            processed_count NUMBER DEFAULT 0;
        BEGIN
            
            -- Transform raw Asana users to STG_ASANA_USERS
            INSERT INTO STG_TRANSFORMED.STG_ASANA_USERS (
                USER_GID,
                USER_NAME,
                USER_EMAIL,
                WORKSPACE_GID,
                WORKSPACE_NAME,
                IS_ACTIVE,
                ROLE,
                DEPARTMENT,
                PHOTO_URL,
                CREATED_AT,
                LAST_UPDATED,
                DATA_SOURCE,
                CONFIDENCE_SCORE,
                AI_MEMORY_METADATA
            )
            SELECT 
                _ESTUARY_DATA:gid::VARCHAR AS USER_GID,
                _ESTUARY_DATA:name::VARCHAR AS USER_NAME,
                _ESTUARY_DATA:email::VARCHAR AS USER_EMAIL,
                _ESTUARY_DATA:workspace:gid::VARCHAR AS WORKSPACE_GID,
                _ESTUARY_DATA:workspace:name::VARCHAR AS WORKSPACE_NAME,
                COALESCE(_ESTUARY_DATA:is_active::BOOLEAN, TRUE) AS IS_ACTIVE,
                _ESTUARY_DATA:role::VARCHAR AS ROLE,
                _ESTUARY_DATA:department::VARCHAR AS DEPARTMENT,
                _ESTUARY_DATA:photo:image_128x128::VARCHAR AS PHOTO_URL,
                CURRENT_TIMESTAMP() AS CREATED_AT,
                CURRENT_TIMESTAMP() AS LAST_UPDATED,
                'ASANA_API' AS DATA_SOURCE,
                CASE 
                    WHEN _ESTUARY_DATA:gid IS NOT NULL AND _ESTUARY_DATA:email IS NOT NULL THEN 0.95
                    WHEN _ESTUARY_DATA:gid IS NOT NULL THEN 0.75
                    ELSE 0.5
                END AS CONFIDENCE_SCORE,
                OBJECT_CONSTRUCT(
                    'source_table', 'RAW_ESTUARY._ESTUARY_RAW_ASANA_USERS',
                    'transformation_date', CURRENT_TIMESTAMP()::STRING,
                    'asana_gid', _ESTUARY_DATA:gid::VARCHAR,
                    'has_email', CASE WHEN _ESTUARY_DATA:email IS NOT NULL THEN TRUE ELSE FALSE END,
                    'has_photo', CASE WHEN _ESTUARY_DATA:photo IS NOT NULL THEN TRUE ELSE FALSE END
                ) AS AI_MEMORY_METADATA
            FROM RAW_ESTUARY._ESTUARY_RAW_ASANA_USERS
            WHERE _ESTUARY_DATA:gid IS NOT NULL
            AND NOT EXISTS (
                SELECT 1 FROM STG_TRANSFORMED.STG_ASANA_USERS 
                WHERE USER_GID = _ESTUARY_DATA:gid::VARCHAR
            );
            
            GET DIAGNOSTICS processed_count = ROW_COUNT;
            
            INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                RECORDS_PROCESSED, START_TIME, END_TIME, DURATION_SECONDS
            ) VALUES (
                'TRANSFORM_ASANA_USERS', 'STG_TRANSFORMED', 'STG_ASANA_USERS', 'TRANSFORM', 'SUCCESS',
                processed_count, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 0
            );
            
            RETURN 'Successfully processed ' || processed_count || ' Asana users';
            
        EXCEPTION
            WHEN OTHER THEN
                INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                    JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                    ERROR_MESSAGE, START_TIME, END_TIME
                ) VALUES (
                    'TRANSFORM_ASANA_USERS', 'STG_TRANSFORMED', 'STG_ASANA_USERS', 'TRANSFORM', 'FAILED',
                    SQLERRM, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()
                );
                
                RETURN 'Error transforming Asana users: ' || SQLERRM;
        END;
        $$;
        """

        procedures.append(
            await self.execute_sql_with_result_tracking(
                transform_users_sql, "TRANSFORM_ASANA_USERS"
            )
        )

        return procedures

    async def deploy_asana_ai_enrichment_procedures(self) -> List[DeploymentResult]:
        """Deploy AI enrichment procedures for Asana data"""
        logger.info("🤖 Deploying Asana AI enrichment procedures")

        procedures = []

        # AI Enrichment for Projects
        ai_enrichment_projects_sql = """
        CREATE OR REPLACE PROCEDURE GENERATE_ASANA_PROJECT_AI_EMBEDDINGS()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        DECLARE
            processed_count NUMBER DEFAULT 0;
        BEGIN
            
            -- Generate embeddings for Asana projects
            UPDATE STG_TRANSFORMED.STG_ASANA_PROJECTS
            SET 
                AI_MEMORY_EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_768(
                    'e5-base-v2',
                    COALESCE(PROJECT_NAME, '') || ' ' || 
                    COALESCE(PROJECT_DESCRIPTION, '') || ' ' ||
                    COALESCE(PROJECT_NOTES, '') || ' ' ||
                    COALESCE(TEAM_NAME, '') || ' ' ||
                    COALESCE(OWNER_NAME, '')
                ),
                AI_PROJECT_SUMMARY = CASE 
                    WHEN LENGTH(COALESCE(PROJECT_DESCRIPTION, '') || COALESCE(PROJECT_NOTES, '')) > 500
                    THEN SNOWFLAKE.CORTEX.SUMMARIZE(PROJECT_DESCRIPTION || ' ' || PROJECT_NOTES)
                    ELSE NULL
                END,
                AI_RISK_ASSESSMENT = CASE 
                    WHEN PROJECT_STATUS = 'OVERDUE' OR (DUE_DATE IS NOT NULL AND DUE_DATE < CURRENT_DATE) THEN 'HIGH'
                    WHEN COMPLETION_PERCENTAGE < 50 AND DUE_DATE IS NOT NULL AND DUE_DATE <= CURRENT_DATE + 30 THEN 'MEDIUM'
                    ELSE 'LOW'
                END,
                AI_HEALTH_SCORE = CASE 
                    WHEN IS_ARCHIVED = TRUE THEN 0.1
                    WHEN PROJECT_STATUS = 'COMPLETED' THEN 1.0
                    WHEN COMPLETION_PERCENTAGE >= 80 THEN 0.9
                    WHEN COMPLETION_PERCENTAGE >= 60 THEN 0.7
                    WHEN COMPLETION_PERCENTAGE >= 40 THEN 0.5
                    WHEN COMPLETION_PERCENTAGE >= 20 THEN 0.3
                    ELSE 0.1
                END,
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    COALESCE(AI_MEMORY_METADATA, OBJECT_CONSTRUCT()),
                    'ai_enrichment_date', CURRENT_TIMESTAMP()::STRING
                ),
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    AI_MEMORY_METADATA,
                    'embedding_model', 'e5-base-v2'
                ),
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    AI_MEMORY_METADATA,
                    'has_ai_summary', CASE WHEN AI_PROJECT_SUMMARY IS NOT NULL THEN TRUE ELSE FALSE END
                ),
                AI_MEMORY_UPDATED_AT = CURRENT_TIMESTAMP()
            WHERE AI_MEMORY_EMBEDDING IS NULL
            AND (PROJECT_NAME IS NOT NULL OR PROJECT_DESCRIPTION IS NOT NULL);
            
            GET DIAGNOSTICS processed_count = ROW_COUNT;
            
            INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                RECORDS_PROCESSED, START_TIME, END_TIME, DURATION_SECONDS
            ) VALUES (
                'GENERATE_ASANA_PROJECT_AI_EMBEDDINGS', 'STG_TRANSFORMED', 'STG_ASANA_PROJECTS', 'AI_ENRICHMENT', 'SUCCESS',
                processed_count, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 0
            );
            
            RETURN 'Successfully generated AI embeddings for ' || processed_count || ' Asana projects';
            
        EXCEPTION
            WHEN OTHER THEN
                INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                    JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                    ERROR_MESSAGE, START_TIME, END_TIME
                ) VALUES (
                    'GENERATE_ASANA_PROJECT_AI_EMBEDDINGS', 'STG_TRANSFORMED', 'STG_ASANA_PROJECTS', 'AI_ENRICHMENT', 'FAILED',
                    SQLERRM, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()
                );
                
                RETURN 'Error generating AI embeddings for Asana projects: ' || SQLERRM;
        END;
        $$;
        """

        procedures.append(
            await self.execute_sql_with_result_tracking(
                ai_enrichment_projects_sql, "GENERATE_ASANA_PROJECT_AI_EMBEDDINGS"
            )
        )

        # AI Enrichment for Tasks
        ai_enrichment_tasks_sql = """
        CREATE OR REPLACE PROCEDURE GENERATE_ASANA_TASK_AI_EMBEDDINGS()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        DECLARE
            processed_count NUMBER DEFAULT 0;
        BEGIN
            
            -- Generate embeddings for Asana tasks
            UPDATE STG_TRANSFORMED.STG_ASANA_TASKS
            SET 
                AI_MEMORY_EMBEDDING = SNOWFLAKE.CORTEX.EMBED_TEXT_768(
                    'e5-base-v2',
                    COALESCE(TASK_NAME, '') || ' ' || 
                    COALESCE(TASK_DESCRIPTION, '') || ' ' ||
                    COALESCE(TASK_NOTES, '') || ' ' ||
                    COALESCE(PROJECT_NAME, '') || ' ' ||
                    COALESCE(ASSIGNEE_NAME, '')
                ),
                AI_TASK_SENTIMENT = CASE 
                    WHEN TASK_DESCRIPTION IS NOT NULL AND LENGTH(TASK_DESCRIPTION) > 50
                    THEN SNOWFLAKE.CORTEX.SENTIMENT(TASK_DESCRIPTION)
                    ELSE NULL
                END,
                AI_TASK_SUMMARY = CASE 
                    WHEN LENGTH(COALESCE(TASK_DESCRIPTION, '') || COALESCE(TASK_NOTES, '')) > 500
                    THEN SNOWFLAKE.CORTEX.SUMMARIZE(TASK_DESCRIPTION || ' ' || TASK_NOTES)
                    ELSE NULL
                END,
                AI_URGENCY_SCORE = CASE 
                    WHEN TASK_STATUS = 'OVERDUE' THEN 1.0
                    WHEN TASK_STATUS = 'DUE_SOON' THEN 0.8
                    WHEN PRIORITY_LEVEL = 'HIGH' THEN 0.7
                    WHEN PRIORITY_LEVEL = 'MEDIUM' THEN 0.5
                    ELSE 0.3
                END,
                AI_COMPLETION_PREDICTION = CASE 
                    WHEN IS_COMPLETED = TRUE THEN 'COMPLETED'
                    WHEN DUE_DATE IS NOT NULL AND DUE_DATE < CURRENT_DATE THEN 'OVERDUE'
                    WHEN DUE_DATE IS NOT NULL AND DUE_DATE <= CURRENT_DATE + 7 THEN 'AT_RISK'
                    WHEN ASSIGNEE_GID IS NULL THEN 'UNASSIGNED'
                    ELSE 'ON_TRACK'
                END,
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    COALESCE(AI_MEMORY_METADATA, OBJECT_CONSTRUCT()),
                    'ai_enrichment_date', CURRENT_TIMESTAMP()::STRING
                ),
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    AI_MEMORY_METADATA,
                    'embedding_model', 'e5-base-v2'
                ),
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    AI_MEMORY_METADATA,
                    'has_sentiment', CASE WHEN AI_TASK_SENTIMENT IS NOT NULL THEN TRUE ELSE FALSE END
                ),
                AI_MEMORY_METADATA = OBJECT_INSERT(
                    AI_MEMORY_METADATA,
                    'has_ai_summary', CASE WHEN AI_TASK_SUMMARY IS NOT NULL THEN TRUE ELSE FALSE END
                ),
                AI_MEMORY_UPDATED_AT = CURRENT_TIMESTAMP()
            WHERE AI_MEMORY_EMBEDDING IS NULL
            AND (TASK_NAME IS NOT NULL OR TASK_DESCRIPTION IS NOT NULL);
            
            GET DIAGNOSTICS processed_count = ROW_COUNT;
            
            INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                RECORDS_PROCESSED, START_TIME, END_TIME, DURATION_SECONDS
            ) VALUES (
                'GENERATE_ASANA_TASK_AI_EMBEDDINGS', 'STG_TRANSFORMED', 'STG_ASANA_TASKS', 'AI_ENRICHMENT', 'SUCCESS',
                processed_count, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP(), 0
            );
            
            RETURN 'Successfully generated AI embeddings for ' || processed_count || ' Asana tasks';
            
        EXCEPTION
            WHEN OTHER THEN
                INSERT INTO OPS_MONITORING.ETL_JOB_LOGS (
                    JOB_NAME, SCHEMA_NAME, TABLE_NAME, OPERATION_TYPE, STATUS,
                    ERROR_MESSAGE, START_TIME, END_TIME
                ) VALUES (
                    'GENERATE_ASANA_TASK_AI_EMBEDDINGS', 'STG_TRANSFORMED', 'STG_ASANA_TASKS', 'AI_ENRICHMENT', 'FAILED',
                    SQLERRM, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP()
                );
                
                RETURN 'Error generating AI embeddings for Asana tasks: ' || SQLERRM;
        END;
        $$;
        """

        procedures.append(
            await self.execute_sql_with_result_tracking(
                ai_enrichment_tasks_sql, "GENERATE_ASANA_TASK_AI_EMBEDDINGS"
            )
        )

        return procedures

    async def deploy_asana_scheduled_tasks(self) -> List[DeploymentResult]:
        """Deploy scheduled tasks for automated Asana data processing"""
        logger.info("⏰ Deploying Asana scheduled tasks")

        tasks = []

        # Main Asana transformation task
        main_transform_task_sql = """
        CREATE OR REPLACE TASK TASK_TRANSFORM_ASANA_DATA
            WAREHOUSE = WH_SOPHIA_AI_PROCESSING
            SCHEDULE = 'USING CRON 0 */2 * * * UTC'
            COMMENT = 'Transform raw Asana data to structured schema every 2 hours'
        AS
        BEGIN
            CALL TRANSFORM_ASANA_PROJECTS();
            CALL TRANSFORM_ASANA_TASKS();
            CALL TRANSFORM_ASANA_USERS();
        END;
        """

        tasks.append(
            await self.execute_sql_with_result_tracking(
                main_transform_task_sql, "TASK_TRANSFORM_ASANA_DATA"
            )
        )

        # AI enrichment task
        ai_enrichment_task_sql = """
        CREATE OR REPLACE TASK TASK_ASANA_AI_ENRICHMENT
            WAREHOUSE = WH_SOPHIA_AI_PROCESSING
            SCHEDULE = 'USING CRON 0 */4 * * * UTC'
            COMMENT = 'Generate AI embeddings and enrichment for Asana data every 4 hours'
            AFTER TASK_TRANSFORM_ASANA_DATA
        AS
        BEGIN
            CALL GENERATE_ASANA_PROJECT_AI_EMBEDDINGS();
            CALL GENERATE_ASANA_TASK_AI_EMBEDDINGS();
        END;
        """

        tasks.append(
            await self.execute_sql_with_result_tracking(
                ai_enrichment_task_sql, "TASK_ASANA_AI_ENRICHMENT"
            )
        )

        # Resume tasks
        resume_tasks_sql = """
        ALTER TASK TASK_TRANSFORM_ASANA_DATA RESUME;
        ALTER TASK TASK_ASANA_AI_ENRICHMENT RESUME;
        """

        tasks.append(
            await self.execute_sql_with_result_tracking(
                resume_tasks_sql, "RESUME_ASANA_TASKS"
            )
        )

        return tasks

    async def deploy_asana_data_quality_monitoring(self) -> List[DeploymentResult]:
        """Deploy data quality monitoring for Asana data"""
        logger.info("📊 Deploying Asana data quality monitoring")

        monitoring = []

        # Data quality monitoring procedure
        quality_monitoring_sql = """
        CREATE OR REPLACE PROCEDURE MONITOR_ASANA_DATA_QUALITY()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        DECLARE
            project_count NUMBER;
            task_count NUMBER;
            user_count NUMBER;
            quality_score FLOAT;
        BEGIN
            
            -- Count records in each table
            SELECT COUNT(*) INTO project_count FROM STG_TRANSFORMED.STG_ASANA_PROJECTS;
            SELECT COUNT(*) INTO task_count FROM STG_TRANSFORMED.STG_ASANA_TASKS;
            SELECT COUNT(*) INTO user_count FROM STG_TRANSFORMED.STG_ASANA_USERS;
            
            -- Calculate overall quality score
            quality_score := CASE 
                WHEN project_count > 0 AND task_count > 0 AND user_count > 0 THEN 1.0
                WHEN project_count > 0 AND task_count > 0 THEN 0.8
                WHEN project_count > 0 OR task_count > 0 THEN 0.6
                ELSE 0.0
            END;
            
            -- Insert monitoring record
            INSERT INTO OPS_MONITORING.DATA_QUALITY_METRICS (
                SCHEMA_NAME, TABLE_NAME, METRIC_NAME, METRIC_VALUE, 
                MEASUREMENT_DATE, QUALITY_THRESHOLD, IS_PASSING
            ) VALUES 
            ('STG_TRANSFORMED', 'STG_ASANA_PROJECTS', 'RECORD_COUNT', project_count, CURRENT_TIMESTAMP(), 1, project_count >= 1),
            ('STG_TRANSFORMED', 'STG_ASANA_TASKS', 'RECORD_COUNT', task_count, CURRENT_TIMESTAMP(), 1, task_count >= 1),
            ('STG_TRANSFORMED', 'STG_ASANA_USERS', 'RECORD_COUNT', user_count, CURRENT_TIMESTAMP(), 1, user_count >= 1),
            ('STG_TRANSFORMED', 'ASANA_OVERALL', 'QUALITY_SCORE', quality_score, CURRENT_TIMESTAMP(), 0.8, quality_score >= 0.8);
            
            RETURN 'Quality monitoring completed. Projects: ' || project_count || ', Tasks: ' || task_count || ', Users: ' || user_count || ', Quality Score: ' || quality_score;
            
        EXCEPTION
            WHEN OTHER THEN
                RETURN 'Error in quality monitoring: ' || SQLERRM;
        END;
        $$;
        """

        monitoring.append(
            await self.execute_sql_with_result_tracking(
                quality_monitoring_sql, "MONITOR_ASANA_DATA_QUALITY"
            )
        )

        # Quality monitoring task
        quality_task_sql = """
        CREATE OR REPLACE TASK TASK_MONITOR_ASANA_QUALITY
            WAREHOUSE = WH_SOPHIA_AI_PROCESSING
            SCHEDULE = 'USING CRON 0 */6 * * * UTC'
            COMMENT = 'Monitor Asana data quality every 6 hours'
        AS
        BEGIN
            CALL MONITOR_ASANA_DATA_QUALITY();
        END;
        """

        monitoring.append(
            await self.execute_sql_with_result_tracking(
                quality_task_sql, "TASK_MONITOR_ASANA_QUALITY"
            )
        )

        # Resume quality monitoring task
        resume_quality_task_sql = "ALTER TASK TASK_MONITOR_ASANA_QUALITY RESUME;"

        monitoring.append(
            await self.execute_sql_with_result_tracking(
                resume_quality_task_sql, "RESUME_QUALITY_MONITORING"
            )
        )

        return monitoring

    async def deploy_all_asana_infrastructure(
        self,
    ) -> Dict[str, List[DeploymentResult]]:
        """Deploy complete Asana Snowflake infrastructure"""
        logger.info("🚀 Deploying complete Asana Snowflake infrastructure")

        all_results = {
            "transformation_procedures": await self.deploy_asana_transformation_procedures(),
            "ai_enrichment_procedures": await self.deploy_asana_ai_enrichment_procedures(),
            "scheduled_tasks": await self.deploy_asana_scheduled_tasks(),
            "quality_monitoring": await self.deploy_asana_data_quality_monitoring(),
        }

        # Generate deployment summary
        total_deployments = sum(len(results) for results in all_results.values())
        successful_deployments = sum(
            len([r for r in results if r.status == "SUCCESS"])
            for results in all_results.values()
        )

        logger.info(
            f"✅ Asana infrastructure deployment completed: {successful_deployments}/{total_deployments} successful"
        )

        return all_results

    async def close(self) -> None:
        """Clean up resources"""
        if self.cortex_service:
            await self.cortex_service.close()


async def main():
    """Main deployment function"""
    parser = argparse.ArgumentParser(
        description="Deploy Asana Snowflake Infrastructure"
    )
    parser.add_argument("--env", default="dev", choices=["dev", "staging", "prod"])
    parser.add_argument(
        "--execute-all", action="store_true", help="Execute all deployment steps"
    )
    parser.add_argument(
        "--component",
        choices=["transform", "ai", "tasks", "monitoring"],
        help="Deploy specific component",
    )

    args = parser.parse_args()

    deployer = AsanaSnowflakeDeployer(args.env)

    try:
        await deployer.initialize()

        if args.execute_all:
            results = await deployer.deploy_all_asana_infrastructure()
            print("✅ Complete Asana infrastructure deployment completed")

            # Print summary
            for category, category_results in results.items():
                print(f"\n{category.replace('_', ' ').title()}:")
                for result in category_results:
                    status_icon = "✅" if result.status == "SUCCESS" else "❌"
                    print(f"  {status_icon} {result.component}: {result.message}")

        elif args.component:
            if args.component == "transform":
                results = await deployer.deploy_asana_transformation_procedures()
            elif args.component == "ai":
                results = await deployer.deploy_asana_ai_enrichment_procedures()
            elif args.component == "tasks":
                results = await deployer.deploy_asana_scheduled_tasks()
            elif args.component == "monitoring":
                results = await deployer.deploy_asana_data_quality_monitoring()

            print(f"✅ {args.component.title()} deployment completed")
            for result in results:
                status_icon = "✅" if result.status == "SUCCESS" else "❌"
                print(f"  {status_icon} {result.component}: {result.message}")

        else:
            print("❌ Please specify --execute-all or --component")

    except Exception as e:
        print(f"❌ Deployment failed: {e}")
        return 1
    finally:
        await deployer.close()

    return 0


if __name__ == "__main__":
    exit(asyncio.run(main()))
