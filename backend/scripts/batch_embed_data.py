#!/usr/bin/env python3
"""
Batch Embedding Generation Script for Sophia AI

This script iterates through new/updated records in STG_GONG_CALL_TRANSCRIPTS and
STG_HUBSPOT_DEALS tables and generates embeddings using Snowflake Cortex AI.

Features:
- Batch processing with configurable batch sizes
- Progress tracking and resumption
- Error handling with retry logic
- Performance monitoring and metrics
- Integration with Sophia AI configuration system
- Support for multiple embedding models
- Incremental processing based on update timestamps

Usage:
    python backend/scripts/batch_embed_data.py --table STG_GONG_CALL_TRANSCRIPTS --batch-size 100
    python backend/scripts/batch_embed_data.py --table STG_HUBSPOT_DEALS --force-refresh
    python backend/scripts/batch_embed_data.py --all-tables --dry-run
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 684 lines

Recommended decomposition:
- batch_embed_data_core.py - Core functionality
- batch_embed_data_utils.py - Utility functions
- batch_embed_data_models.py - Data models
- batch_embed_data_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import argparse
import asyncio
import logging
import sys
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

import structlog
from tqdm import tqdm

from backend.core.snowflake_config_manager import SnowflakeConfigManager

# Import Sophia AI components
from backend.utils.snowflake_cortex_service import SnowflakeCortexService

# Configure structured logging
logging.basicConfig(level=logging.INFO)
logger = structlog.get_logger()


class EmbeddingTable(Enum):
    """Supported tables for embedding generation"""

    GONG_CALL_TRANSCRIPTS = "STG_GONG_CALL_TRANSCRIPTS"
    HUBSPOT_DEALS = "STG_HUBSPOT_DEALS"
    GONG_CALLS = "STG_GONG_CALLS"
    # New data sources
    SLACK_MESSAGES = "STG_SLACK_MESSAGES"
    SLACK_CONVERSATIONS = "STG_SLACK_CONVERSATIONS"
    LINEAR_ISSUES = "STG_LINEAR_ISSUES"
    KB_ARTICLES = "KB_ARTICLES"
    KB_ENTITIES = "KB_ENTITIES"
    KB_UNSTRUCTURED_DOCUMENTS = "KB_UNSTRUCTURED_DOCUMENTS"
    # Foundational knowledge
    FOUNDATIONAL_EMPLOYEES = "FOUNDATIONAL_KNOWLEDGE.EMPLOYEES"
    FOUNDATIONAL_CUSTOMERS = "FOUNDATIONAL_KNOWLEDGE.CUSTOMERS"
    FOUNDATIONAL_PRODUCTS = "FOUNDATIONAL_KNOWLEDGE.PRODUCTS_SERVICES"
    FOUNDATIONAL_COMPETITORS = "FOUNDATIONAL_KNOWLEDGE.COMPETITORS"


@dataclass
class EmbeddingConfig:
    """Configuration for embedding generation"""

    table_name: str
    id_column: str
    text_column: str
    embedding_column: str = "ai_memory_embedding"
    metadata_column: str = "ai_memory_metadata"
    updated_column: str = "ai_memory_updated_at"
    where_condition: str = ""
    batch_size: int = 50
    model: str = "e5-base-v2"


@dataclass
class ProcessingStats:
    """Statistics for embedding processing"""

    total_records: int = 0
    processed_records: int = 0
    successful_embeddings: int = 0
    failed_embeddings: int = 0
    skipped_records: int = 0
    start_time: datetime = None
    end_time: datetime = None

    @property
    def duration_seconds(self) -> float:
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0

    @property
    def success_rate(self) -> float:
        if self.processed_records > 0:
            return self.successful_embeddings / self.processed_records
        return 0.0

    @property
    def records_per_second(self) -> float:
        if self.duration_seconds > 0:
            return self.processed_records / self.duration_seconds
        return 0.0


class BatchEmbeddingProcessor:
    """Main class for batch embedding processing"""

    def __init__(self):
        self.cortex_service = SnowflakeCortexService()
        self.config_manager = SnowflakeConfigManager()
        self.stats = ProcessingStats()

        # Default configuration
        self.default_batch_size = 50
        self.default_model = "e5-base-v2"
        self.max_retries = 3

        # Table configurations
        self.table_configs = {
            EmbeddingTable.GONG_CALL_TRANSCRIPTS: EmbeddingConfig(
                table_name="STG_GONG_CALL_TRANSCRIPTS",
                id_column="TRANSCRIPT_ID",
                text_column="TRANSCRIPT_TEXT",
                where_condition="TRANSCRIPT_TEXT IS NOT NULL AND LENGTH(TRANSCRIPT_TEXT) > 10",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.HUBSPOT_DEALS: EmbeddingConfig(
                table_name="STG_HUBSPOT_DEALS",
                id_column="DEAL_ID",
                text_column="DEAL_NAME || ' - ' || COALESCE(DEAL_STAGE, '') || ' - ' || COALESCE(PIPELINE_NAME, '')",
                where_condition="DEAL_NAME IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.GONG_CALLS: EmbeddingConfig(
                table_name="STG_GONG_CALLS",
                id_column="CALL_ID",
                text_column="CALL_TITLE || ' - ' || COALESCE(ACCOUNT_NAME, '') || ' - ' || COALESCE(DEAL_STAGE, '')",
                where_condition="CALL_TITLE IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            # New Slack configurations
            EmbeddingTable.SLACK_MESSAGES: EmbeddingConfig(
                table_name="SLACK_DATA.STG_SLACK_MESSAGES",
                id_column="MESSAGE_ID",
                text_column="MESSAGE_TEXT",
                where_condition="MESSAGE_TEXT IS NOT NULL AND LENGTH(MESSAGE_TEXT) > 5",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.SLACK_CONVERSATIONS: EmbeddingConfig(
                table_name="SLACK_DATA.STG_SLACK_CONVERSATIONS",
                id_column="CONVERSATION_ID",
                text_column="CONVERSATION_TITLE || ' - ' || COALESCE(CONVERSATION_SUMMARY, '')",
                where_condition="CONVERSATION_TITLE IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            # Linear configurations
            EmbeddingTable.LINEAR_ISSUES: EmbeddingConfig(
                table_name="LINEAR_DATA.STG_LINEAR_ISSUES",
                id_column="ISSUE_ID",
                text_column="ISSUE_TITLE || ' - ' || COALESCE(ISSUE_DESCRIPTION, '') || ' - ' || COALESCE(PROJECT_NAME, '')",
                where_condition="ISSUE_TITLE IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            # Knowledge Base configurations
            EmbeddingTable.KB_ARTICLES: EmbeddingConfig(
                table_name="KNOWLEDGE_BASE.KB_ARTICLES",
                id_column="ARTICLE_ID",
                text_column="ARTICLE_TITLE || ' - ' || COALESCE(ARTICLE_SUMMARY, '') || ' - ' || COALESCE(SUBSTR(ARTICLE_CONTENT, 1, 1000), '')",
                where_condition="ARTICLE_TITLE IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.KB_ENTITIES: EmbeddingConfig(
                table_name="KNOWLEDGE_BASE.KB_ENTITIES",
                id_column="ENTITY_ID",
                text_column="ENTITY_NAME || ' - ' || COALESCE(ENTITY_DESCRIPTION, '') || ' - ' || COALESCE(ENTITY_TYPE, '')",
                where_condition="ENTITY_NAME IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.KB_UNSTRUCTURED_DOCUMENTS: EmbeddingConfig(
                table_name="KNOWLEDGE_BASE.KB_UNSTRUCTURED_DOCUMENTS",
                id_column="DOCUMENT_ID",
                text_column="DOCUMENT_TITLE || ' - ' || COALESCE(DOCUMENT_SUMMARY, '') || ' - ' || COALESCE(SUBSTR(EXTRACTED_TEXT, 1, 2000), '')",
                where_condition="DOCUMENT_TITLE IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            # Foundational Knowledge configurations
            EmbeddingTable.FOUNDATIONAL_EMPLOYEES: EmbeddingConfig(
                table_name="FOUNDATIONAL_KNOWLEDGE.EMPLOYEES",
                id_column="EMPLOYEE_ID",
                text_column="FULL_NAME || ' - ' || COALESCE(JOB_TITLE, '') || ' - ' || COALESCE(DEPARTMENT, '') || ' - ' || COALESCE(ARRAY_TO_STRING(PRIMARY_SKILLS, ' '), '')",
                where_condition="FULL_NAME IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.FOUNDATIONAL_CUSTOMERS: EmbeddingConfig(
                table_name="FOUNDATIONAL_KNOWLEDGE.CUSTOMERS",
                id_column="CUSTOMER_ID",
                text_column="COMPANY_NAME || ' - ' || COALESCE(INDUSTRY, '') || ' - ' || COALESCE(CUSTOMER_TIER, '') || ' - ' || COALESCE(HEADQUARTERS_CITY, '')",
                where_condition="COMPANY_NAME IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.FOUNDATIONAL_PRODUCTS: EmbeddingConfig(
                table_name="FOUNDATIONAL_KNOWLEDGE.PRODUCTS_SERVICES",
                id_column="PRODUCT_ID",
                text_column="PRODUCT_NAME || ' - ' || COALESCE(PRODUCT_DESCRIPTION, '') || ' - ' || COALESCE(PRODUCT_CATEGORY, '') || ' - ' || COALESCE(VALUE_PROPOSITION, '')",
                where_condition="PRODUCT_NAME IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
            EmbeddingTable.FOUNDATIONAL_COMPETITORS: EmbeddingConfig(
                table_name="FOUNDATIONAL_KNOWLEDGE.COMPETITORS",
                id_column="COMPETITOR_ID",
                text_column="COMPANY_NAME || ' - ' || COALESCE(COMPANY_DESCRIPTION, '') || ' - ' || COALESCE(MARKET_SEGMENT, '') || ' - ' || COALESCE(COMPETITIVE_TIER, '')",
                where_condition="COMPANY_NAME IS NOT NULL",
                batch_size=self.default_batch_size,
                model=self.default_model,
            ),
        }

    async def initialize(self):
        """Initialize the processor"""
        await self.cortex_service.initialize()
        logger.info("Batch embedding processor initialized")

    async def close(self):
        """Clean up resources"""
        await self.cortex_service.close()
        logger.info("Batch embedding processor closed")

    async def get_records_to_process(
        self,
        config: EmbeddingConfig,
        force_refresh: bool = False,
        limit: int | None = None,
    ) -> list[dict[str, Any]]:
        """Get records that need embedding generation"""

        # Build WHERE conditions
        conditions = [config.where_condition] if config.where_condition else []

        if not force_refresh:
            # Only process records without embeddings or updated since last embedding
            conditions.append(
                f"""
                ({config.embedding_column} IS NULL
                 OR {config.updated_column} IS NULL
                 OR UPDATED_AT > {config.updated_column})
            """
            )

        where_clause = " AND ".join(conditions) if conditions else "1=1"

        # Build query
        query = f"""
        SELECT
            {config.id_column} as id,
            {config.text_column} as text_content,
            {config.embedding_column} as current_embedding,
            {config.updated_column} as last_embedding_update,
            UPDATED_AT as last_record_update
        FROM {config.table_name}
        WHERE {where_clause}
        ORDER BY UPDATED_AT DESC
        """

        if limit:
            query += f" LIMIT {limit}"

        try:
            cursor = self.cortex_service.connection.cursor()
            cursor.execute(query, query_params if "query_params" in locals() else ())

            columns = [desc[0] for desc in cursor.description]
            results = cursor.fetchall()

            records = []
            for row in results:
                record = dict(zip(columns, row, strict=False))
                records.append(record)

            logger.info(
                f"Found {len(records)} records to process in {config.table_name}"
            )
            return records

        except Exception as e:
            logger.error(f"Error querying records from {config.table_name}: {e}")
            raise
        finally:
            if cursor:
                cursor.close()

    async def process_batch(
        self, records: list[dict[str, Any]], config: EmbeddingConfig
    ) -> tuple[int, int]:
        """Process a batch of records for embedding generation"""

        successful = 0
        failed = 0

        for record in records:
            record_id = record["ID"]
            text_content = record["TEXT_CONTENT"]

            if not text_content or not text_content.strip():
                logger.warning(f"Skipping record {record_id}: empty text content")
                self.stats.skipped_records += 1
                continue

            # Prepare metadata
            metadata = {
                "table_name": config.table_name,
                "text_column": config.text_column,
                "embedding_model": config.model,
                "processed_at": datetime.now().isoformat(),
                "batch_processing": True,
                "original_text_length": len(text_content),
            }

            # Attempt to store embedding with retries
            retry_count = 0
            while retry_count <= self.max_retries:
                try:
                    success = (
                        await self.cortex_service.store_embedding_in_business_table(
                            table_name=config.table_name,
                            record_id=record_id,
                            text_content=text_content,
                            embedding_column=config.embedding_column,
                            metadata=metadata,
                            model=config.model,
                        )
                    )

                    if success:
                        successful += 1
                        logger.debug(f"Successfully processed {record_id}")
                        break
                    else:
                        raise Exception("Embedding storage returned False")

                except Exception as e:
                    retry_count += 1
                    if retry_count <= self.max_retries:
                        wait_time = 2**retry_count  # Exponential backoff
                        logger.warning(
                            f"Retry {retry_count}/{self.max_retries} for {record_id} after {wait_time}s: {e}"
                        )
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(
                            f"Failed to process {record_id} after {self.max_retries} retries: {e}"
                        )
                        failed += 1
                        break

        return successful, failed

    async def process_table(
        self,
        table: EmbeddingTable,
        force_refresh: bool = False,
        dry_run: bool = False,
        limit: int | None = None,
    ) -> ProcessingStats:
        """Process all records in a specific table"""

        config = self.table_configs[table]
        logger.info(f"Starting embedding processing for {config.table_name}")

        # Ensure embedding columns exist
        try:
            await self.cortex_service.ensure_embedding_columns_exist(config.table_name)
        except Exception as e:
            logger.error(
                f"Failed to ensure embedding columns exist in {config.table_name}: {e}"
            )
            raise

        # Get records to process
        records = await self.get_records_to_process(config, force_refresh, limit)

        if not records:
            logger.info(f"No records to process in {config.table_name}")
            return ProcessingStats()

        # Initialize stats
        stats = ProcessingStats()
        stats.total_records = len(records)
        stats.start_time = datetime.now()

        if dry_run:
            logger.info(
                f"DRY RUN: Would process {len(records)} records in {config.table_name}"
            )
            return stats

        # Process records in batches
        batch_size = config.batch_size
        total_batches = (len(records) + batch_size - 1) // batch_size

        with tqdm(total=len(records), desc=f"Processing {config.table_name}") as pbar:
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(records))
                batch = records[start_idx:end_idx]

                logger.info(
                    f"Processing batch {batch_idx + 1}/{total_batches} ({len(batch)} records)"
                )

                try:
                    successful, failed = await self.process_batch(batch, config)

                    stats.processed_records += len(batch)
                    stats.successful_embeddings += successful
                    stats.failed_embeddings += failed

                    pbar.update(len(batch))
                    pbar.set_postfix(
                        {
                            "Success": f"{stats.successful_embeddings}/{stats.processed_records}",
                            "Rate": f"{stats.success_rate:.2%}",
                        }
                    )

                except Exception as e:
                    logger.error(f"Error processing batch {batch_idx + 1}: {e}")
                    stats.failed_embeddings += len(batch)
                    stats.processed_records += len(batch)
                    pbar.update(len(batch))

        stats.end_time = datetime.now()

        # Log final statistics
        logger.info(f"Completed processing {config.table_name}")
        logger.info(f"Total records: {stats.total_records}")
        logger.info(f"Successful embeddings: {stats.successful_embeddings}")
        logger.info(f"Failed embeddings: {stats.failed_embeddings}")
        logger.info(f"Success rate: {stats.success_rate:.2%}")
        logger.info(f"Processing rate: {stats.records_per_second:.2f} records/second")
        logger.info(f"Duration: {stats.duration_seconds:.2f} seconds")

        return stats

    async def process_all_tables(
        self,
        force_refresh: bool = False,
        dry_run: bool = False,
        limit: int | None = None,
    ) -> dict[str, ProcessingStats]:
        """Process all supported tables"""

        logger.info("Starting batch embedding processing for all tables")

        results = {}
        total_stats = ProcessingStats()
        total_stats.start_time = datetime.now()

        for table in EmbeddingTable:
            try:
                stats = await self.process_table(table, force_refresh, dry_run, limit)
                results[table.value] = stats

                # Aggregate stats
                total_stats.total_records += stats.total_records
                total_stats.processed_records += stats.processed_records
                total_stats.successful_embeddings += stats.successful_embeddings
                total_stats.failed_embeddings += stats.failed_embeddings
                total_stats.skipped_records += stats.skipped_records

            except Exception as e:
                logger.error(f"Failed to process table {table.value}: {e}")
                results[table.value] = ProcessingStats()

        total_stats.end_time = datetime.now()

        # Log overall statistics
        logger.info("Completed batch embedding processing for all tables")
        logger.info(f"Overall total records: {total_stats.total_records}")
        logger.info(
            f"Overall successful embeddings: {total_stats.successful_embeddings}"
        )
        logger.info(f"Overall failed embeddings: {total_stats.failed_embeddings}")
        logger.info(f"Overall success rate: {total_stats.success_rate:.2%}")
        logger.info(
            f"Overall processing rate: {total_stats.records_per_second:.2f} records/second"
        )
        logger.info(f"Overall duration: {total_stats.duration_seconds:.2f} seconds")

        return results

    async def get_processing_status(self) -> dict[str, Any]:
        """Get current processing status for all tables"""

        status = {}

        for table in EmbeddingTable:
            config = self.table_configs[table]

            try:
                # Query table statistics
                query = f"""
                SELECT
                    COUNT(*) as total_records,
                    COUNT({config.embedding_column}) as records_with_embeddings,
                    COUNT(CASE WHEN {config.embedding_column} IS NULL THEN 1 END) as records_without_embeddings,
                    MAX({config.updated_column}) as last_embedding_update,
                    MAX(UPDATED_AT) as last_record_update
                FROM {config.table_name}
                WHERE {config.where_condition if config.where_condition else "1=1"}
                """

                cursor = self.cortex_service.connection.cursor()
                cursor.execute(
                    query, query_params if "query_params" in locals() else ()
                )
                result = cursor.fetchone()

                status[table.value] = {
                    "total_records": result[0],
                    "records_with_embeddings": result[1],
                    "records_without_embeddings": result[2],
                    "embedding_coverage": result[1] / result[0] if result[0] > 0 else 0,
                    "last_embedding_update": result[3],
                    "last_record_update": result[4],
                    "needs_processing": result[2] > 0,
                }

            except Exception as e:
                logger.error(f"Error getting status for {table.value}: {e}")
                status[table.value] = {"error": str(e)}
            finally:
                if cursor:
                    cursor.close()

        return status


async def main():
    """Main function for command-line usage"""

    parser = argparse.ArgumentParser(
        description="Batch Embedding Generation for Sophia AI"
    )
    parser.add_argument(
        "--table",
        choices=[table.value for table in EmbeddingTable],
        help="Specific table to process",
    )
    parser.add_argument(
        "--all-tables", action="store_true", help="Process all supported tables"
    )
    parser.add_argument(
        "--force-refresh",
        action="store_true",
        help="Force refresh all embeddings, even if they already exist",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be processed without actually doing it",
    )
    parser.add_argument(
        "--batch-size", type=int, help="Batch size for processing (overrides config)"
    )
    parser.add_argument(
        "--limit", type=int, help="Limit number of records to process (for testing)"
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="Show current processing status for all tables",
    )
    parser.add_argument("--model", default="e5-base-v2", help="Embedding model to use")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Configure logging
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Validate arguments
    if not args.table and not args.all_tables and not args.status:
        parser.error("Must specify either --table, --all-tables, or --status")

    if args.table and args.all_tables:
        parser.error("Cannot specify both --table and --all-tables")

    # Initialize processor
    processor = BatchEmbeddingProcessor()

    try:
        await processor.initialize()

        # Override batch size if specified
        if args.batch_size:
            for config in processor.table_configs.values():
                config.batch_size = args.batch_size

        # Override model if specified
        if args.model:
            for config in processor.table_configs.values():
                config.model = args.model

        if args.status:
            # Show status
            status = await processor.get_processing_status()
            for _table_name, table_status in status.items():
                if "error" in table_status:
                    pass
                else:
                    pass

        elif args.all_tables:
            # Process all tables
            results = await processor.process_all_tables(
                force_refresh=args.force_refresh, dry_run=args.dry_run, limit=args.limit
            )

            # Print summary
            for _table_name, _stats in results.items():
                pass

        else:
            # Process specific table
            table_enum = EmbeddingTable(args.table)
            await processor.process_table(
                table_enum,
                force_refresh=args.force_refresh,
                dry_run=args.dry_run,
                limit=args.limit,
            )

    except KeyboardInterrupt:
        logger.info("Processing interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        sys.exit(1)
    finally:
        await processor.close()


if __name__ == "__main__":
    asyncio.run(main())
