from datetime import UTC, datetime

#!/usr/bin/env python3
"""
Enhanced Gong Snowflake Setup Deployment Script

Production-ready deployment script with comprehensive error handling,
data quality validation, monitoring integration, and rollback capabilities.

Features:
- Enhanced error handling and retry logic
- Comprehensive logging and monitoring
- Data quality validation procedures
- Rollback capabilities
- Performance optimization
- Security compliance
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 776 lines

Recommended decomposition:
- enhanced_deploy_gong_snowflake_setup_core.py - Core functionality
- enhanced_deploy_gong_snowflake_setup_utils.py - Utility functions
- enhanced_deploy_gong_snowflake_setup_models.py - Data models
- enhanced_deploy_gong_snowflake_setup_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import argparse
import asyncio
import json
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any

import snowflake.connector

from backend.core.auto_esc_config import get_config_value
from backend.utils.snowflake_cortex_service import SnowflakeCortexService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeploymentEnvironment(Enum):
    """Deployment environments"""

    DEV = "dev"
    STAGING = "staging"
    PROD = "prod"


class DeploymentPhase(Enum):
    """Deployment phases for rollback capabilities"""

    SCHEMA_CREATION = "schema_creation"
    RAW_TABLES = "raw_tables"
    STG_TABLES = "stg_tables"
    PROCEDURES = "procedures"
    TASKS = "tasks"
    PERMISSIONS = "permissions"
    VALIDATION = "validation"


@dataclass
class DeploymentConfig:
    """Enhanced deployment configuration"""

    environment: DeploymentEnvironment
    database: str
    warehouse: str
    role: str
    dry_run: bool = False
    force_deploy: bool = False
    rollback_enabled: bool = True
    validation_enabled: bool = True

    # Schema configurations
    raw_schema: str = "RAW_ESTUARY"
    stg_schema: str = "STG_TRANSFORMED"
    ops_schema: str = "OPS_MONITORING"
    ai_memory_schema: str = "AI_MEMORY"


class EnhancedGongSnowflakeDeployer:
    """
    Enhanced Gong Snowflake Deployer

    Production-ready deployment with:
    - Comprehensive error handling and rollback
    - Data quality validation procedures
    - Performance monitoring integration
    - Security compliance validation
    - Idempotent operations
    """

    def __init__(self, config: DeploymentConfig):
        self.config = config
        self.connection: snowflake.connector.SnowflakeConnection | None = None
        self.cortex_service: SnowflakeCortexService | None = None

        # Deployment tracking
        self.deployment_log: list[dict[str, Any]] = []
        self.deployment_id = (
            f"GONG_DEPLOY_{datetime.now(UTC).strftime('%Y%m%d_%H%M%S')}"
        )
        self.completed_phases: list[DeploymentPhase] = []

    async def deploy_complete_infrastructure(self) -> dict[str, Any]:
        """Deploy complete Gong infrastructure with enhanced error handling"""
        start_time = datetime.now(UTC)

        try:
            logger.info("üöÄ Starting enhanced Gong Snowflake deployment")
            logger.info(f"Environment: {self.config.environment.value}")
            logger.info(f"Database: {self.config.database}")
            logger.info(f"Deployment ID: {self.deployment_id}")

            if self.config.dry_run:
                logger.info("üîç DRY RUN MODE - No changes will be made")

            # Initialize connections
            await self._initialize_connections()

            # Execute deployment phases
            deployment_phases = [
                (DeploymentPhase.SCHEMA_CREATION, self._deploy_schemas),
                (DeploymentPhase.RAW_TABLES, self._deploy_raw_estuary_tables),
                (DeploymentPhase.STG_TABLES, self._deploy_stg_transformed_tables),
                (DeploymentPhase.PROCEDURES, self._deploy_transformation_procedures),
                (DeploymentPhase.TASKS, self._deploy_automated_tasks),
                (DeploymentPhase.PERMISSIONS, self._deploy_permissions),
                (DeploymentPhase.VALIDATION, self._validate_deployment),
            ]

            for phase, deploy_func in deployment_phases:
                try:
                    logger.info(f"üìã Executing phase: {phase.value}")
                    await deploy_func()
                    self.completed_phases.append(phase)
                    await self._log_phase_completion(phase, "SUCCESS")

                except Exception as e:
                    await self._log_phase_completion(phase, "FAILED", str(e))

                    if self.config.rollback_enabled:
                        logger.warning(
                            f"‚ö†Ô∏è Phase {phase.value} failed, initiating rollback"
                        )
                        await self._rollback_deployment()

                    raise Exception(f"Deployment failed at phase {phase.value}: {e}")

            # Generate deployment summary
            total_time = (datetime.now(UTC) - start_time).total_seconds()

            summary = {
                "deployment_id": self.deployment_id,
                "status": "SUCCESS",
                "environment": self.config.environment.value,
                "database": self.config.database,
                "start_time": start_time.isoformat(),
                "end_time": datetime.now(UTC).isoformat(),
                "total_time_seconds": total_time,
                "completed_phases": [phase.value for phase in self.completed_phases],
                "deployment_log": self.deployment_log,
                "dry_run": self.config.dry_run,
            }

            logger.info("‚úÖ Enhanced Gong Snowflake deployment completed successfully")
            logger.info(f"Total time: {total_time:.2f} seconds")

            return summary

        except Exception as e:
            logger.error(f"‚ùå Deployment failed: {e}")

            return {
                "deployment_id": self.deployment_id,
                "status": "FAILED",
                "error": str(e),
                "completed_phases": [phase.value for phase in self.completed_phases],
                "deployment_log": self.deployment_log,
            }
        finally:
            await self._cleanup_connections()

    async def _initialize_connections(self) -> None:
        """Initialize database connections"""
        try:
            # Initialize Snowflake connection
            self.connection = snowflake.connector.connect(
                account=get_config_value("snowflake_account"),
                user=get_config_value("snowflake_user"),
                password=get_config_value("snowflake_password"),
                warehouse=self.config.warehouse,
                database=self.config.database,
                role=self.config.role,
            )

            # Initialize Cortex service for advanced operations
            self.cortex_service = SnowflakeCortexService()
            await self.cortex_service.initialize()

            logger.info("‚úÖ Database connections initialized")

        except Exception as e:
            logger.error(f"Failed to initialize connections: {e}")
            raise

    async def _deploy_schemas(self) -> None:
        """Deploy all required schemas"""
        schemas = [
            (self.config.raw_schema, "Raw Estuary ingestion data"),
            (self.config.stg_schema, "Staged and transformed data"),
            (self.config.ops_schema, "Operational monitoring and logging"),
            (self.config.ai_memory_schema, "AI Memory and semantic search"),
        ]

        for schema_name, description in schemas:
            sql = f"""
            CREATE SCHEMA IF NOT EXISTS {self.config.database}.{schema_name}
            COMMENT = '{description} - Environment: {self.config.environment.value}';
            """
            await self._execute_sql(f"create_schema_{schema_name}", sql)

    async def _deploy_raw_estuary_tables(self) -> None:
        """Deploy RAW_ESTUARY tables with enhanced structure"""

        # Enhanced RAW_GONG_CALLS_RAW table
        calls_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.raw_schema}.RAW_GONG_CALLS_RAW (
            _ESTUARY_AB_ID VARCHAR(64) PRIMARY KEY,
            _ESTUARY_EMITTED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            _ESTUARY_NORMALIZED_AT TIMESTAMP_LTZ,
            _ESTUARY_RAW_GONG_CALLS_HASHID VARCHAR(64),
            _ESTUARY_DATA VARIANT NOT NULL,

            -- Enhanced extraction helper columns
            CALL_ID VARCHAR(255) AS (_ESTUARY_DATA:id::VARCHAR),
            CALL_STARTED_AT TIMESTAMP_LTZ AS (TRY_TO_TIMESTAMP(_ESTUARY_DATA:started::STRING)),
            CALL_DIRECTION VARCHAR(50) AS (_ESTUARY_DATA:direction::VARCHAR),
            CALL_DURATION_SECONDS NUMBER AS (_ESTUARY_DATA:duration::NUMBER),

            -- Enhanced processing tracking
            PROCESSED BOOLEAN DEFAULT FALSE,
            PROCESSED_AT TIMESTAMP_LTZ,
            PROCESSING_ERROR VARCHAR(16777216),
            PROCESSING_EXECUTION_ID VARCHAR(255),

            -- Data quality tracking
            DATA_QUALITY_SCORE FLOAT,
            DATA_QUALITY_ISSUES VARIANT,
            QUALITY_VALIDATED_AT TIMESTAMP_LTZ,

            -- Enhanced metadata
            INGESTED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            CORRELATION_ID VARCHAR(255),
            SOURCE_SYSTEM VARCHAR(100) DEFAULT 'gong',
            INGESTION_BATCH_ID VARCHAR(255)
        )
        COMMENT = 'Enhanced raw Gong calls data from Estuary with quality tracking';
        """
        await self._execute_sql("create_raw_calls", calls_sql)

        # Enhanced RAW_GONG_TRANSCRIPTS_RAW table
        transcripts_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.raw_schema}.RAW_GONG_TRANSCRIPTS_RAW (
            _ESTUARY_AB_ID VARCHAR(64) PRIMARY KEY,
            _ESTUARY_EMITTED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            _ESTUARY_NORMALIZED_AT TIMESTAMP_LTZ,
            _ESTUARY_RAW_GONG_TRANSCRIPTS_HASHID VARCHAR(64),
            _ESTUARY_DATA VARIANT NOT NULL,

            -- Enhanced extraction helper columns
            CALL_ID VARCHAR(255) AS (_ESTUARY_DATA:callId::VARCHAR),
            TRANSCRIPT_LENGTH NUMBER AS (LENGTH(_ESTUARY_DATA:transcript::STRING)),
            SPEAKER_COUNT NUMBER AS (ARRAY_SIZE(_ESTUARY_DATA:speakers)),

            -- Enhanced processing tracking
            PROCESSED BOOLEAN DEFAULT FALSE,
            PROCESSED_AT TIMESTAMP_LTZ,
            PROCESSING_ERROR VARCHAR(16777216),
            PROCESSING_EXECUTION_ID VARCHAR(255),

            -- Data quality tracking
            DATA_QUALITY_SCORE FLOAT,
            DATA_QUALITY_ISSUES VARIANT,
            QUALITY_VALIDATED_AT TIMESTAMP_LTZ,

            -- Enhanced metadata
            INGESTED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            CORRELATION_ID VARCHAR(255),
            SOURCE_SYSTEM VARCHAR(100) DEFAULT 'gong',
            INGESTION_BATCH_ID VARCHAR(255)
        )
        COMMENT = 'Enhanced raw Gong transcripts data from Estuary with quality tracking';
        """
        await self._execute_sql("create_raw_transcripts", transcripts_sql)

    async def _deploy_stg_transformed_tables(self) -> None:
        """Deploy STG_TRANSFORMED tables with AI Memory integration"""

        # Enhanced STG_GONG_CALLS table
        stg_calls_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.stg_schema}.STG_GONG_CALLS (
            CALL_ID VARCHAR(255) PRIMARY KEY,
            CALL_TITLE VARCHAR(500),
            CALL_DATETIME_UTC TIMESTAMP_LTZ,
            CALL_DURATION_SECONDS NUMBER,
            CALL_DIRECTION VARCHAR(50),
            CALL_SYSTEM VARCHAR(100),
            CALL_SCOPE VARCHAR(100),
            CALL_MEDIA VARCHAR(50),
            CALL_LANGUAGE VARCHAR(10),
            CALL_URL VARCHAR(1000),

            -- Enhanced primary user/owner information
            PRIMARY_USER_ID VARCHAR(255),
            PRIMARY_USER_EMAIL VARCHAR(255),
            PRIMARY_USER_NAME VARCHAR(255),
            PRIMARY_USER_DEPARTMENT VARCHAR(255),
            PRIMARY_USER_ROLE VARCHAR(255),

            -- Enhanced CRM Integration fields
            HUBSPOT_DEAL_ID VARCHAR(255),
            HUBSPOT_CONTACT_ID VARCHAR(255),
            HUBSPOT_COMPANY_ID VARCHAR(255),
            CRM_OPPORTUNITY_ID VARCHAR(255),
            CRM_ACCOUNT_ID VARCHAR(255),
            SALESFORCE_OPPORTUNITY_ID VARCHAR(255),

            -- Enhanced business context
            DEAL_STAGE VARCHAR(100),
            DEAL_VALUE NUMBER(15,2),
            DEAL_PROBABILITY FLOAT,
            ACCOUNT_NAME VARCHAR(500),
            ACCOUNT_TIER VARCHAR(50),
            CONTACT_NAME VARCHAR(500),
            CONTACT_ROLE VARCHAR(255),

            -- Enhanced call quality metrics
            TALK_RATIO FLOAT,
            LONGEST_MONOLOGUE_SECONDS NUMBER,
            INTERACTIVITY_SCORE FLOAT,
            QUESTIONS_ASKED_COUNT NUMBER,
            INTERRUPTIONS_COUNT NUMBER,
            SPEAKING_PACE_WPM NUMBER,

            -- Enhanced AI-generated insights (Cortex)
            SENTIMENT_SCORE FLOAT,
            SENTIMENT_LABEL VARCHAR(20),
            CALL_SUMMARY VARCHAR(16777216),
            KEY_TOPICS VARIANT,
            RISK_INDICATORS VARIANT,
            NEXT_STEPS VARIANT,
            COACHING_INSIGHTS VARIANT,
            COMPETITIVE_MENTIONS VARIANT,

            -- Enhanced AI Memory columns for semantic search
            AI_MEMORY_EMBEDDING VECTOR(FLOAT, 768),
            AI_MEMORY_METADATA VARCHAR(16777216),
            AI_MEMORY_UPDATED_AT TIMESTAMP_NTZ,
            AI_MEMORY_MODEL_VERSION VARCHAR(50),

            -- Enhanced data quality and processing metadata
            DATA_QUALITY_SCORE FLOAT,
            DATA_QUALITY_ISSUES VARIANT,
            PROCESSING_EXECUTION_ID VARCHAR(255),
            TRANSFORMATION_VERSION VARCHAR(50),

            -- Enhanced lifecycle metadata
            CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            UPDATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            PROCESSED_BY_CORTEX BOOLEAN DEFAULT FALSE,
            CORTEX_PROCESSED_AT TIMESTAMP_LTZ,
            LAST_ACCESSED_AT TIMESTAMP_LTZ,
            ACCESS_COUNT NUMBER DEFAULT 0
        )
        COMMENT = 'Enhanced structured Gong calls with comprehensive AI Memory integration and quality tracking';
        """
        await self._execute_sql("create_stg_calls", stg_calls_sql)

        # Enhanced STG_GONG_CALL_TRANSCRIPTS table
        stg_transcripts_sql = f"""
        CREATE TABLE IF NOT EXISTS {self.config.database}.{self.config.stg_schema}.STG_GONG_CALL_TRANSCRIPTS (
            TRANSCRIPT_ID VARCHAR(255) PRIMARY KEY,
            CALL_ID VARCHAR(255) NOT NULL,
            SEGMENT_INDEX NUMBER,

            -- Enhanced speaker information
            SPEAKER_ID VARCHAR(255),
            SPEAKER_NAME VARCHAR(255),
            SPEAKER_EMAIL VARCHAR(255),
            SPEAKER_TYPE VARCHAR(50), -- internal, external, customer, prospect
            SPEAKER_ROLE VARCHAR(255),
            SPEAKER_COMPANY VARCHAR(255),

            -- Enhanced transcript content
            TRANSCRIPT_TEXT VARCHAR(16777216),
            START_TIME_SECONDS NUMBER,
            END_TIME_SECONDS NUMBER,
            SEGMENT_DURATION_SECONDS NUMBER,
            WORD_COUNT NUMBER,
            CHARACTER_COUNT NUMBER,

            -- Enhanced linguistic analysis
            LANGUAGE_DETECTED VARCHAR(10),
            CONFIDENCE_SCORE FLOAT,
            SPEAKING_RATE_WPM NUMBER,
            PAUSE_COUNT NUMBER,
            FILLER_WORDS_COUNT NUMBER,

            -- Enhanced AI processing results (Cortex)
            SEGMENT_SENTIMENT FLOAT,
            SEGMENT_SENTIMENT_LABEL VARCHAR(20),
            SEGMENT_SUMMARY VARCHAR(4000),
            EXTRACTED_ENTITIES VARIANT,
            KEY_PHRASES VARIANT,
            INTENT_CLASSIFICATION VARCHAR(100),
            EMOTION_ANALYSIS VARIANT,

            -- Enhanced AI Memory columns for semantic search
            AI_MEMORY_EMBEDDING VECTOR(FLOAT, 768),
            AI_MEMORY_METADATA VARCHAR(16777216),
            AI_MEMORY_UPDATED_AT TIMESTAMP_NTZ,
            AI_MEMORY_MODEL_VERSION VARCHAR(50),

            -- Enhanced data quality and processing metadata
            DATA_QUALITY_SCORE FLOAT,
            DATA_QUALITY_ISSUES VARIANT,
            PROCESSING_EXECUTION_ID VARCHAR(255),

            -- Enhanced lifecycle metadata
            CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
            PROCESSED_BY_CORTEX BOOLEAN DEFAULT FALSE,
            CORTEX_PROCESSED_AT TIMESTAMP_LTZ,

            -- Enhanced foreign key constraint
            FOREIGN KEY (CALL_ID) REFERENCES {self.config.database}.{self.config.stg_schema}.STG_GONG_CALLS(CALL_ID)
        )
        COMMENT = 'Enhanced structured Gong call transcripts with comprehensive AI processing and quality tracking';
        """
        await self._execute_sql("create_stg_transcripts", stg_transcripts_sql)

    async def _deploy_transformation_procedures(self) -> None:
        """Deploy enhanced transformation procedures"""

        # Load and execute the enhanced transformation procedures
        procedures_file = "backend/etl/snowflake/gong_transformation_procedures.sql"

        try:
            with open(procedures_file) as f:
                procedures_sql = f.read()

            # Replace database placeholder if needed
            procedures_sql = procedures_sql.replace(
                "SOPHIA_AI_DEV", self.config.database
            )

            # Execute the procedures
            await self._execute_sql("deploy_transformation_procedures", procedures_sql)

        except FileNotFoundError:
            logger.warning(f"Procedures file not found: {procedures_file}")
            # Fallback to basic procedure creation
            await self._create_basic_transformation_procedures()

    async def _create_basic_transformation_procedures(self) -> None:
        """Create basic transformation procedures as fallback"""

        basic_transform_proc = f"""
        CREATE OR REPLACE PROCEDURE {self.config.database}.{self.config.stg_schema}.TRANSFORM_GONG_CALLS_ENHANCED()
        RETURNS STRING
        LANGUAGE SQL
        AS
        $$
        BEGIN
            MERGE INTO {self.config.database}.{self.config.stg_schema}.STG_GONG_CALLS AS target
            USING (
                SELECT
                    _ESTUARY_DATA:id::VARCHAR AS CALL_ID,
                    _ESTUARY_DATA:title::VARCHAR AS CALL_TITLE,
                    _ESTUARY_DATA:started::TIMESTAMP_LTZ AS CALL_DATETIME_UTC,
                    _ESTUARY_DATA:duration::NUMBER AS CALL_DURATION_SECONDS,
                    _ESTUARY_DATA:direction::VARCHAR AS CALL_DIRECTION,
                    CURRENT_TIMESTAMP() AS UPDATED_AT
                FROM {self.config.database}.{self.config.raw_schema}.RAW_GONG_CALLS_RAW
                WHERE PROCESSED = FALSE
            ) AS source
            ON target.CALL_ID = source.CALL_ID
            WHEN MATCHED THEN UPDATE SET
                CALL_TITLE = source.CALL_TITLE,
                UPDATED_AT = source.UPDATED_AT
            WHEN NOT MATCHED THEN INSERT (
                CALL_ID, CALL_TITLE, CALL_DATETIME_UTC, CALL_DURATION_SECONDS,
                CALL_DIRECTION, UPDATED_AT
            ) VALUES (
                source.CALL_ID, source.CALL_TITLE, source.CALL_DATETIME_UTC,
                source.CALL_DURATION_SECONDS, source.CALL_DIRECTION, source.UPDATED_AT
            );

            UPDATE {self.config.database}.{self.config.raw_schema}.RAW_GONG_CALLS_RAW
            SET PROCESSED = TRUE, PROCESSED_AT = CURRENT_TIMESTAMP()
            WHERE PROCESSED = FALSE;

            RETURN 'Basic Gong calls transformation completed';
        END;
        $$;
        """
        await self._execute_sql("create_basic_transform_proc", basic_transform_proc)

    async def _deploy_automated_tasks(self) -> None:
        """Deploy automated tasks for data processing"""

        # Task for hourly transformation
        transform_task_sql = f"""
        CREATE OR REPLACE TASK {self.config.database}.{self.config.stg_schema}.TASK_TRANSFORM_GONG_CALLS
        WAREHOUSE = {self.config.warehouse}
        SCHEDULE = 'USING CRON 0 * * * * UTC'
        COMMENT = 'Hourly transformation of Gong calls data'
        AS
        CALL {self.config.database}.{self.config.stg_schema}.TRANSFORM_GONG_CALLS_ENHANCED();
        """
        await self._execute_sql("create_transform_task", transform_task_sql)

        # Resume the task
        resume_task_sql = f"""
        ALTER TASK {self.config.database}.{self.config.stg_schema}.TASK_TRANSFORM_GONG_CALLS RESUME;
        """
        await self._execute_sql("resume_transform_task", resume_task_sql)

    async def _deploy_permissions(self) -> None:
        """Deploy role-based permissions"""

        # Grant permissions to Estuary role
        estuary_permissions = f"""
        GRANT USAGE ON DATABASE {self.config.database} TO ROLE ROLE_SOPHIA_ESTUARY_INGEST;
        GRANT USAGE ON SCHEMA {self.config.database}.{self.config.raw_schema} TO ROLE ROLE_SOPHIA_ESTUARY_INGEST;
        GRANT INSERT, SELECT, UPDATE ON ALL TABLES IN SCHEMA {self.config.database}.{self.config.raw_schema} TO ROLE ROLE_SOPHIA_ESTUARY_INGEST;
        """
        await self._execute_sql("grant_estuary_permissions", estuary_permissions)

    async def _validate_deployment(self) -> None:
        """Validate the deployment"""

        if not self.config.validation_enabled:
            logger.info("‚è≠Ô∏è Validation disabled, skipping validation phase")
            return

        # Validate schemas exist
        schema_validation = f"""
        SELECT schema_name
        FROM {self.config.database}.INFORMATION_SCHEMA.SCHEMATA
        WHERE schema_name IN ('{self.config.raw_schema}', '{self.config.stg_schema}', '{self.config.ops_schema}', '{self.config.ai_memory_schema}')
        """

        result = await self._execute_query(schema_validation)
        expected_schemas = {
            self.config.raw_schema,
            self.config.stg_schema,
            self.config.ops_schema,
            self.config.ai_memory_schema,
        }
        found_schemas = {row[0] for row in result}

        if not expected_schemas.issubset(found_schemas):
            missing_schemas = expected_schemas - found_schemas
            raise Exception(f"Missing schemas: {missing_schemas}")

        logger.info("‚úÖ Schema validation passed")

        # Validate key tables exist
        table_validation = f"""
        SELECT table_name
        FROM {self.config.database}.INFORMATION_SCHEMA.TABLES
        WHERE table_schema = '{self.config.stg_schema}'
        AND table_name IN ('STG_GONG_CALLS', 'STG_GONG_CALL_TRANSCRIPTS')
        """

        result = await self._execute_query(table_validation)
        expected_tables = {"STG_GONG_CALLS", "STG_GONG_CALL_TRANSCRIPTS"}
        found_tables = {row[0] for row in result}

        if not expected_tables.issubset(found_tables):
            missing_tables = expected_tables - found_tables
            raise Exception(f"Missing tables: {missing_tables}")

        logger.info("‚úÖ Table validation passed")

    async def _rollback_deployment(self) -> None:
        """Rollback deployment to previous state"""

        if not self.config.rollback_enabled:
            logger.warning("‚ö†Ô∏è Rollback disabled, manual cleanup required")
            return

        logger.info("üîÑ Initiating deployment rollback...")

        # Rollback in reverse order of completed phases
        for phase in reversed(self.completed_phases):
            try:
                await self._rollback_phase(phase)
                logger.info(f"‚úÖ Rolled back phase: {phase.value}")
            except Exception as e:
                logger.error(f"‚ùå Failed to rollback phase {phase.value}: {e}")

    async def _rollback_phase(self, phase: DeploymentPhase) -> None:
        """Rollback a specific deployment phase"""

        if phase == DeploymentPhase.TASKS:
            # Suspend tasks
            suspend_sql = f"""
            ALTER TASK IF EXISTS {self.config.database}.{self.config.stg_schema}.TASK_TRANSFORM_GONG_CALLS SUSPEND;
            """
            await self._execute_sql(f"rollback_{phase.value}", suspend_sql)

        # Add more rollback logic for other phases as needed

    async def _execute_sql(self, operation_name: str, sql: str) -> None:
        """Execute SQL with enhanced error handling and logging"""

        if self.config.dry_run:
            logger.info(f"[DRY RUN] {operation_name}: {sql[:100]}...")
            return

        try:
            cursor = self.connection.cursor()
            cursor.execute(sql)

            self.deployment_log.append(
                {
                    "operation": operation_name,
                    "status": "SUCCESS",
                    "timestamp": datetime.now(UTC).isoformat(),
                    "sql_preview": sql[:200] + "..." if len(sql) > 200 else sql,
                }
            )

            logger.info(f"‚úÖ {operation_name} completed successfully")

        except Exception as e:
            self.deployment_log.append(
                {
                    "operation": operation_name,
                    "status": "FAILED",
                    "error": str(e),
                    "timestamp": datetime.now(UTC).isoformat(),
                    "sql_preview": sql[:200] + "..." if len(sql) > 200 else sql,
                }
            )

            logger.error(f"‚ùå {operation_name} failed: {e}")
            raise
        finally:
            if cursor:
                cursor.close()

    async def _execute_query(self, sql: str) -> list[tuple]:
        """Execute query and return results"""
        cursor = self.connection.cursor()
        try:
            cursor.execute(sql)
            return cursor.fetchall()
        finally:
            cursor.close()

    async def _log_phase_completion(
        self, phase: DeploymentPhase, status: str, error: str = None
    ) -> None:
        """Log phase completion"""

        log_entry = {
            "deployment_id": self.deployment_id,
            "phase": phase.value,
            "status": status,
            "timestamp": datetime.now(UTC).isoformat(),
            "environment": self.config.environment.value,
        }

        if error:
            log_entry["error"] = error

        self.deployment_log.append(log_entry)

    async def _cleanup_connections(self) -> None:
        """Clean up database connections"""
        try:
            if self.connection:
                self.connection.close()
            if self.cortex_service:
                await self.cortex_service.close()
            logger.info("‚úÖ Database connections cleaned up")
        except Exception as e:
            logger.warning(f"Warning during cleanup: {e}")


async def main():
    """Main function for CLI usage"""
    parser = argparse.ArgumentParser(
        description="Enhanced Gong Snowflake Setup Deployment"
    )
    parser.add_argument(
        "--env",
        choices=["dev", "staging", "prod"],
        default="dev",
        help="Deployment environment",
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="Perform dry run without making changes"
    )
    parser.add_argument(
        "--force-deploy",
        action="store_true",
        help="Force deployment even if validation fails",
    )
    parser.add_argument(
        "--disable-rollback",
        action="store_true",
        help="Disable automatic rollback on failure",
    )
    parser.add_argument(
        "--disable-validation",
        action="store_true",
        help="Disable deployment validation",
    )
    parser.add_argument(
        "--output",
        default="deployment_results.json",
        help="Output file for deployment results",
    )

    args = parser.parse_args()

    # Create deployment configuration
    config = DeploymentConfig(
        environment=DeploymentEnvironment(args.env),
        database=f"SOPHIA_AI_{args.env.upper()}",
        warehouse=(
            "WH_SOPHIA_ETL_TRANSFORM"
            if args.env == "dev"
            else f"WH_SOPHIA_{args.env.upper()}"
        ),
        role=(
            "ROLE_SOPHIA_ESTUARY_INGEST"
            if args.env == "dev"
            else f"ROLE_SOPHIA_{args.env.upper()}"
        ),
        dry_run=args.dry_run,
        force_deploy=args.force_deploy,
        rollback_enabled=not args.disable_rollback,
        validation_enabled=not args.disable_validation,
    )

    # Initialize and run deployment
    deployer = EnhancedGongSnowflakeDeployer(config)

    try:
        results = await deployer.deploy_complete_infrastructure()

        # Save results to file
        with open(args.output, "w") as f:
            json.dump(results, f, indent=2)

        # Print summary
        print(f"\n{'=' * 60}")
        print("Enhanced Gong Snowflake Deployment Results")
        print(f"{'=' * 60}")
        print(f"Deployment ID: {results['deployment_id']}")
        print(f"Environment: {results.get('environment', 'unknown')}")
        print(f"Status: {results['status']}")

        if results["status"] == "SUCCESS":
            print("‚úÖ Deployment completed successfully!")
            print(f"Total time: {results.get('total_time_seconds', 0):.2f} seconds")
            print(f"Completed phases: {len(results.get('completed_phases', []))}")
        else:
            print(f"‚ùå Deployment failed: {results.get('error', 'Unknown error')}")
            print(f"Completed phases: {results.get('completed_phases', [])}")

        print(f"\nDetailed results saved to: {args.output}")

        return 0 if results["status"] == "SUCCESS" else 1

    except Exception as e:
        print(f"‚ùå Deployment execution failed: {e}")
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))
