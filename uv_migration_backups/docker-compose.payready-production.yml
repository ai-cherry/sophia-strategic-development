# Pay Ready AI Business Intelligence Production Stack
# Optimized for comprehensive competitive intelligence and sales acceleration

version: '3.8'

services:
  # Core Pay Ready Business Intelligence Stack
  enhanced-backend:
    extends:
      file: docker-compose.production.yml
      service: enhanced-backend
    environment:
      - PAYREADY_MODE=business_intelligence
      - BUZZ_INTEGRATION=enabled
      - COMPETITIVE_INTEL=enabled
      - NMHC_TARGETING=enabled
      - ROI_VALIDATION=enabled
    volumes:
      - ./backend/services/payready_business_intelligence.py:/app/backend/services/payready_business_intelligence.py
    networks:
      - payready-bi-network
    deploy:
      resources:
        limits:
          memory: 4GB
          cpus: '2.0'

  # Competitive Intelligence MCP Servers
  apollo-mcp:
    build: 
      context: ./mcp-servers/apollo_io/
      dockerfile: Dockerfile
    container_name: payready-apollo-mcp
    ports: 
      - "3010:3000"
    environment:
      - APOLLO_API_KEY=${APOLLO_API_KEY}
      - PAYREADY_INTEGRATION=true
      - TARGET_MARKET=NMHC_TOP_50
      - ENRICHMENT_LEVEL=comprehensive
    volumes:
      - apollo-data:/app/data
    networks:
      - payready-bi-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2GB
          cpus: '1.0'
  
  linkedin-intelligence-mcp:
    build: 
      context: ./mcp-servers/linkedin_intel/
      dockerfile: Dockerfile
    container_name: payready-linkedin-mcp
    ports: 
      - "3011:3000"
    environment:
      - LINKEDIN_API_KEY=${LINKEDIN_API_KEY}
      - COMPLIANCE_MODE=strict
      - GDPR_COMPLIANT=true
      - CONTACT_ENRICHMENT=true
    volumes:
      - linkedin-data:/app/data
    networks:
      - payready-bi-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2GB
          cpus: '1.0'
      
  competitive-monitor-mcp:
    build: 
      context: ./mcp-servers/competitive_monitor/
      dockerfile: Dockerfile
    container_name: payready-competitive-monitor
    ports: 
      - "3012:3000"
    environment:
      - ELISE_AI_MONITOR=enabled
      - HUNTER_WARFIELD_MONITOR=enabled
      - COLLECTTECH_MONITOR=enabled
      - ENTRATA_MONITOR=enabled
      - YARDI_MONITOR=enabled
      - MONITORING_FREQUENCY=300  # 5 minutes
      - ALERT_CHANNEL=#competitive-intelligence
    volumes:
      - competitive-data:/app/data
    networks:
      - payready-bi-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2GB
          cpus: '1.0'

  # NMHC Top 50 Targeting MCP
  nmhc-targeting-mcp:
    build: 
      context: ./mcp-servers/nmhc_targeting/
      dockerfile: Dockerfile
    container_name: payready-nmhc-targeting
    ports: 
      - "3013:3000"
    environment:
      - COSTAR_API_KEY=${COSTAR_API_KEY}
      - APOLLO_API_KEY=${APOLLO_API_KEY}
      - TARGET_LIST=NMHC_TOP_50
      - ENRICHMENT_DEPTH=maximum
      - COMPETITIVE_ANALYSIS=enabled
    volumes:
      - nmhc-data:/app/data
    depends_on:
      - apollo-mcp
      - competitive-monitor-mcp
    networks:
      - payready-bi-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4GB
          cpus: '2.0'

  # Buzz ROI Validation MCP
  buzz-roi-validation-mcp:
    build: 
      context: ./mcp-servers/buzz_roi_validation/
      dockerfile: Dockerfile
    container_name: payready-buzz-roi
    ports: 
      - "3014:3000"
    environment:
      - SNOWFLAKE_CONNECTION=${SNOWFLAKE_CONNECTION}
      - BUZZ_API_KEY=${BUZZ_API_KEY}
      - ROI_CALCULATION_MODE=comprehensive
      - REPORT_GENERATION=enabled
    volumes:
      - roi-data:/app/data
      - roi-reports:/app/reports
    networks:
      - payready-bi-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2GB
          cpus: '1.0'

  # Enhanced Weaviate for Competitive Intelligence
  weaviate-competitive:
    extends:
      file: docker-compose.production.yml
      service: weaviate
    environment:
      - PERSISTENCE_DATA_PATH=/var/lib/weaviate
      - DEFAULT_VECTORIZER_MODULE=text2vec-openai
      - ENABLE_MODULES=text2vec-openai,generative-openai
      - CLUSTER_HOSTNAME=weaviate-competitive
    volumes:
      - weaviate-competitive-data:/var/lib/weaviate
    networks:
      - payready-bi-network
    deploy:
      resources:
        limits:
          memory: 8GB
          cpus: '2.0'

  # Pay Ready BI Monitoring Dashboard
  payready-bi-dashboard:
    build:
      context: ./monitoring/payready_dashboard/
      dockerfile: Dockerfile
    container_name: payready-bi-monitoring
    ports:
      - "3015:8501"  # Streamlit dashboard
    environment:
      - MONITORING_MODE=business_intelligence
      - ALERT_ENABLED=true
      - SLACK_WEBHOOK=${SLACK_WEBHOOK_URL}
    volumes:
      - monitoring-data:/app/data
    networks:
      - payready-bi-network
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1GB
          cpus: '0.5'

volumes:
  linkedin-data:
  competitive-data:
  roi-data:
  roi-reports:
  weaviate-competitive-data:
  monitoring-data:

networks:
  payready-bi-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
