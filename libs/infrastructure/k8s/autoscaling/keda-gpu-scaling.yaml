apiVersion: v1
kind: ConfigMap
metadata:
  name: keda-gpu-scaling-config
  namespace: sophia-ai
data:
  gpu_threshold_high: "80"
  gpu_threshold_low: "20"
  memory_threshold_high: "75"
  memory_threshold_low: "25"
  embedding_queue_threshold: "50"
  response_time_threshold: "200"
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: unified-memory-service-gpu-scaler
  namespace: sophia-ai
spec:
  scaleTargetRef:
    name: unified-memory-service
  minReplicaCount: 1
  maxReplicaCount: 10
  pollingInterval: 15  # Check every 15 seconds for GPU workloads
  cooldownPeriod: 60   # Wait 60s before scaling down
  triggers:
  # GPU utilization trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: gpu_utilization_high
      threshold: "80"
      query: avg(nvidia_gpu_utilization_percent{job="lambda-labs-gpu-exporter"})
  # GPU memory pressure trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: gpu_memory_pressure
      threshold: "75"
      query: avg(nvidia_gpu_memory_used_percent{job="lambda-labs-gpu-exporter"})
  # Embedding queue length trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: embedding_queue_length
      threshold: "50"
      query: sum(embedding_queue_length{service="unified-memory-service"})
  # Response time degradation trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: response_time_p95
      threshold: "200"
      query: histogram_quantile(0.95, rate(embedding_latency_ms_bucket[5m]))
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: unified-project-mcp-scaler
  namespace: sophia-ai
spec:
  scaleTargetRef:
    name: unified-project-mcp
  minReplicaCount: 1
  maxReplicaCount: 5
  pollingInterval: 30
  cooldownPeriod: 120
  triggers:
  # Request rate trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: mcp_request_rate
      threshold: "10"
      query: sum(rate(mcp_route_requests_total[5m]))
  # Response time trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: mcp_response_time_p95
      threshold: "500"
      query: histogram_quantile(0.95, rate(mcp_route_latency_seconds_bucket[5m])) * 1000
  # Service health trigger (scale up if unhealthy services)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: unhealthy_services_count
      threshold: "2"
      query: sum(mcp_service_health == 0)
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: qdrant-gpu-scaler
  namespace: sophia-ai
spec:
  scaleTargetRef:
    name: qdrant
  minReplicaCount: 1
  maxReplicaCount: 6
  pollingInterval: 20
  cooldownPeriod: 90
  triggers:
  # Vector search load trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: vector_search_requests_per_second
      threshold: "20"
      query: sum(rate(search_latency_ms_count[5m]))
  # GPU memory for vector operations
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: Qdrant_gpu_memory_usage
      threshold: "70"
      query: avg(nvidia_gpu_memory_used_percent{job="qdrant-gpu-exporter"})
  # Cache miss rate (scale up if cache is ineffective)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: cache_miss_rate
      threshold: "30"
      query: (sum(rate(redis_cache_miss_total[5m])) / sum(rate(redis_cache_requests_total[5m]))) * 100
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: lambda-labs-inference-scaler
  namespace: sophia-ai
spec:
  scaleTargetRef:
    name: lambda-labs-inference
  minReplicaCount: 1
  maxReplicaCount: 8
  pollingInterval: 10  # Very responsive for inference workloads
  cooldownPeriod: 45
  triggers:
  # GPU utilization for inference
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: inference_gpu_utilization
      threshold: "85"
      query: avg(nvidia_gpu_utilization_percent{job="lambda-labs-inference"})
  # Inference request queue
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: inference_queue_depth
      threshold: "25"
      query: sum(inference_queue_length{service="lambda-labs-inference"})
  # Model loading time (scale up if cold starts)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: model_loading_time
      threshold: "2000"
      query: histogram_quantile(0.95, rate(model_loading_time_ms_bucket[5m]))
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: redis-cache-scaler
  namespace: sophia-ai
spec:
  scaleTargetRef:
    name: redis-cache
  minReplicaCount: 1
  maxReplicaCount: 4
  pollingInterval: 60
  cooldownPeriod: 180
  triggers:
  # Memory usage trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: redis_memory_usage
      threshold: "80"
      query: avg(redis_memory_used_percent{service="redis-cache"})
  # Connection count trigger
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: redis_connection_count
      threshold: "1000"
      query: sum(redis_connected_clients{service="redis-cache"})
  # Cache hit rate degradation (scale up if performance drops)
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: cache_hit_rate_degradation
      threshold: "70"
      query: (sum(rate(redis_cache_hit_total[5m])) / sum(rate(redis_cache_requests_total[5m]))) * 100
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: mcp-gateway-scaler
  namespace: sophia-ai
spec:
  scaleTargetRef:
    name: mcp-gateway
  minReplicaCount: 2
  maxReplicaCount: 12
  pollingInterval: 15
  cooldownPeriod: 60
  triggers:
  # Overall MCP request rate
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: mcp_gateway_request_rate
      threshold: "50"
      query: sum(rate(mcp_requests_total[5m]))
  # Response time degradation
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: mcp_gateway_response_time_p99
      threshold: "1000"
      query: histogram_quantile(0.99, rate(mcp_response_time_bucket[5m])) * 1000
  # Error rate increase
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: mcp_gateway_error_rate
      threshold: "5"
      query: (sum(rate(mcp_requests_total{status=~"5.."}[5m])) / sum(rate(mcp_requests_total[5m]))) * 100
---
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: keda-gpu-scaling-monitor
  namespace: sophia-ai
spec:
  selector:
    matchLabels:
      app: keda-gpu-scaler
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
---
apiVersion: v1
kind: Service
metadata:
  name: gpu-metrics-aggregator
  namespace: sophia-ai
  labels:
    app: gpu-metrics-aggregator
spec:
  selector:
    app: gpu-metrics-aggregator
  ports:
  - port: 9090
    targetPort: 9090
    name: metrics
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-metrics-aggregator
  namespace: sophia-ai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpu-metrics-aggregator
  template:
    metadata:
      labels:
        app: gpu-metrics-aggregator
    spec:
      containers:
      - name: gpu-metrics-aggregator
        image: prom/prometheus:latest
        ports:
        - containerPort: 9090
          name: metrics
        env:
        - name: PROMETHEUS_CONFIG
          value: |
            global:
              scrape_interval: 15s
            scrape_configs:
            - job_name: 'lambda-labs-gpu-exporter'
              static_configs:
              - targets: ['lambda-labs-gpu-exporter:9100']
            - job_name: 'qdrant-gpu-exporter'
              static_configs:
              - targets: ['qdrant-gpu-exporter:9100']
            - job_name: 'unified-memory-service'
              static_configs:
              - targets: ['unified-memory-service:9100']
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
          limits:
            memory: 512Mi
            cpu: 200m
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: keda-scaling-alerts
  namespace: sophia-ai
data:
  alerts.yml: |
    groups:
    - name: keda-gpu-scaling
      rules:
      - alert: GPUUtilizationHigh
        expr: avg(nvidia_gpu_utilization_percent) > 90
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High GPU utilization detected"
          description: "GPU utilization is {{ $value }}% for more than 2 minutes"
      
      - alert: GPUMemoryPressure
        expr: avg(nvidia_gpu_memory_used_percent) > 85
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "GPU memory pressure detected"
          description: "GPU memory usage is {{ $value }}% for more than 1 minute"
      
      - alert: EmbeddingQueueBacklog
        expr: sum(embedding_queue_length) > 100
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Embedding queue backlog detected"
          description: "Embedding queue length is {{ $value }} for more than 30 seconds"
      
      - alert: MCPServiceUnhealthy
        expr: sum(mcp_service_health == 0) > 3
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Multiple MCP services unhealthy"
          description: "{{ $value }} MCP services are unhealthy for more than 5 minutes"
      
      - alert: ResponseTimeDegradation
        expr: histogram_quantile(0.95, rate(embedding_latency_ms_bucket[5m])) > 500
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Response time degradation detected"
          description: "95th percentile response time is {{ $value }}ms for more than 2 minutes" 