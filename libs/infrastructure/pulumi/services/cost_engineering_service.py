"""
Cost Engineering Service - Phase 2 Enhancement

This service implements comprehensive cost optimization strategies for LLM operations:
- Dynamic model routing based on complexity and cost
- Intelligent caching with semantic awareness
- Cost monitoring and reporting
- Token usage optimization
- A/B testing for cost optimization strategies
- Predictive cost modeling

Key Features:
- Multi-tier model routing (small, medium, large models)
- Semantic caching with similarity matching
- Real-time cost tracking and budgeting
- Automated cost optimization recommendations
- Performance vs cost trade-off analysis
- Usage pattern analysis and optimization
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 1035 lines

Recommended decomposition:
- cost_engineering_service_core.py - Core functionality
- cost_engineering_service_utils.py - Utility functions
- cost_engineering_service_models.py - Data models
- cost_engineering_service_handlers.py - Request handlers

"""

import asyncio
import logging
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

from core.enhanced_cache_manager import EnhancedCacheManager
from infrastructure.mcp_servers.enhanced_ai_memory_mcp_server import (
    EnhancedAiMemoryMCPServer,
)
from infrastructure.security.audit_logger import AuditLogger

logger = logging.getLogger(__name__)

class ModelTier(Enum):
    """Model tiers based on capability and cost"""

    SMALL = "small"  # Fast, cheap, simple tasks
    MEDIUM = "medium"  # Balanced performance and cost
    LARGE = "large"  # High capability, expensive
    PREMIUM = "premium"  # Highest capability, most expensive

class TaskComplexity(Enum):
    """Task complexity levels"""

    SIMPLE = "simple"  # Basic Q&A, simple classification
    MODERATE = "moderate"  # Analysis, summarization
    COMPLEX = "complex"  # Multi-step reasoning, complex analysis
    EXPERT = "expert"  # Specialized domain knowledge required

class CostOptimizationStrategy(Enum):
    """Cost optimization strategies"""

    PERFORMANCE_FIRST = "performance_first"  # Prioritize performance over cost
    BALANCED = "balanced"  # Balance performance and cost
    COST_FIRST = "cost_first"  # Minimize cost, acceptable performance
    ADAPTIVE = "adaptive"  # Adapt based on usage patterns

@dataclass
class ModelConfig:
    """Configuration for a specific model"""

    model_id: str
    tier: ModelTier
    cost_per_token: float
    max_tokens: int
    avg_latency_ms: float
    quality_score: float  # 0.0 to 1.0
    capabilities: list[str]
    provider: str = "QDRANT_cortex"

@dataclass
class CostMetrics:
    """Cost tracking metrics"""

    total_tokens: int = 0
    total_cost: float = 0.0
    request_count: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    avg_latency_ms: float = 0.0
    cost_savings: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class TaskRequest:
    """Request for LLM task processing"""

    request_id: str
    user_id: str
    task_type: str
    prompt: str
    context: str | None = None
    max_tokens: int | None = None
    temperature: float = 0.7
    required_quality: float = 0.8  # Minimum quality threshold
    max_cost: float | None = None
    priority: str = "normal"  # low, normal, high
    metadata: dict[str, Any] = field(default_factory=dict)

@dataclass
class TaskResponse:
    """Response from LLM task processing"""

    request_id: str
    response_text: str
    model_used: str
    tokens_used: int
    cost: float
    latency_ms: float
    quality_score: float
    cache_hit: bool
    optimization_applied: list[str]
    metadata: dict[str, Any] = field(default_factory=dict)

class CostEngineeringService:
    """
    Comprehensive cost engineering service for LLM operations

    Implements intelligent model routing, caching, and cost optimization
    to minimize expenses while maintaining quality and performance.
    """

    def __init__(self):
        self.cortex_service: QdrantUnifiedMemoryService | None = None
        self.cache_manager = EnhancedCacheManager()
        self.audit_logger = AuditLogger()
        self.ai_memory: EnhancedAiMemoryMCPServer | None = None

        # Model configurations
        self.model_configs: dict[str, ModelConfig] = {}
        self.model_routing_rules: dict[TaskComplexity, list[str]] = {}

        # Cost tracking
        self.cost_metrics: dict[str, CostMetrics] = {}  # Per user/session
        self.global_metrics = CostMetrics()

        # Optimization settings
        self.optimization_strategy = CostOptimizationStrategy.BALANCED
        self.cost_budget_daily: float | None = None
        self.cost_budget_monthly: float | None = None

        # Caching settings
        self.semantic_cache_threshold = 0.85  # Similarity threshold for cache hits
        self.cache_ttl_hours = 24

        # A/B testing
        self.ab_test_groups: dict[str, str] = {}  # user_id -> group
        self.ab_test_results: dict[str, list[dict[str, Any]]] = {}

        self.initialized = False

    async def initialize(self) -> None:
        """Initialize the cost engineering service"""
        if self.initialized:
            return

        try:
            # Initialize services
            self.cortex_service = SophiaUnifiedMemoryService()
            self.ai_memory = EnhancedAiMemoryMCPServer()
            await self.ai_memory.initialize()

            # Initialize model configurations
            await self._initialize_model_configs()

            # Initialize routing rules
            self._initialize_routing_rules()

            # Start background tasks
            asyncio.create_task(self._cost_monitoring_task())
            asyncio.create_task(self._optimization_analysis_task())

            self.initialized = True
            logger.info("âœ… Cost Engineering Service initialized")

        except Exception as e:
            logger.exception(f"Failed to initialize Cost Engineering Service: {e}")
            raise

    async def _initialize_model_configs(self) -> None:
        """Initialize model configurations with cost and performance data"""

        self.model_configs = {
            "mistral-7b": ModelConfig(
                model_id="mistral-7b",
                tier=ModelTier.SMALL,
                cost_per_token=0.0001,
                max_tokens=8192,
                avg_latency_ms=500,
                quality_score=0.75,
                capabilities=["text_generation", "simple_qa", "classification"],
                provider="QDRANT_cortex",
            ),
            "mixtral-8x7b": ModelConfig(
                model_id="mixtral-8x7b",
                tier=ModelTier.MEDIUM,
                cost_per_token=0.0003,
                max_tokens=32768,
                avg_latency_ms=800,
                quality_score=0.85,
                capabilities=[
                    "text_generation",
                    "analysis",
                    "reasoning",
                    "summarization",
                ],
                provider="QDRANT_cortex",
            ),
            "llama2-70b-chat": ModelConfig(
                model_id="llama2-70b-chat",
                tier=ModelTier.LARGE,
                cost_per_token=0.0008,
                max_tokens=4096,
                avg_latency_ms=1200,
                quality_score=0.92,
                capabilities=[
                    "complex_reasoning",
                    "expert_analysis",
                    "creative_writing",
                ],
                provider="QDRANT_cortex",
            ),

                model_id="qdrant-arctic",
                tier=ModelTier.PREMIUM,
                cost_per_token=0.0015,
                max_tokens=4096,
                avg_latency_ms=1500,
                quality_score=0.95,
                capabilities=["expert_reasoning", "specialized_analysis", "research"],
                provider="QDRANT_cortex",
            ),
        }

    def _initialize_routing_rules(self) -> None:
        """Initialize model routing rules based on task complexity"""
        self.model_routing_rules = {
            TaskComplexity.SIMPLE: ["mistral-7b", "mixtral-8x7b"],
            TaskComplexity.MODERATE: ["mixtral-8x7b", "llama2-70b-chat"],
            TaskComplexity.COMPLEX: ["llama2-70b-chat", "qdrant-arctic"],
            TaskComplexity.EXPERT: ["qdrant-arctic", "llama2-70b-chat"],
        }

    async def process_task(self, task_request: TaskRequest) -> TaskResponse:
        """
        Process a task with cost optimization

        Args:
            task_request: Task request with requirements

        Returns:
            Task response with cost and performance metrics
        """
        if not self.initialized:
            await self.initialize()

        start_time = datetime.now()
        optimization_applied = []

        try:
            # Check cache first
            cache_result = await self._check_semantic_cache(task_request)
            if cache_result:
                optimization_applied.append("cache_hit")

                # Update metrics
                await self._update_metrics(
                    task_request.user_id,
                    tokens_used=0,
                    cost=0.0,
                    latency_ms=(datetime.now() - start_time).total_seconds() * 1000,
                    cache_hit=True,
                )

                return TaskResponse(
                    request_id=task_request.request_id,
                    response_text=cache_result["response"],
                    model_used=cache_result["model_used"],
                    tokens_used=0,
                    cost=0.0,
                    latency_ms=(datetime.now() - start_time).total_seconds() * 1000,
                    quality_score=cache_result.get("quality_score", 0.9),
                    cache_hit=True,
                    optimization_applied=optimization_applied,
                    metadata=cache_result.get("metadata", {}),
                )

            # Analyze task complexity
            complexity = await self._analyze_task_complexity(task_request)
            optimization_applied.append(f"complexity_analysis_{complexity.value}")

            # Select optimal model
            selected_model = await self._select_optimal_model(task_request, complexity)
            optimization_applied.append(f"model_selection_{selected_model}")

            # Optimize prompt for cost efficiency
            optimized_prompt = await self._optimize_prompt(
                task_request.prompt, selected_model
            )
            if optimized_prompt != task_request.prompt:
                optimization_applied.append("prompt_optimization")

            # Execute task
            response_text, tokens_used = await self._execute_task(
                prompt=optimized_prompt,
                model_id=selected_model,
                max_tokens=task_request.max_tokens,
                temperature=task_request.temperature,
            )

            # Calculate cost
            model_config = self.model_configs[selected_model]
            cost = tokens_used * model_config.cost_per_token

            # Calculate quality score
            quality_score = await self._assess_response_quality(
                task_request.prompt, response_text, task_request.required_quality
            )

            # Cache the result
            await self._cache_result(
                task_request, response_text, selected_model, quality_score
            )
            optimization_applied.append("result_cached")

            # Update metrics
            latency_ms = (datetime.now() - start_time).total_seconds() * 1000
            await self._update_metrics(
                task_request.user_id,
                tokens_used=tokens_used,
                cost=cost,
                latency_ms=latency_ms,
                cache_hit=False,
            )

            # Log the task
            await self.audit_logger.log_llm_operation(
                user_id=task_request.user_id,
                operation_type="cost_optimized_task",
                model_used=selected_model,
                tokens_used=tokens_used,
                cost=cost,
                details={
                    "task_type": task_request.task_type,
                    "complexity": complexity.value,
                    "optimization_applied": optimization_applied,
                    "quality_score": quality_score,
                },
            )

            return TaskResponse(
                request_id=task_request.request_id,
                response_text=response_text,
                model_used=selected_model,
                tokens_used=tokens_used,
                cost=cost,
                latency_ms=latency_ms,
                quality_score=quality_score,
                cache_hit=False,
                optimization_applied=optimization_applied,
                metadata={
                    "complexity": complexity.value,
                    "model_tier": model_config.tier.value,
                },
            )

        except Exception as e:
            logger.exception(f"Error processing task: {e}")
            raise

    async def _check_semantic_cache(
        self, task_request: TaskRequest
    ) -> dict[str, Any] | None:
        """Check semantic cache for similar requests"""
        try:
            # Create cache key based on task content
            cache_key = f"semantic_{hash(task_request.prompt + task_request.task_type)}"

            # Check exact match first
            exact_result = await self.cache_manager.get(cache_key)
            if exact_result:
                return exact_result

            # Check semantic similarity using AI Memory
            similar_results = await self.ai_memory.search_similar_content(
                query=task_request.prompt,
                content_type="llm_response",
                similarity_threshold=self.semantic_cache_threshold,
                limit=1,
            )

            if similar_results:
                return similar_results[0]

            return None

        except Exception as e:
            logger.exception(f"Error checking semantic cache: {e}")
            return None

    async def _analyze_task_complexity(
        self, task_request: TaskRequest
    ) -> TaskComplexity:
        """Analyze task complexity to determine appropriate model tier"""
        try:
            # Use a simple model to analyze complexity
            async with self.cortex_service as cortex:
                complexity_prompt = f"""
                Analyze the complexity of this task and classify it:

                Task Type: {task_request.task_type}
                Prompt: {task_request.prompt[:500]}...

                Complexity Levels:
                - SIMPLE: Basic Q&A, simple classification, straightforward tasks
                - MODERATE: Analysis, summarization, multi-step tasks
                - COMPLEX: Complex reasoning, detailed analysis, creative tasks
                - EXPERT: Specialized domain knowledge, research, expert-level analysis

                Return only the complexity level: SIMPLE, MODERATE, COMPLEX, or EXPERT
                """

                complexity_result = await 
                    prompt=complexity_prompt,
                    max_tokens=10,
                    model="mistral-7b",  # Use small model for analysis
                )

                # Parse result
                complexity_str = complexity_result.strip().upper()
                if complexity_str in ["SIMPLE", "MODERATE", "COMPLEX", "EXPERT"]:
                    return TaskComplexity(complexity_str.lower())

            # Fallback to heuristic analysis
            return self._heuristic_complexity_analysis(task_request)

        except Exception as e:
            logger.exception(f"Error analyzing task complexity: {e}")
            return self._heuristic_complexity_analysis(task_request)

    def _heuristic_complexity_analysis(
        self, task_request: TaskRequest
    ) -> TaskComplexity:
        """Fallback heuristic complexity analysis"""
        prompt_length = len(task_request.prompt)

        # Simple heuristics based on prompt characteristics
        if any(
            word in task_request.prompt.lower()
            for word in ["analyze", "compare", "evaluate", "assess"]
        ):
            if prompt_length > 500:
                return TaskComplexity.COMPLEX
            else:
                return TaskComplexity.MODERATE
        elif any(
            word in task_request.prompt.lower()
            for word in ["research", "expert", "specialized", "technical"]
        ):
            return TaskComplexity.EXPERT
        elif prompt_length > 1000:
            return TaskComplexity.COMPLEX
        elif prompt_length > 200:
            return TaskComplexity.MODERATE
        else:
            return TaskComplexity.SIMPLE

    async def _select_optimal_model(
        self, task_request: TaskRequest, complexity: TaskComplexity
    ) -> str:
        """Select the optimal model based on requirements and strategy"""
        try:
            # Get candidate models for this complexity
            candidate_models = self.model_routing_rules.get(
                complexity, ["mixtral-8x7b"]
            )

            # Filter by budget constraints
            if task_request.max_cost:
                affordable_models = []
                for model_id in candidate_models:
                    model_config = self.model_configs[model_id]
                    estimated_tokens = min(
                        task_request.max_tokens or 500, model_config.max_tokens
                    )
                    estimated_cost = estimated_tokens * model_config.cost_per_token

                    if estimated_cost <= task_request.max_cost:
                        affordable_models.append(model_id)

                if affordable_models:
                    candidate_models = affordable_models

            # Apply optimization strategy
            if self.optimization_strategy == CostOptimizationStrategy.COST_FIRST:
                # Select cheapest model that meets quality requirements
                best_model = min(
                    candidate_models, key=lambda m: self.model_configs[m].cost_per_token
                )
            elif (
                self.optimization_strategy == CostOptimizationStrategy.PERFORMANCE_FIRST
            ):
                # Select highest quality model
                best_model = max(
                    candidate_models, key=lambda m: self.model_configs[m].quality_score
                )
            elif self.optimization_strategy == CostOptimizationStrategy.ADAPTIVE:
                # Use usage patterns to select model
                best_model = await self._adaptive_model_selection(
                    candidate_models, task_request
                )
            else:  # BALANCED
                # Balance cost and quality
                best_model = self._balanced_model_selection(
                    candidate_models, task_request
                )

            return best_model

        except Exception as e:
            logger.exception(f"Error selecting optimal model: {e}")
            return "mixtral-8x7b"  # Fallback to medium model

    def _balanced_model_selection(
        self, candidate_models: list[str], task_request: TaskRequest
    ) -> str:
        """Select model balancing cost and quality"""
        best_score = -1
        best_model = candidate_models[0]

        for model_id in candidate_models:
            model_config = self.model_configs[model_id]

            # Calculate value score (quality / cost ratio)
            if model_config.cost_per_token > 0:
                value_score = model_config.quality_score / model_config.cost_per_token

                # Adjust for quality requirements
                if model_config.quality_score >= task_request.required_quality:
                    value_score *= 1.2  # Bonus for meeting quality threshold

                if value_score > best_score:
                    best_score = value_score
                    best_model = model_id

        return best_model

    async def _adaptive_model_selection(
        self, candidate_models: list[str], task_request: TaskRequest
    ) -> str:
        """Adaptive model selection based on usage patterns"""
        try:
            # Get user's historical performance data
            user_metrics = self.cost_metrics.get(task_request.user_id)
            if not user_metrics:
                return self._balanced_model_selection(candidate_models, task_request)

            # Analyze user's cost vs quality preferences
            if user_metrics.total_cost > 0 and user_metrics.request_count > 10:
                avg_cost_per_request = (
                    user_metrics.total_cost / user_metrics.request_count
                )

                # If user typically uses expensive models, prefer quality
                if avg_cost_per_request > 0.01:
                    return max(
                        candidate_models,
                        key=lambda m: self.model_configs[m].quality_score,
                    )
                else:
                    return min(
                        candidate_models,
                        key=lambda m: self.model_configs[m].cost_per_token,
                    )

            return self._balanced_model_selection(candidate_models, task_request)

        except Exception as e:
            logger.exception(f"Error in adaptive model selection: {e}")
            return self._balanced_model_selection(candidate_models, task_request)

    async def _optimize_prompt(self, prompt: str, model_id: str) -> str:
        """Optimize prompt for cost efficiency while maintaining effectiveness"""
        try:
            # Skip optimization for small prompts
            if len(prompt) < 200:
                return prompt

            # Use a small model to optimize the prompt
            async with self.cortex_service as cortex:
                optimization_prompt = f"""
                Optimize this prompt for efficiency while maintaining its intent and effectiveness:

                Original Prompt: {prompt}

                Guidelines:
                1. Remove redundant words and phrases
                2. Make instructions more concise
                3. Maintain all key requirements
                4. Keep the same tone and intent

                Return only the optimized prompt:
                """

                optimized = await 
                    prompt=optimization_prompt,
                    max_tokens=len(prompt) // 2,  # Target 50% reduction
                    model="mistral-7b",  # Use small model for optimization
                )

                # Only use optimized version if it's significantly shorter
                if len(optimized) < len(prompt) * 0.8:
                    return optimized.strip()

            return prompt

        except Exception as e:
            logger.exception(f"Error optimizing prompt: {e}")
            return prompt

    async def _execute_task(
        self,
        prompt: str,
        model_id: str,
        max_tokens: int | None = None,
        temperature: float = 0.7,
    ) -> tuple[str, int]:
        """Execute the task using the selected model"""
        try:
            async with self.cortex_service as cortex:
                response = await 
                    prompt=prompt,
                    max_tokens=max_tokens or 500,
                    temperature=temperature,
                    model=model_id,
                )

                # Estimate tokens used (rough approximation)
                tokens_used = len(prompt.split()) + len(response.split())

                return response, tokens_used

        except Exception as e:
            logger.exception(f"Error executing task: {e}")
            raise

    async def _assess_response_quality(
        self, prompt: str, response: str, required_quality: float
    ) -> float:
        """Assess the quality of the response"""
        try:
            # Simple quality assessment based on response characteristics
            quality_score = 0.5  # Base score

            # Length appropriateness
            if 50 <= len(response) <= 2000:
                quality_score += 0.2

            # Coherence (simple check)
            if (
                response
                and not response.startswith("I cannot")
                and not response.startswith("I don't")
            ):
                quality_score += 0.2

            # Relevance (keyword overlap)
            prompt_words = set(prompt.lower().split())
            response_words = set(response.lower().split())
            overlap = len(prompt_words.intersection(response_words))
            if overlap > 0:
                quality_score += min(0.1, overlap / len(prompt_words))

            return min(1.0, quality_score)

        except Exception as e:
            logger.exception(f"Error assessing response quality: {e}")
            return 0.7  # Default quality score

    async def _cache_result(
        self,
        task_request: TaskRequest,
        response: str,
        model_used: str,
        quality_score: float,
    ) -> None:
        """Cache the result for future use"""
        try:
            cache_key = f"semantic_{hash(task_request.prompt + task_request.task_type)}"
            cache_data = {
                "response": response,
                "model_used": model_used,
                "quality_score": quality_score,
                "timestamp": datetime.now(),
                "metadata": {
                    "task_type": task_request.task_type,
                    "user_id": task_request.user_id,
                },
            }

            # Cache with TTL
            await self.cache_manager.set(
                cache_key, cache_data, ttl=self.cache_ttl_hours * 3600
            )

            # Also store in AI Memory for semantic search
            await self.ai_memory.store_llm_response(
                prompt=task_request.prompt,
                response=response,
                model_used=model_used,
                metadata={
                    "quality_score": quality_score,
                    "task_type": task_request.task_type,
                    "cost_optimized": True,
                },
            )

        except Exception as e:
            logger.exception(f"Error caching result: {e}")

    async def _update_metrics(
        self,
        user_id: str,
        tokens_used: int,
        cost: float,
        latency_ms: float,
        cache_hit: bool,
    ) -> None:
        """Update cost and performance metrics"""
        try:
            # Update user metrics
            if user_id not in self.cost_metrics:
                self.cost_metrics[user_id] = CostMetrics()

            user_metrics = self.cost_metrics[user_id]
            user_metrics.total_tokens += tokens_used
            user_metrics.total_cost += cost
            user_metrics.request_count += 1

            if cache_hit:
                user_metrics.cache_hits += 1
                user_metrics.cost_savings += cost  # Estimated savings
            else:
                user_metrics.cache_misses += 1

            # Update average latency
            if user_metrics.request_count > 1:
                user_metrics.avg_latency_ms = (
                    user_metrics.avg_latency_ms * (user_metrics.request_count - 1)
                    + latency_ms
                ) / user_metrics.request_count
            else:
                user_metrics.avg_latency_ms = latency_ms

            user_metrics.timestamp = datetime.now()

            # Update global metrics
            self.global_metrics.total_tokens += tokens_used
            self.global_metrics.total_cost += cost
            self.global_metrics.request_count += 1

            if cache_hit:
                self.global_metrics.cache_hits += 1
                self.global_metrics.cost_savings += cost
            else:
                self.global_metrics.cache_misses += 1

            # Update global average latency
            if self.global_metrics.request_count > 1:
                self.global_metrics.avg_latency_ms = (
                    self.global_metrics.avg_latency_ms
                    * (self.global_metrics.request_count - 1)
                    + latency_ms
                ) / self.global_metrics.request_count
            else:
                self.global_metrics.avg_latency_ms = latency_ms

            self.global_metrics.timestamp = datetime.now()

        except Exception as e:
            logger.exception(f"Error updating metrics: {e}")

    async def get_cost_report(self, user_id: str | None = None) -> dict[str, Any]:
        """Generate cost report for user or global"""
        try:
            if user_id:
                metrics = self.cost_metrics.get(user_id, CostMetrics())
                scope = f"user_{user_id}"
            else:
                metrics = self.global_metrics
                scope = "global"

            # Calculate cache hit rate
            total_requests = metrics.cache_hits + metrics.cache_misses
            cache_hit_rate = (
                metrics.cache_hits / total_requests if total_requests > 0 else 0
            )

            # Calculate cost per request
            cost_per_request = (
                metrics.total_cost / metrics.request_count
                if metrics.request_count > 0
                else 0
            )

            # Calculate savings rate
            potential_cost = metrics.total_cost + metrics.cost_savings
            savings_rate = (
                metrics.cost_savings / potential_cost if potential_cost > 0 else 0
            )

            return {
                "scope": scope,
                "period": "all_time",
                "metrics": {
                    "total_requests": metrics.request_count,
                    "total_tokens": metrics.total_tokens,
                    "total_cost": round(metrics.total_cost, 4),
                    "cost_savings": round(metrics.cost_savings, 4),
                    "cache_hit_rate": round(cache_hit_rate, 3),
                    "avg_latency_ms": round(metrics.avg_latency_ms, 2),
                    "cost_per_request": round(cost_per_request, 4),
                    "savings_rate": round(savings_rate, 3),
                },
                "optimization_summary": {
                    "cache_effectiveness": (
                        "high"
                        if cache_hit_rate > 0.3
                        else "medium"
                        if cache_hit_rate > 0.1
                        else "low"
                    ),
                    "cost_efficiency": (
                        "high"
                        if cost_per_request < 0.01
                        else "medium"
                        if cost_per_request < 0.05
                        else "low"
                    ),
                    "performance": (
                        "fast"
                        if metrics.avg_latency_ms < 1000
                        else "medium"
                        if metrics.avg_latency_ms < 2000
                        else "slow"
                    ),
                },
                "recommendations": await self._generate_cost_recommendations(metrics),
                "last_updated": metrics.timestamp,
            }

        except Exception as e:
            logger.exception(f"Error generating cost report: {e}")
            return {"error": str(e)}

    async def _generate_cost_recommendations(self, metrics: CostMetrics) -> list[str]:
        """Generate cost optimization recommendations"""
        recommendations = []

        try:
            # Cache hit rate recommendations
            total_requests = metrics.cache_hits + metrics.cache_misses
            cache_hit_rate = (
                metrics.cache_hits / total_requests if total_requests > 0 else 0
            )

            if cache_hit_rate < 0.2:
                recommendations.append(
                    "Increase cache TTL or improve semantic similarity threshold to boost cache hit rate"
                )

            # Cost per request recommendations
            cost_per_request = (
                metrics.total_cost / metrics.request_count
                if metrics.request_count > 0
                else 0
            )

            if cost_per_request > 0.05:
                recommendations.append(
                    "Consider using smaller models for simple tasks to reduce cost per request"
                )

            # Latency recommendations
            if metrics.avg_latency_ms > 2000:
                recommendations.append(
                    "Optimize prompts or use faster models to improve response times"
                )

            # General recommendations
            if metrics.request_count > 100:
                recommendations.append(
                    "Consider implementing request batching for better cost efficiency"
                )

            if not recommendations:
                recommendations.append(
                    "Cost optimization is performing well - continue current strategy"
                )

            return recommendations

        except Exception as e:
            logger.exception(f"Error generating recommendations: {e}")
            return ["Unable to generate recommendations due to error"]

    async def set_optimization_strategy(
        self, strategy: CostOptimizationStrategy
    ) -> None:
        """Set the cost optimization strategy"""
        self.optimization_strategy = strategy
        logger.info(f"Cost optimization strategy set to: {strategy.value}")

    async def set_cost_budget(
        self, daily_budget: float | None = None, monthly_budget: float | None = None
    ) -> None:
        """Set cost budgets"""
        self.cost_budget_daily = daily_budget
        self.cost_budget_monthly = monthly_budget
        logger.info(
            f"Cost budgets set - Daily: {daily_budget}, Monthly: {monthly_budget}"
        )

    async def _cost_monitoring_task(self) -> None:
        """Background task to monitor costs and send alerts"""
        while True:
            try:
                # Check daily budget
                if self.cost_budget_daily:
                    today_cost = await self._calculate_daily_cost()
                    if today_cost > self.cost_budget_daily:
                        await self._send_budget_alert(
                            "daily", today_cost, self.cost_budget_daily
                        )

                # Check monthly budget
                if self.cost_budget_monthly:
                    month_cost = await self._calculate_monthly_cost()
                    if month_cost > self.cost_budget_monthly:
                        await self._send_budget_alert(
                            "monthly", month_cost, self.cost_budget_monthly
                        )

                # Sleep for 1 hour
                await asyncio.sleep(3600)

            except Exception as e:
                logger.exception(f"Error in cost monitoring: {e}")
                await asyncio.sleep(300)  # Sleep 5 minutes on error

    async def _optimization_analysis_task(self) -> None:
        """Background task to analyze optimization opportunities"""
        while True:
            try:
                # Analyze usage patterns every 6 hours
                await asyncio.sleep(21600)

                # Analyze model usage patterns
                await self._analyze_model_usage_patterns()

                # Analyze cache effectiveness
                await self._analyze_cache_effectiveness()

                # Update routing rules if needed
                await self._update_routing_rules()

            except Exception as e:
                logger.exception(f"Error in optimization analysis: {e}")
                await asyncio.sleep(1800)  # Sleep 30 minutes on error

    async def _calculate_daily_cost(self) -> float:
        """Calculate today's total cost"""
        today = datetime.now().date()
        daily_cost = 0.0

        for user_metrics in self.cost_metrics.values():
            if user_metrics.timestamp.date() == today:
                daily_cost += user_metrics.total_cost

        return daily_cost

    async def _calculate_monthly_cost(self) -> float:
        """Calculate this month's total cost"""
        current_month = datetime.now().replace(day=1).date()
        monthly_cost = 0.0

        for user_metrics in self.cost_metrics.values():
            if user_metrics.timestamp.date() >= current_month:
                monthly_cost += user_metrics.total_cost

        return monthly_cost

    async def _send_budget_alert(
        self, period: str, actual_cost: float, budget: float
    ) -> None:
        """Send budget alert"""
        logger.warning(
            f"Budget exceeded - {period}: ${actual_cost:.2f} / ${budget:.2f}"
        )

        # Log budget alert
        await self.audit_logger.log_system_event(
            event_type="budget_alert",
            details={
                "period": period,
                "actual_cost": actual_cost,
                "budget": budget,
                "overage": actual_cost - budget,
            },
        )

    async def _analyze_model_usage_patterns(self) -> None:
        """Analyze model usage patterns for optimization"""
        # Implementation would analyze which models are used most
        # and their cost-effectiveness
        pass

    async def _analyze_cache_effectiveness(self) -> None:
        """Analyze cache effectiveness and adjust parameters"""
        # Implementation would analyze cache hit rates and adjust
        # similarity thresholds and TTL values
        pass

    async def _update_routing_rules(self) -> None:
        """Update routing rules based on performance data"""
        # Implementation would update routing rules based on
        # observed performance and cost data
        pass

# Global instance
cost_engineering_service = CostEngineeringService()
