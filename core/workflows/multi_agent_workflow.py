from datetime import UTC, datetime

"""
Sophia AI - Multi-Agent Workflow Framework
Orchestrates collaboration between multiple AI agents for complex business intelligence workflows
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 758 lines

Recommended decomposition:
- multi_agent_workflow_core.py - Core functionality
- multi_agent_workflow_utils.py - Utility functions
- multi_agent_workflow_models.py - Data models
- multi_agent_workflow_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import logging
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

logger = logging.getLogger(__name__)


class AgentRole(Enum):
    """Agent roles in collaborative workflows."""

    COORDINATOR = "coordinator"
    ANALYZER = "analyzer"
    EXECUTOR = "executor"
    VALIDATOR = "validator"
    REPORTER = "reporter"
    SYNTHESIZER = "synthesizer"
    MONITOR = "monitor"


class WorkflowStatus(Enum):
    """Workflow execution status."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    PAUSED = "paused"
    CANCELLED = "cancelled"


class TaskStatus(Enum):
    """Individual task status."""

    WAITING = "waiting"
    READY = "ready"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
    RETRYING = "retrying"


class WorkflowPriority(Enum):
    """Workflow execution priority."""

    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class WorkflowTask:
    """Individual task in a multi-agent workflow."""

    task_id: str
    agent_type: str
    agent_role: AgentRole
    input_data: dict[str, Any]
    dependencies: list[str] = field(default_factory=list)
    timeout_seconds: int = 300
    retry_attempts: int = 3
    priority: WorkflowPriority = WorkflowPriority.MEDIUM
    expected_output_schema: dict[str, Any] | None = None
    validation_rules: list[str] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class WorkflowResult:
    """Result from a workflow task."""

    task_id: str
    agent_type: str
    status: TaskStatus
    output_data: dict[str, Any]
    execution_time_seconds: float
    start_time: datetime
    end_time: datetime
    error_message: str | None = None
    retry_count: int = 0
    validation_passed: bool = True
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class WorkflowDefinition:
    """Complete workflow definition."""

    workflow_id: str
    name: str
    description: str
    tasks: list[WorkflowTask]
    priority: WorkflowPriority = WorkflowPriority.MEDIUM
    timeout_seconds: int = 1800  # 30 minutes
    enable_parallel_execution: bool = True
    enable_auto_retry: bool = True
    success_criteria: list[str] = field(default_factory=list)
    failure_conditions: list[str] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class WorkflowExecution:
    """Workflow execution state and results."""

    workflow_id: str
    execution_id: str
    status: WorkflowStatus
    start_time: datetime
    end_time: datetime | None = None
    task_results: dict[str, WorkflowResult] = field(default_factory=dict)
    execution_metrics: dict[str, Any] = field(default_factory=dict)
    error_message: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)


class AgentWorkflowInterface:
    """Interface that agents must implement to participate in workflows."""

    async def analyze(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """Analyze data and return insights."""
        raise NotImplementedError("Agents must implement analyze method")

    async def execute(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """Execute an action based on input data."""
        raise NotImplementedError("Agents must implement execute method")

    async def validate(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """Validate data or results."""
        raise NotImplementedError("Agents must implement validate method")

    async def report(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """Generate reports from data."""
        raise NotImplementedError("Agents must implement report method")

    async def synthesize(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """Synthesize information from multiple sources."""
        raise NotImplementedError("Agents must implement synthesize method")

    async def monitor(self, input_data: dict[str, Any]) -> dict[str, Any]:
        """Monitor systems or processes."""
        raise NotImplementedError("Agents must implement monitor method")


class MultiAgentWorkflow:
    """
    Orchestrates collaboration between multiple AI agents in sophisticated workflows.

    Features:
    - Dynamic task dependency resolution
    - Parallel and sequential execution modes
    - Intelligent retry and error handling
    - Performance monitoring and optimization
    - Result validation and quality assurance
    - Cross-agent data sharing and synthesis
    """

    def __init__(self, workflow_definition: WorkflowDefinition):
        self.workflow_def = workflow_definition
        self.agents: dict[str, AgentWorkflowInterface] = {}
        self.task_results: dict[str, WorkflowResult] = {}
        self.dependency_graph: dict[str, set[str]] = {}
        self.reverse_dependencies: dict[str, set[str]] = {}
        self.execution_queue: deque = deque()
        self.running_tasks: dict[str, asyncio.Task] = {}

        # Build dependency graph
        self._build_dependency_graph()

    def _build_dependency_graph(self) -> None:
        """Build task dependency graph for execution planning."""
        self.dependency_graph = {}
        self.reverse_dependencies = defaultdict(set)

        for task in self.workflow_def.tasks:
            self.dependency_graph[task.task_id] = set(task.dependencies)
            for dependency in task.dependencies:
                self.reverse_dependencies[dependency].add(task.task_id)

        # Validate no circular dependencies
        if self._has_circular_dependencies():
            raise ValueError("Circular dependencies detected in workflow")

    def _has_circular_dependencies(self) -> bool:
        """Check for circular dependencies using DFS."""
        visited = set()
        rec_stack = set()

        def dfs(node: str) -> bool:
            if node in rec_stack:
                return True
            if node in visited:
                return False

            visited.add(node)
            rec_stack.add(node)

            for dependency in self.dependency_graph.get(node, []):
                if dfs(dependency):
                    return True

            rec_stack.remove(node)
            return False

        for task in self.workflow_def.tasks:
            if task.task_id not in visited and dfs(task.task_id):
                return True

        return False

    def register_agent(
        self, agent_type: str, agent_instance: AgentWorkflowInterface
    ) -> None:
        """Register an agent for workflow participation."""
        self.agents[agent_type] = agent_instance
        logger.info(f"âœ… Registered agent: {agent_type}")

    async def execute_workflow(self) -> WorkflowExecution:
        """Execute the complete multi-agent workflow."""
        execution_id = f"{self.workflow_def.workflow_id}_{int(time.time())}"
        start_time = datetime.now(UTC)

        execution = WorkflowExecution(
            workflow_id=self.workflow_def.workflow_id,
            execution_id=execution_id,
            status=WorkflowStatus.RUNNING,
            start_time=start_time,
        )

        try:
            logger.info(f"ðŸš€ Starting workflow execution: {execution_id}")

            # Validate all required agents are registered
            required_agents = {task.agent_type for task in self.workflow_def.tasks}
            missing_agents = required_agents - set(self.agents.keys())
            if missing_agents:
                raise ValueError(f"Missing required agents: {missing_agents}")

            # Execute workflow with timeout
            try:
                await asyncio.wait_for(
                    self._execute_workflow_tasks(),
                    timeout=self.workflow_def.timeout_seconds,
                )

                # Determine final status based on task results
                if all(
                    result.status == TaskStatus.COMPLETED
                    for result in self.task_results.values()
                ):
                    execution.status = WorkflowStatus.COMPLETED
                else:
                    execution.status = WorkflowStatus.FAILED

            except TimeoutError:
                execution.status = WorkflowStatus.FAILED
                execution.error_message = f"Workflow timeout after {self.workflow_def.timeout_seconds} seconds"
                logger.exception(f"âŒ Workflow {execution_id} timed out")

            # Finalize execution
            execution.end_time = datetime.now(UTC)
            execution.task_results = self.task_results.copy()
            execution.execution_metrics = self._calculate_execution_metrics(execution)

            # Generate workflow summary
            total_duration = (execution.end_time - execution.start_time).total_seconds()
            success_rate = sum(
                1
                for r in self.task_results.values()
                if r.status == TaskStatus.COMPLETED
            ) / len(self.task_results)

            logger.info(
                f"âœ… Workflow {execution_id} completed in {total_duration:.2f}s with {success_rate:.1%} success rate"
            )

            return execution

        except Exception as e:
            execution.status = WorkflowStatus.FAILED
            execution.error_message = str(e)
            execution.end_time = datetime.now(UTC)
            execution.task_results = self.task_results.copy()

            logger.exception(f"âŒ Workflow {execution_id} failed: {e}")
            return execution

    async def _execute_workflow_tasks(self) -> None:
        """Execute workflow tasks with dependency resolution and parallel processing."""
        # Initialize execution queue with tasks that have no dependencies
        ready_tasks = [
            task for task in self.workflow_def.tasks if not task.dependencies
        ]
        for task in ready_tasks:
            self.execution_queue.append(task)

        while self.execution_queue or self.running_tasks:
            # Start new tasks if possible
            tasks_to_start = []
            while (
                self.execution_queue
                and len(self.running_tasks) < self._get_max_parallel_tasks()
            ):
                task = self.execution_queue.popleft()
                tasks_to_start.append(task)

            # Start tasks in parallel
            for task in tasks_to_start:
                asyncio_task = asyncio.create_task(self._execute_single_task(task))
                self.running_tasks[task.task_id] = asyncio_task

            if not self.running_tasks:
                break

            # Wait for at least one task to complete
            done, pending = await asyncio.wait(
                self.running_tasks.values(), return_when=asyncio.FIRST_COMPLETED
            )

            # Process completed tasks
            for asyncio_task in done:
                task_id = None
                for tid, t in self.running_tasks.items():
                    if t == asyncio_task:
                        task_id = tid
                        break

                if task_id:
                    del self.running_tasks[task_id]

                    # Add dependent tasks to queue if their dependencies are satisfied
                    for dependent_task_id in self.reverse_dependencies.get(task_id, []):
                        if self._are_dependencies_satisfied(dependent_task_id):
                            dependent_task = next(
                                task
                                for task in self.workflow_def.tasks
                                if task.task_id == dependent_task_id
                            )
                            if dependent_task_id not in self.task_results:
                                self.execution_queue.append(dependent_task)

    def _get_max_parallel_tasks(self) -> int:
        """Get maximum number of parallel tasks based on workflow configuration."""
        if not self.workflow_def.enable_parallel_execution:
            return 1

        # Limit based on priority
        if self.workflow_def.priority == WorkflowPriority.CRITICAL:
            return 8
        elif self.workflow_def.priority == WorkflowPriority.HIGH:
            return 6
        elif self.workflow_def.priority == WorkflowPriority.MEDIUM:
            return 4
        else:
            return 2

    def _are_dependencies_satisfied(self, task_id: str) -> bool:
        """Check if all dependencies for a task are satisfied."""
        task = next(task for task in self.workflow_def.tasks if task.task_id == task_id)

        for dependency in task.dependencies:
            if dependency not in self.task_results:
                return False
            if self.task_results[dependency].status != TaskStatus.COMPLETED:
                return False

        return True

    async def _execute_single_task(self, task: WorkflowTask) -> WorkflowResult:
        """Execute a single task with retry logic and validation."""
        start_time = datetime.now(UTC)
        retry_count = 0

        while retry_count <= task.retry_attempts:
            try:
                logger.info(
                    f"ðŸ”„ Executing task {task.task_id} (attempt {retry_count + 1})"
                )

                # Prepare input data with dependency results
                input_data = await self._prepare_task_input(task)

                # Execute task based on agent role
                agent = self.agents[task.agent_type]

                if task.agent_role == AgentRole.ANALYZER:
                    output = await agent.analyze(input_data)
                elif task.agent_role == AgentRole.EXECUTOR:
                    output = await agent.execute(input_data)
                elif task.agent_role == AgentRole.VALIDATOR:
                    output = await agent.validate(input_data)
                elif task.agent_role == AgentRole.REPORTER:
                    output = await agent.report(input_data)
                elif task.agent_role == AgentRole.SYNTHESIZER:
                    output = await agent.synthesize(input_data)
                elif task.agent_role == AgentRole.MONITOR:
                    output = await agent.monitor(input_data)
                else:
                    # Default to analyze for COORDINATOR and unknown roles
                    output = await agent.analyze(input_data)

                # Validate output
                validation_passed = await self._validate_task_output(task, output)

                end_time = datetime.now(UTC)
                execution_time = (end_time - start_time).total_seconds()

                # Create successful result
                result = WorkflowResult(
                    task_id=task.task_id,
                    agent_type=task.agent_type,
                    status=TaskStatus.COMPLETED,
                    output_data=output,
                    execution_time_seconds=execution_time,
                    start_time=start_time,
                    end_time=end_time,
                    retry_count=retry_count,
                    validation_passed=validation_passed,
                )

                self.task_results[task.task_id] = result
                logger.info(f"âœ… Task {task.task_id} completed successfully")
                return result

            except Exception as e:
                retry_count += 1
                error_msg = str(e)

                if retry_count > task.retry_attempts:
                    # Final failure
                    end_time = datetime.now(UTC)
                    execution_time = (end_time - start_time).total_seconds()

                    result = WorkflowResult(
                        task_id=task.task_id,
                        agent_type=task.agent_type,
                        status=TaskStatus.FAILED,
                        output_data={},
                        execution_time_seconds=execution_time,
                        start_time=start_time,
                        end_time=end_time,
                        error_message=error_msg,
                        retry_count=retry_count - 1,
                        validation_passed=False,
                    )

                    self.task_results[task.task_id] = result
                    logger.exception(
                        f"âŒ Task {task.task_id} failed after {retry_count - 1} retries: {error_msg}"
                    )
                    return result
                else:
                    # Retry with exponential backoff
                    backoff_time = 2 ** (retry_count - 1)
                    logger.warning(
                        f"âš ï¸ Task {task.task_id} failed, retrying in {backoff_time}s: {error_msg}"
                    )
                    await asyncio.sleep(backoff_time)

    async def _prepare_task_input(self, task: WorkflowTask) -> dict[str, Any]:
        """Prepare input data for task execution including dependency results."""
        input_data = task.input_data.copy()

        # Add dependency results
        dependency_results = {}
        for dependency in task.dependencies:
            if dependency in self.task_results:
                dependency_results[dependency] = self.task_results[
                    dependency
                ].output_data

        input_data["dependency_results"] = dependency_results
        input_data["task_metadata"] = task.metadata

        return input_data

    async def _validate_task_output(
        self, task: WorkflowTask, output: dict[str, Any]
    ) -> bool:
        """Validate task output against expected schema and rules."""
        try:
            # Schema validation
            if task.expected_output_schema:
                # Basic schema validation
                required_fields = task.expected_output_schema.get("required", [])
                for field in required_fields:
                    if field not in output:
                        logger.warning(
                            f"Missing required field '{field}' in task {task.task_id} output"
                        )
                        return False

            # Custom validation rules
            for rule in task.validation_rules:
                # Simple rule evaluation
                if "required_keys" in rule:
                    required_keys = rule.split(":")[1].split(",")
                    for key in required_keys:
                        if key.strip() not in output:
                            return False

            return True

        except Exception as e:
            logger.exception(f"Validation error for task {task.task_id}: {e}")
            return False

    def _calculate_execution_metrics(
        self, execution: WorkflowExecution
    ) -> dict[str, Any]:
        """Calculate comprehensive execution metrics."""
        total_duration = 0
        if execution.end_time:
            total_duration = (execution.end_time - execution.start_time).total_seconds()

        task_metrics = {}
        for task_id, result in execution.task_results.items():
            task_metrics[task_id] = {
                "execution_time": result.execution_time_seconds,
                "retry_count": result.retry_count,
                "validation_passed": result.validation_passed,
                "status": result.status.value,
            }

        # Calculate aggregate metrics
        completed_tasks = sum(
            1
            for r in execution.task_results.values()
            if r.status == TaskStatus.COMPLETED
        )
        failed_tasks = sum(
            1 for r in execution.task_results.values() if r.status == TaskStatus.FAILED
        )
        total_retries = sum(r.retry_count for r in execution.task_results.values())
        avg_execution_time = (
            sum(r.execution_time_seconds for r in execution.task_results.values())
            / len(execution.task_results)
            if execution.task_results
            else 0
        )

        return {
            "total_duration_seconds": total_duration,
            "total_tasks": len(execution.task_results),
            "completed_tasks": completed_tasks,
            "failed_tasks": failed_tasks,
            "success_rate": (
                completed_tasks / len(execution.task_results)
                if execution.task_results
                else 0
            ),
            "total_retries": total_retries,
            "average_task_execution_time": avg_execution_time,
            "task_metrics": task_metrics,
            "workflow_efficiency": (
                completed_tasks / (total_duration / 60) if total_duration > 0 else 0
            ),  # tasks per minute
        }


class ProjectIntelligenceWorkflow(MultiAgentWorkflow):
    """
    Specialized workflow for comprehensive project intelligence analysis.

    Coordinates multiple agents to analyze project ecosystem across platforms.
    """

    def __init__(self, project_context: dict[str, Any]):
        # Define project intelligence workflow
        workflow_def = WorkflowDefinition(
            workflow_id="project_intelligence",
            name="Project Intelligence Analysis",
            description="Comprehensive project analysis across Linear, Asana, Gong, and HubSpot",
            tasks=[
                # Phase 1: Data Collection (Parallel)
                WorkflowTask(
                    task_id="collect_linear_data",
                    agent_type="linear_project_health",
                    agent_role=AgentRole.ANALYZER,
                    input_data={"project_id": project_context.get("linear_project_id")},
                    expected_output_schema={
                        "required": ["project_health", "issues", "team_performance"]
                    },
                    priority=WorkflowPriority.HIGH,
                ),
                WorkflowTask(
                    task_id="collect_asana_data",
                    agent_type="asana_project_intelligence",
                    agent_role=AgentRole.ANALYZER,
                    input_data={
                        "project_gid": project_context.get("asana_project_gid")
                    },
                    expected_output_schema={
                        "required": ["project_status", "tasks", "team_metrics"]
                    },
                    priority=WorkflowPriority.HIGH,
                ),
                WorkflowTask(
                    task_id="collect_gong_data",
                    agent_type="call_analysis",
                    agent_role=AgentRole.ANALYZER,
                    input_data={
                        "project_keywords": project_context.get("keywords", [])
                    },
                    expected_output_schema={
                        "required": ["call_insights", "sentiment_analysis"]
                    },
                    priority=WorkflowPriority.HIGH,
                ),
                WorkflowTask(
                    task_id="collect_hubspot_data",
                    agent_type="sales_intelligence",
                    agent_role=AgentRole.ANALYZER,
                    input_data={"deal_ids": project_context.get("deal_ids", [])},
                    expected_output_schema={
                        "required": ["deal_analysis", "customer_insights"]
                    },
                    priority=WorkflowPriority.HIGH,
                ),
                # Phase 2: Cross-Platform Analysis (Depends on data collection)
                WorkflowTask(
                    task_id="cross_platform_analysis",
                    agent_type="sales_intelligence",
                    agent_role=AgentRole.SYNTHESIZER,
                    input_data={"analysis_scope": "cross_platform"},
                    dependencies=[
                        "collect_linear_data",
                        "collect_asana_data",
                        "collect_gong_data",
                        "collect_hubspot_data",
                    ],
                    expected_output_schema={
                        "required": ["cross_platform_insights", "correlation_analysis"]
                    },
                    priority=WorkflowPriority.CRITICAL,
                ),
                # Phase 3: Risk Assessment (Depends on analysis)
                WorkflowTask(
                    task_id="risk_assessment",
                    agent_type="linear_project_health",
                    agent_role=AgentRole.VALIDATOR,
                    input_data={"assessment_type": "comprehensive_risk"},
                    dependencies=["cross_platform_analysis"],
                    expected_output_schema={
                        "required": [
                            "risk_score",
                            "risk_factors",
                            "mitigation_strategies",
                        ]
                    },
                    priority=WorkflowPriority.CRITICAL,
                ),
                # Phase 4: Strategic Recommendations (Depends on risk assessment)
                WorkflowTask(
                    task_id="strategic_recommendations",
                    agent_type="sales_coach",
                    agent_role=AgentRole.EXECUTOR,
                    input_data={"recommendation_scope": "strategic"},
                    dependencies=["risk_assessment"],
                    expected_output_schema={
                        "required": [
                            "recommendations",
                            "action_items",
                            "success_metrics",
                        ]
                    },
                    priority=WorkflowPriority.HIGH,
                ),
                # Phase 5: Executive Report (Depends on all previous tasks)
                WorkflowTask(
                    task_id="executive_report",
                    agent_type="marketing_analysis",
                    agent_role=AgentRole.REPORTER,
                    input_data={"report_type": "executive_summary"},
                    dependencies=["strategic_recommendations"],
                    expected_output_schema={
                        "required": ["executive_summary", "key_findings", "next_steps"]
                    },
                    priority=WorkflowPriority.MEDIUM,
                ),
            ],
            priority=WorkflowPriority.HIGH,
            timeout_seconds=1800,  # 30 minutes
            enable_parallel_execution=True,
            success_criteria=[
                "all_data_collected",
                "analysis_completed",
                "report_generated",
            ],
        )

        super().__init__(workflow_def)


class BusinessIntelligenceWorkflow(MultiAgentWorkflow):
    """
    Workflow for comprehensive business intelligence analysis across all data sources.
    """

    def __init__(self, analysis_scope: str = "comprehensive"):
        workflow_def = WorkflowDefinition(
            workflow_id="business_intelligence",
            name="Business Intelligence Analysis",
            description="Comprehensive business analysis across all platforms and data sources",
            tasks=[
                # Data aggregation phase
                WorkflowTask(
                    task_id="aggregate_sales_data",
                    agent_type="sales_intelligence",
                    agent_role=AgentRole.ANALYZER,
                    input_data={"scope": analysis_scope, "include_forecasts": True},
                ),
                WorkflowTask(
                    task_id="aggregate_marketing_data",
                    agent_type="marketing_analysis",
                    agent_role=AgentRole.ANALYZER,
                    input_data={"scope": analysis_scope, "include_campaigns": True},
                ),
                # Cross-functional analysis
                WorkflowTask(
                    task_id="cross_functional_analysis",
                    agent_type="sales_intelligence",
                    agent_role=AgentRole.SYNTHESIZER,
                    input_data={"analysis_type": "cross_functional"},
                    dependencies=["aggregate_sales_data", "aggregate_marketing_data"],
                ),
                # Business recommendations
                WorkflowTask(
                    task_id="business_recommendations",
                    agent_type="sales_coach",
                    agent_role=AgentRole.EXECUTOR,
                    input_data={"recommendation_type": "business_optimization"},
                    dependencies=["cross_functional_analysis"],
                ),
                # Executive dashboard update
                WorkflowTask(
                    task_id="update_executive_dashboard",
                    agent_type="marketing_analysis",
                    agent_role=AgentRole.REPORTER,
                    input_data={"dashboard_type": "executive"},
                    dependencies=["business_recommendations"],
                ),
            ],
            priority=WorkflowPriority.MEDIUM,
            enable_parallel_execution=True,
        )

        super().__init__(workflow_def)
