name: Sophia AI Complete Stack Deployment

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Deployment environment"
        required: true
        default: "production"
        type: choice
        options:
          - production
          - staging
          - development
      instance_type:
        description: "Lambda Labs instance type"
        required: true
        default: "gpu_1x_a100"
        type: choice
        options:
          - gpu_1x_a10
          - gpu_1x_a100
          - gpu_1x_h100
          - gpu_2x_a100
      force_recreate:
        description: "Force recreate infrastructure"
        required: false
        default: false
        type: boolean
      skip_tests:
        description: "Skip health checks"
        required: false
        default: false
        type: boolean
      deploy_components:
        description: "Components to deploy (comma-separated)"
        required: false
        default: "all"
        type: string
  push:
    branches:
      - main
    paths:
      - "backend/**"
      - "infrastructure/**"
      - "scripts/**"
      - ".github/workflows/sophia-complete-stack-deployment.yml"
  schedule:
    # Daily deployment check at 6 AM UTC
    - cron: "0 6 * * *"

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "20"
  PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
  LAMBDA_LABS_API_KEY: ${{ secrets.LAMBDA_LABS_API_KEY }}
  ESTUARY_ACCESS_TOKEN: ${{ secrets.ESTUARY_ACCESS_TOKEN }}
  HUBSPOT_ACCESS_TOKEN: ${{ secrets.HUBSPOT_ACCESS_TOKEN }}
  GONG_ACCESS_KEY: ${{ secrets.GONG_ACCESS_KEY }}
  GONG_ACCESS_KEY_SECRET: ${{ secrets.GONG_ACCESS_KEY_SECRET }}
  SNOWFLAKE_USER: "PROGRAMMATIC_SERVICE_USER"
  SNOWFLAKE_PASSWORD: ${{ secrets.SOPHIA_AI_TOKEN }}
  SNOWFLAKE_ACCOUNT: "MYJDJNU-FP71296"
  SNOWFLAKE_DATABASE: "SOPHIA_AI_DB"
  SNOWFLAKE_WAREHOUSE: "SOPHIA_AI_WH"
  SNOWFLAKE_ROLE: "SOPHIA_AI_ROLE"

jobs:
  pre-deployment-validation:
    name: Pre-Deployment Validation
    runs-on: ubuntu-latest
    outputs:
      deployment-id: ${{ steps.generate-id.outputs.deployment-id }}
      components-to-deploy: ${{ steps.parse-components.outputs.components }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate deployment ID
        id: generate-id
        run: |
          DEPLOYMENT_ID="sophia-$(date +%Y%m%d-%H%M%S)-$(echo $GITHUB_SHA | cut -c1-8)"
          echo "deployment-id=$DEPLOYMENT_ID" >> $GITHUB_OUTPUT
          echo "ðŸ†” Deployment ID: $DEPLOYMENT_ID"

      - name: Parse deployment components
        id: parse-components
        run: |
          COMPONENTS="${{ github.event.inputs.deploy_components || 'all' }}"
          if [ "$COMPONENTS" = "all" ]; then
            COMPONENTS="environment_setup,secrets_management,lambda_labs_infrastructure,postgresql_setup,redis_setup,snowflake_setup,estuary_flow_pipeline,api_deployment,monitoring_setup,health_checks"
          fi
          echo "components=$COMPONENTS" >> $GITHUB_OUTPUT
          echo "ðŸ“¦ Components to deploy: $COMPONENTS"

      - name: Validate secrets
        run: |
          echo "ðŸ” Validating required secrets..."
          REQUIRED_SECRETS=(
            "PULUMI_ACCESS_TOKEN"
            "LAMBDA_LABS_API_KEY"
            "ESTUARY_ACCESS_TOKEN"
            "HUBSPOT_ACCESS_TOKEN"
            "GONG_ACCESS_KEY"
            "SOPHIA_AI_TOKEN"
          )

          MISSING_SECRETS=()
          for secret in "${REQUIRED_SECRETS[@]}"; do
            if [ -z "${!secret}" ]; then
              MISSING_SECRETS+=("$secret")
            else
              echo "âœ… $secret is available"
            fi
          done

          if [ ${#MISSING_SECRETS[@]} -ne 0 ]; then
            echo "âŒ Missing required secrets: ${MISSING_SECRETS[*]}"
            exit 1
          fi

          echo "âœ… All required secrets are available"

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install uv && uv pip install .[dev]

      - name: Validate project structure
        run: |
          echo "ðŸ“ Validating project structure..."
          REQUIRED_FILES=(
            "scripts/deploy-complete-sophia-stack.py"
            "backend/etl/enhanced_unified_data_pipeline.py"
            "backend/snowflake_setup/enhanced_data_pipeline_schema.sql"
            "infrastructure/pulumi-esc-comprehensive-update.py"
          )

          for file in "${REQUIRED_FILES[@]}"; do
            if [ ! -f "$file" ]; then
              echo "âŒ Missing required file: $file"
              exit 1
            else
              echo "âœ… Found: $file"
            fi
          done

      - name: Syntax validation
        run: |
          echo "ðŸ” Validating Python syntax..."
          python -m py_compile scripts/deploy-complete-sophia-stack.py
          python -m py_compile backend/etl/enhanced_unified_data_pipeline.py
          python -m py_compile infrastructure/pulumi-esc-comprehensive-update.py
          echo "âœ… Python syntax validation passed"

  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: pre-deployment-validation
    if: contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'lambda_labs_infrastructure') || contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'all')

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Pulumi CLI
        run: |
          curl -fsSL https://get.pulumi.com | sh
          echo "$HOME/.pulumi/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install uv && uv pip install .[dev]

      - name: Update Pulumi ESC configuration
        run: |
          echo "ðŸ” Updating Pulumi ESC configuration..."
          python infrastructure/pulumi-esc-comprehensive-update.py
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: Deploy Lambda Labs infrastructure
        run: |
          echo "ðŸ–¥ï¸ Deploying Lambda Labs infrastructure..."
          python scripts/lambda-labs-complete-setup.py \
            --instance-type "${{ github.event.inputs.instance_type || 'gpu_1x_a100' }}" \
            --force-recreate "${{ github.event.inputs.force_recreate || 'false' }}"
        env:
          LAMBDA_LABS_API_KEY: ${{ secrets.LAMBDA_LABS_API_KEY }}

      - name: Validate infrastructure deployment
        run: |
          echo "âœ… Infrastructure deployment validation..."
          # Add infrastructure validation logic here
          echo "Infrastructure deployment completed successfully"

  deploy-databases:
    name: Deploy Database Components
    runs-on: ubuntu-latest
    needs: [pre-deployment-validation, deploy-infrastructure]
    if: always() && (contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'postgresql_setup') || contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'snowflake_setup') || contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'all'))

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install uv && uv pip install .[dev]
          pip install snowflake-connector-python asyncpg redis

      - name: Install Snowflake CLI
        run: |
          curl -O https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-1.2.28-linux_x86_64.bash
          bash snowsql-1.2.28-linux_x86_64.bash
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Set up PostgreSQL schemas
        run: |
          echo "ðŸ˜ Setting up PostgreSQL schemas..."
          python backend/database/postgresql_staging_manager.py --setup
        env:
          DATABASE_HOST: ${{ secrets.DATABASE_HOST }}
          DATABASE_PORT: ${{ secrets.DATABASE_PORT }}
          DATABASE_NAME: ${{ secrets.DATABASE_NAME }}
          DATABASE_USER: ${{ secrets.DATABASE_USER }}
          DATABASE_PASSWORD: ${{ secrets.DATABASE_PASSWORD }}

      - name: Set up Snowflake schemas
        run: |
          echo "â„ï¸ Setting up Snowflake schemas..."
          snowsql -f backend/snowflake_setup/enhanced_data_pipeline_schema.sql
        env:
          SNOWSQL_USER: ${{ env.SNOWFLAKE_USER }}
          SNOWSQL_PWD: ${{ env.SNOWFLAKE_PASSWORD }}
          SNOWSQL_ACCOUNT: ${{ env.SNOWFLAKE_ACCOUNT }}
          SNOWSQL_DATABASE: ${{ env.SNOWFLAKE_DATABASE }}
          SNOWSQL_WAREHOUSE: ${{ env.SNOWFLAKE_WAREHOUSE }}
          SNOWSQL_ROLE: ${{ env.SNOWFLAKE_ROLE }}

      - name: Test database connections
        run: |
          echo "ðŸ” Testing database connections..."
          python -c "
          import asyncio
          import asyncpg
          import redis
          import snowflake.connector
          import os

          async def test_connections():
              # Test PostgreSQL
              try:
                  conn = await asyncpg.connect(
                      host=os.getenv('DATABASE_HOST', 'localhost'),
                      port=int(os.getenv('DATABASE_PORT', 5432)),
                      database=os.getenv('DATABASE_NAME', 'sophia_ai'),
                      user=os.getenv('DATABASE_USER'),
                      password=os.getenv('DATABASE_PASSWORD')
                  )
                  await conn.close()
                  print('âœ… PostgreSQL connection successful')
              except Exception as e:
                  print(f'âŒ PostgreSQL connection failed: {e}')

              # Test Redis
              try:
                  r = redis.Redis(
                      host=os.getenv('REDIS_HOST', 'localhost'),
                      port=int(os.getenv('REDIS_PORT', 6379)),
                      password=os.getenv('REDIS_PASSWORD')
                  )
                  r.ping()
                  print('âœ… Redis connection successful')
              except Exception as e:
                  print(f'âŒ Redis connection failed: {e}')

              # Test Snowflake
              try:
                  conn = snowflake.connector.connect(
                      user=os.getenv('SNOWFLAKE_USER'),
                      password=os.getenv('SNOWFLAKE_PASSWORD'),
                      account=os.getenv('SNOWFLAKE_ACCOUNT'),
                      warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),
                      database=os.getenv('SNOWFLAKE_DATABASE'),
                      role=os.getenv('SNOWFLAKE_ROLE')
                  )
                  conn.close()
                  print('âœ… Snowflake connection successful')
              except Exception as e:
                  print(f'âŒ Snowflake connection failed: {e}')

          asyncio.run(test_connections())
          "

  deploy-data-pipeline:
    name: Deploy Data Pipeline
    runs-on: ubuntu-latest
    needs: [pre-deployment-validation, deploy-databases]
    if: always() && (contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'estuary_flow_pipeline') || contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'all'))

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install uv && uv pip install .[dev]

      - name: Deploy Estuary Flow pipeline
        run: |
          echo "ðŸŒŠ Deploying Estuary Flow data pipeline..."
          python -c "
          import asyncio
          from backend.etl.enhanced_unified_data_pipeline import setup_sophia_data_pipeline

          async def deploy():
              try:
                  result = await setup_sophia_data_pipeline()
                  print('âœ… Data pipeline deployment successful')
                  print(f'Pipeline result: {result}')
              except Exception as e:
                  print(f'âŒ Data pipeline deployment failed: {e}')
                  raise

          asyncio.run(deploy())
          "
        env:
          ESTUARY_ACCESS_TOKEN: ${{ secrets.ESTUARY_ACCESS_TOKEN }}
          HUBSPOT_ACCESS_TOKEN: ${{ secrets.HUBSPOT_ACCESS_TOKEN }}
          GONG_ACCESS_KEY: ${{ secrets.GONG_ACCESS_KEY }}
          GONG_ACCESS_KEY_SECRET: ${{ secrets.GONG_ACCESS_KEY_SECRET }}

      - name: Validate data pipeline
        run: |
          echo "ðŸ” Validating data pipeline..."
          python -c "
          import asyncio
          from backend.etl.enhanced_unified_data_pipeline import get_sophia_pipeline_status

          async def validate():
              try:
                  status = await get_sophia_pipeline_status()
                  print(f'âœ… Pipeline status: {status.status}')
                  print(f'Active sources: {status.sources_active}')
              except Exception as e:
                  print(f'âš ï¸ Pipeline validation warning: {e}')

          asyncio.run(validate())
          "

  deploy-api-services:
    name: Deploy API Services
    runs-on: ubuntu-latest
    needs: [pre-deployment-validation, deploy-data-pipeline]
    if: always() && (contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'api_deployment') || contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'all'))

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Vercel CLI
        run: npm install -g vercel

      - name: Deploy to Vercel
        run: |
          echo "ðŸš€ Deploying to Vercel..."
          vercel --prod --yes --token ${{ secrets.VERCEL_TOKEN }}
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}

      - name: Validate API deployment
        run: |
          echo "ðŸ” Validating API deployment..."
          sleep 30  # Wait for deployment to propagate

          # Test API health endpoint
          RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" https://sophia-main.vercel.app/api/health)
          if [ "$RESPONSE" = "200" ]; then
            echo "âœ… API health check passed"
          else
            echo "âŒ API health check failed with status: $RESPONSE"
            exit 1
          fi

  run-health-checks:
    name: Run Health Checks
    runs-on: ubuntu-latest
    needs: [pre-deployment-validation, deploy-api-services]
    if: always() && !github.event.inputs.skip_tests && (contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'health_checks') || contains(needs.pre-deployment-validation.outputs.components-to-deploy, 'all'))

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install uv && uv pip install .[dev]

      - name: Run comprehensive health checks
        run: |
          echo "ðŸ¥ Running comprehensive health checks..."
          python -c "
          import asyncio
          import aiohttp
          import json

          async def run_health_checks():
              results = {}

              # API health check
              try:
                  async with aiohttp.ClientSession() as session:
                      async with session.get('https://sophia-main.vercel.app/api/health') as response:
                          if response.status == 200:
                              results['api'] = {'status': 'healthy', 'response_time': response.headers.get('X-Response-Time')}
                          else:
                              results['api'] = {'status': 'unhealthy', 'status_code': response.status}
              except Exception as e:
                  results['api'] = {'status': 'error', 'error': str(e)}

              # Database health checks would go here
              # Pipeline health checks would go here

              print('ðŸ¥ Health Check Results:')
              print(json.dumps(results, indent=2))

              # Determine overall health
              healthy_components = sum(1 for r in results.values() if r.get('status') == 'healthy')
              total_components = len(results)
              health_percentage = (healthy_components / total_components) * 100 if total_components > 0 else 0

              print(f'ðŸ“Š Overall Health: {health_percentage:.1f}% ({healthy_components}/{total_components} components healthy)')

              if health_percentage < 80:
                  print('âŒ Health check failed - less than 80% of components are healthy')
                  exit(1)
              else:
                  print('âœ… Health check passed')

          asyncio.run(run_health_checks())
          "

  generate-deployment-report:
    name: Generate Deployment Report
    runs-on: ubuntu-latest
    needs:
      [
        pre-deployment-validation,
        deploy-infrastructure,
        deploy-databases,
        deploy-data-pipeline,
        deploy-api-services,
        run-health-checks,
      ]
    if: always()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate deployment report
        run: |
          echo "ðŸ“Š Generating deployment report..."

          cat > deployment_report.json << EOF
          {
            "deployment_id": "${{ needs.pre-deployment-validation.outputs.deployment-id }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "environment": "${{ github.event.inputs.environment || 'production' }}",
            "instance_type": "${{ github.event.inputs.instance_type || 'gpu_1x_a100' }}",
            "components_deployed": "${{ needs.pre-deployment-validation.outputs.components-to-deploy }}",
            "job_results": {
              "pre_deployment_validation": "${{ needs.pre-deployment-validation.result }}",
              "deploy_infrastructure": "${{ needs.deploy-infrastructure.result }}",
              "deploy_databases": "${{ needs.deploy-databases.result }}",
              "deploy_data_pipeline": "${{ needs.deploy-data-pipeline.result }}",
              "deploy_api_services": "${{ needs.deploy-api-services.result }}",
              "run_health_checks": "${{ needs.run-health-checks.result }}"
            },
            "github_context": {
              "ref": "${{ github.ref }}",
              "sha": "${{ github.sha }}",
              "actor": "${{ github.actor }}",
              "workflow": "${{ github.workflow }}",
              "run_id": "${{ github.run_id }}",
              "run_number": "${{ github.run_number }}"
            }
          }
          EOF

          echo "ðŸ“‹ Deployment Report:"
          cat deployment_report.json | jq '.'

      - name: Upload deployment report
        uses: actions/upload-artifact@v4
        with:
          name: deployment-report-${{ needs.pre-deployment-validation.outputs.deployment-id }}
          path: deployment_report.json

      - name: Determine overall deployment status
        run: |
          echo "ðŸŽ¯ Determining overall deployment status..."

          # Count successful jobs
          SUCCESSFUL_JOBS=0
          TOTAL_JOBS=0

          JOBS=("${{ needs.pre-deployment-validation.result }}" "${{ needs.deploy-infrastructure.result }}" "${{ needs.deploy-databases.result }}" "${{ needs.deploy-data-pipeline.result }}" "${{ needs.deploy-api-services.result }}" "${{ needs.run-health-checks.result }}")

          for job_result in "${JOBS[@]}"; do
            if [ "$job_result" != "skipped" ]; then
              TOTAL_JOBS=$((TOTAL_JOBS + 1))
              if [ "$job_result" = "success" ]; then
                SUCCESSFUL_JOBS=$((SUCCESSFUL_JOBS + 1))
              fi
            fi
          done

          SUCCESS_RATE=$((SUCCESSFUL_JOBS * 100 / TOTAL_JOBS))

          echo "ðŸ“Š Deployment Results:"
          echo "  - Successful jobs: $SUCCESSFUL_JOBS"
          echo "  - Total jobs: $TOTAL_JOBS"
          echo "  - Success rate: $SUCCESS_RATE%"

          if [ $SUCCESS_RATE -ge 80 ]; then
            echo "ðŸŽ‰ Deployment completed successfully!"
            echo "deployment_status=success" >> $GITHUB_ENV
          else
            echo "âŒ Deployment failed or partially failed!"
            echo "deployment_status=failed" >> $GITHUB_ENV
            exit 1
          fi

      - name: Notify deployment completion
        if: always()
        run: |
          echo "ðŸ“¢ Deployment notification:"
          echo "  - Deployment ID: ${{ needs.pre-deployment-validation.outputs.deployment-id }}"
          echo "  - Status: ${{ env.deployment_status || 'failed' }}"
          echo "  - Environment: ${{ github.event.inputs.environment || 'production' }}"
          echo "  - Components: ${{ needs.pre-deployment-validation.outputs.components-to-deploy }}"
          echo "  - Workflow: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
