name: Enhanced Estuary Pipeline Monitoring

on
  schedule
    # Run every 4 hours
    - cron: '0 */4 * * *'
  workflow_dispatch
    inputs
      environment
        description: 'Environment to monitor'
        required: true
        default: 'dev'
        type: choice
        options
          - dev
          - staging
          - prod
      alert_threshold
        description: 'Quality score threshold for alerts'
        required: false
        default: '0.85'
        type: string

env
  PYTHON_VERSION: '3.11'
  PULUMI_ORG: scoobyjava-org

jobs
  monitor-pipeline-health
    name: Monitor Pipeline Health
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'dev' }}

    steps
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.local/bin" >> $GITHUB_PATH
          uv sync
      - name: Set up Pulumi ESC
        env
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
        run: |
          curl -fsSL https://get.pulumi.com | sh
          export PATH=$PATH:$HOME/.pulumi/bin
          echo "$HOME/.pulumi/bin" >> $GITHUB_PATH
          pulumi login
      - name: Perform Comprehensive Health Check
        env
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}
        run: |
          export PULUMI_ORG=${{ env.PULUMI_ORG }}
          python -c "
          import asyncio
          import json
          from datetime import datetime
          from backend.etl.estuary.estuary_configuration_manager import EnhancedEstuaryManager
          async def monitor()
              manager = EnhancedEstuaryManager('${{ github.event.inputs.environment || 'dev' }}')
              await manager.initialize()
              # Perform health check
              health_check = await manager.perform_health_check()

              # Validate data quality
              quality_metrics = await manager.validate_gong_data_quality(200)

              # Create monitoring report
              report = {
                  'timestamp': datetime.utcnow().isoformat(),
                  'environment': '${{ github.event.inputs.environment || 'dev' }}',
                  'health_status': health_check['overall_status'],
                  'health_details': health_check,
                  'data_quality': {
                      'score': quality_metrics.quality_score,
                      'total_records': quality_metrics.total_records,
                      'valid_records': quality_metrics.valid_records,
                      'issues': quality_metrics.issues
                  },
                  'alert_threshold': float('${{ github.event.inputs.alert_threshold || '0.85' }}')
              }
              # Save report
              with open('monitoring_report.json', 'w') as f
                  json.dump(report, f, indent=2)

              # Check for alerts
              alerts = []

              if health_check['overall_status'] == 'critical'
                  alerts.append('CRITICAL: Pipeline health is critical')

              if quality_metrics.quality_score < float('${{ github.event.inputs.alert_threshold || '0.85' }}')
                  alerts.append(f'WARNING: Data quality score {quality_metrics.quality_score:.3f} below threshold')

              if quality_metrics.issues
                  alerts.append(f'WARNING: Data quality issues detected: {', '.join(quality_metrics.issues)}')

              # Log results
              print(f'Health Status: {health_check[\"overall_status\"]}')
              print(f'Data Quality Score: {quality_metrics.quality_score:.3f}')
              print(f'Total Records: {quality_metrics.total_records}')

              if alerts
                  print('ALERTS:')
                  for alert in alerts
                      print(f'  - {alert}')
                  # Write alerts to file for GitHub Actions
                  with open('alerts.txt', 'w') as f
                      f.write('\n'.join(alerts))
              else
                  print('✅ No alerts - pipeline operating normally')
              await manager.cleanup()
          asyncio.run(monitor())
          "
      - name: Upload monitoring report
        uses: actions/upload-artifact@v3
        if: always()
        with
          name: monitoring-report-${{ github.event.inputs.environment || 'dev' }}-${{ github.run_number }}
          path: monitoring_report.json
          retention-days: 90

      - name: Check for alerts
        id: check-alerts
        run: |
          if [ -f alerts.txt ]; then
            echo "alerts_found=true" >> $GITHUB_OUTPUT
            echo "alerts<<EOF" >> $GITHUB_OUTPUT
            cat alerts.txt >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "alerts_found=false" >> $GITHUB_OUTPUT
          fi
      - name: Create GitHub Issue for Critical Alerts
        if: steps.check-alerts.outputs.alerts_found == 'true' && contains(steps.check-alerts.outputs.alerts, 'CRITICAL')
        uses: actions/github-script@v6
        with
          script: |
            const alerts = `${{ steps.check-alerts.outputs.alerts }}`;
            const environment = '${{ github.event.inputs.environment || 'dev' }}';
            const timestamp = new Date().toISOString();

            const issueBody = `
            ## 🚨 Critical Pipeline Alert
            **Environment:** ${environment}
            **Timestamp:** ${timestamp}
            **Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

            ### Alerts
            \`\`\`
            ${alerts}
            \`\`\`
            ### Recommended Actions
            1. Check Estuary Flow connector status
            2. Verify modern_stack connectivity
            3. Review data quality issues
            4. Check system resources and performance
            ### Monitoring Report
            Download the full monitoring report from the workflow artifacts.
            ---
            *This issue was automatically created by the pipeline monitoring workflow.*
            `;
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `🚨 Critical Pipeline Alert - ${environment.toUpperCase()} Environment`,
              body: issueBody,
              labels: ['critical', 'pipeline-alert', `env-${environment}`]
            });

  monitor-data-freshness
    name: Monitor Data Freshness
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'dev' }}

    steps
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.local/bin" >> $GITHUB_PATH
          uv sync
      - name: Set up Pulumi ESC
        env
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
        run: |
          curl -fsSL https://get.pulumi.com | sh
          export PATH=$PATH:$HOME/.pulumi/bin
          echo "$HOME/.pulumi/bin" >> $GITHUB_PATH
          pulumi login
      - name: Check Data Freshness
        env
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
          PULUMI_ORG: ${{ env.PULUMI_ORG }}
        run: |
          export PULUMI_ORG=${{ env.PULUMI_ORG }}
          python -c "
          import asyncio
          from datetime import datetime, timedelta
          from backend.utils.modern_stack_cortex_service import modern_stackCortexService
          async def check_freshness()
              cortex = modern_stackCortexService()
              await cortex.initialize()
              # Check raw data freshness
              raw_freshness_query = '''
                  SELECT
                      MAX(_ESTUARY_EMITTED_AT) as latest_raw_data,
                      COUNT(*) as records_last_24h
                  FROM SOPHIA_AI_DEV.RAW_ESTUARY.RAW_GONG_CALLS_RAW
                  WHERE _ESTUARY_EMITTED_AT >= DATEADD('hour', -24, CURRENT_TIMESTAMP())
              '''
              raw_result = await cortex.execute_query(raw_freshness_query)

              # Check transformed data freshness
              stg_freshness_query = '''
                  SELECT
                      MAX(UPDATED_AT) as latest_stg_data,
                      COUNT(*) as records_last_24h
                  FROM SOPHIA_AI_DEV.STG_TRANSFORMED.STG_GONG_CALLS
                  WHERE UPDATED_AT >= DATEADD('hour', -24, CURRENT_TIMESTAMP())
              '''
              stg_result = await cortex.execute_query(stg_freshness_query)

              # Analyze freshness
              now = datetime.utcnow()

              if len(raw_result) > 0 and raw_result.iloc[0]['LATEST_RAW_DATA']
                  latest_raw = raw_result.iloc[0]['LATEST_RAW_DATA']
                  raw_age_hours = (now - latest_raw).total_seconds() / 3600
                  raw_records = raw_result.iloc[0]['RECORDS_LAST_24H']
              else
                  raw_age_hours = 999
                  raw_records = 0

              if len(stg_result) > 0 and stg_result.iloc[0]['LATEST_STG_DATA']
                  latest_stg = stg_result.iloc[0]['LATEST_STG_DATA']
                  stg_age_hours = (now - latest_stg).total_seconds() / 3600
                  stg_records = stg_result.iloc[0]['RECORDS_LAST_24H']
              else
                  stg_age_hours = 999
                  stg_records = 0

              print(f'Raw Data Freshness: {raw_age_hours:.1f} hours ago ({raw_records} records in 24h)')
              print(f'Transformed Data Freshness: {stg_age_hours:.1f} hours ago ({stg_records} records in 24h)')

              # Alert thresholds
              if raw_age_hours > 6:  # Alert if raw data is older than 6 hours
                  print(f'⚠️  WARNING: Raw data is {raw_age_hours:.1f} hours old')

              if stg_age_hours > 8:  # Alert if transformed data is older than 8 hours
                  print(f'⚠️  WARNING: Transformed data is {stg_age_hours:.1f} hours old')

              if raw_records == 0
                  print('🚨 CRITICAL: No raw data ingested in the last 24 hours')

              if stg_records == 0
                  print('🚨 CRITICAL: No data transformed in the last 24 hours')

              if raw_age_hours <= 6 and stg_age_hours <= 8 and raw_records > 0 and stg_records > 0
                  print('✅ Data freshness is within acceptable limits')
              await cortex.close()
          asyncio.run(check_freshness())
          "

  generate-monitoring-summary
    name: Generate Monitoring Summary
    runs-on: ubuntu-latest
    needs: [monitor-pipeline-health, monitor-data-freshness]
    if: always()

    steps
      - name: Download monitoring artifacts
        uses: actions/download-artifact@v3
        continue-on-error: true
        with
          name: monitoring-report-${{ github.event.inputs.environment || 'dev' }}-${{ github.run_number }}
          path: ./monitoring-artifacts

      - name: Generate Summary
        run: |
          echo "## 📊 Pipeline Monitoring Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ github.event.inputs.environment || 'dev' }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f ./monitoring-artifacts/monitoring_report.json ]; then
            echo "### Health Check Results" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat ./monitoring-artifacts/monitoring_report.json | jq '.health_status, .data_quality' >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ⚠️ Monitoring Report Not Available" >> $GITHUB_STEP_SUMMARY
            echo "The monitoring report could not be generated or downloaded." >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Job Status" >> $GITHUB_STEP_SUMMARY
          echo "- **Pipeline Health:** ${{ needs.monitor-pipeline-health.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Freshness:** ${{ needs.monitor-data-freshness.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Automated monitoring by Enhanced Estuary Pipeline*" >> $GITHUB_STEP_SUMMARY
