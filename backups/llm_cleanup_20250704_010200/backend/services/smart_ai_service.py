from __future__ import annotations

"""
Smart AI Service - Parallel Portkey/OpenRouter Gateway Implementation

Implements enterprise-grade LLM strategy with:
- Portkey as primary gateway with direct provider keys
- OpenRouter as separate parallel service for experimentation
- Performance-prioritized intelligent routing
- Comprehensive cost tracking and logging to Snowflake
- Robust error handling and fallback mechanisms
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 812 lines

Recommended decomposition:
- smart_ai_service_core.py - Core functionality
- smart_ai_service_utils.py - Utility functions
- smart_ai_service_models.py - Data models
- smart_ai_service_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import json
import time
from dataclasses import dataclass
from datetime import UTC, datetime
from enum import Enum
from typing import Any

import aiohttp
import structlog

from backend.core.config_manager import get_config_value
from backend.utils.snowflake_cortex_service import SnowflakeCortexService

logger = structlog.get_logger()


class LLMProvider(str, Enum):
    """LLM Provider types"""

    PORTKEY = "portkey"
    OPENROUTER = "openrouter"
    FALLBACK = "fallback"


class TaskType(str, Enum):
    """Task types for intelligent routing"""

    EXECUTIVE_INSIGHTS = "executive_insights"
    COMPETITIVE_ANALYSIS = "competitive_analysis"
    FINANCIAL_ANALYSIS = "financial_analysis"
    MARKET_ANALYSIS = "market_analysis"
    CODE_GENERATION = "code_generation"
    DOCUMENT_ANALYSIS = "document_analysis"
    CREATIVE_CONTENT = "creative_content"
    ROUTINE_QUERIES = "routine_queries"
    EXPERIMENTAL = "experimental"
    BULK_PROCESSING = "bulk_processing"


class PerformanceTier(str, Enum):
    """Performance tiers for model selection"""

    TIER_1 = "tier_1"  # Premium models for critical tasks
    TIER_2 = "tier_2"  # Balanced performance/cost
    COST_OPTIMIZED = "cost_optimized"  # Cost-focused models


@dataclass
class LLMRequest:
    """Standardized LLM request structure"""

    messages: list[dict[str, str]]
    task_type: TaskType
    model_preference: str | None = None
    temperature: float = 0.7
    max_tokens: int = 2000
    cost_sensitivity: float = 0.5  # 0.0 = cost-focused, 1.0 = performance-focused
    performance_priority: bool = True
    is_experimental: bool = False
    user_id: str = "system"
    session_id: str | None = None
    metadata: dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class LLMResponse:
    """Standardized LLM response structure"""

    content: str
    provider: LLMProvider
    model: str
    cost_usd: float
    latency_ms: int
    tokens_used: int
    cache_hit: bool
    quality_score: float
    error: str | None = None
    request_id: str = ""
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now(UTC)


class SmartAIService:
    """
    Enterprise Smart AI Service with parallel Portkey/OpenRouter strategy

    Features:
    - Performance-prioritized intelligent routing
    - Parallel gateway architecture (not nested)
    - Comprehensive cost tracking and analytics
    - Robust error handling with fallbacks
    - Snowflake logging for enterprise monitoring
    """

    def __init__(self):
        # Gateway configurations
        self.portkey_config = {
            "endpoint": "https://api.portkey.ai/v1/chat/completions",
            "api_key": get_config_value("PORTKEY_API_KEY"),
            "virtual_key": get_config_value("PORTKEY_VIRTUAL_KEY_PROD"),
            "features": {
                "semantic_caching": True,
                "smart_routing": True,
                "load_balancing": True,
                "cost_tracking": True,
            },
        }

        self.openrouter_config = {
            "endpoint": "https://openrouter.ai/api/v1/chat/completions",
            "api_key": get_config_value("OPENROUTER_API_KEY"),
            "features": {
                "model_experimentation": True,
                "niche_models": True,
                "cost_optimization": True,
            },
        }

        # Model tier configurations with performance priority
        self.model_tiers = {
            PerformanceTier.TIER_1: {
                "models": ["gpt-4o", "claude-3-opus", "gemini-1.5-pro"],
                "use_cases": [
                    "executive_insights",
                    "competitive_analysis",
                    "financial_analysis",
                ],
                "cost_multiplier": 1.0,
                "preferred_provider": LLMProvider.PORTKEY,
            },
            PerformanceTier.TIER_2: {
                "models": ["claude-3-haiku", "gpt-4-turbo", "deepseek-v3"],
                "use_cases": [
                    "code_generation",
                    "document_analysis",
                    "routine_queries",
                ],
                "cost_multiplier": 0.6,
                "preferred_provider": LLMProvider.PORTKEY,
            },
            PerformanceTier.COST_OPTIMIZED: {
                "models": ["llama-3-70b", "qwen2-72b", "mixtral-8x22b"],
                "use_cases": ["bulk_processing", "experimental", "creative_content"],
                "cost_multiplier": 0.2,
                "preferred_provider": LLMProvider.OPENROUTER,
            },
        }

        # Strategic model assignments (Unified-configurable)
        self.strategic_assignments = {
            TaskType.EXECUTIVE_INSIGHTS: "gpt-4o",
            TaskType.COMPETITIVE_ANALYSIS: "claude-3-opus",
            TaskType.FINANCIAL_ANALYSIS: "gpt-4o",
            TaskType.MARKET_ANALYSIS: "gemini-1.5-pro",
            TaskType.CODE_GENERATION: "deepseek-v3",
            TaskType.DOCUMENT_ANALYSIS: "claude-3-haiku",
            TaskType.CREATIVE_CONTENT: "mixtral-8x22b",
            TaskType.EXPERIMENTAL: "llama-3-70b",
        }

        # Performance and cost tracking
        self.usage_metrics = {}
        self.session = None
        self.snowflake_service = None
        self.initialized = False

    async def initialize(self) -> None:
        """Initialize the Smart AI Service"""
        if self.initialized:
            return

        try:
            # Initialize HTTP session
            timeout = aiohttp.ClientTimeout(total=30)
            self.session = aiohttp.ClientSession(timeout=timeout)

            # Initialize Snowflake service for logging
            self.snowflake_service = SnowflakeCortexService()

            self.initialized = True
            logger.info("âœ… Smart AI Service initialized with parallel gateway strategy")

        except Exception as e:
            logger.error(f"Failed to initialize Smart AI Service: {e}")
            raise

    async def close(self) -> None:
        """Clean up resources"""
        if self.session:
            await self.session.close()

    async def generate_response(self, request: LLMRequest) -> LLMResponse:
        """
        Generate response using intelligent routing strategy

        Args:
            request: Standardized LLM request

        Returns:
            LLM response with comprehensive metadata
        """
        if not self.initialized:
            await self.initialize()

        start_time = time.time()
        request_id = f"smart_ai_{int(time.time() * 1000)}"

        try:
            # Select optimal strategy
            strategy = await self.select_strategy(request)

            logger.info(
                "ðŸ§  Smart routing decision",
                task_type=request.task_type.value,
                provider=strategy["provider"].value,
                model=strategy["model"],
                reasoning=strategy["reasoning"],
            )

            # Execute request with selected strategy
            if strategy["provider"] == LLMProvider.PORTKEY:
                response = await self._call_portkey(request, strategy, request_id)
            elif strategy["provider"] == LLMProvider.OPENROUTER:
                response = await self._call_openrouter(request, strategy, request_id)
            else:
                response = await self._call_fallback(request, strategy, request_id)

            # Calculate final metrics
            response.latency_ms = int((time.time() - start_time) * 1000)
            response.request_id = request_id

            # Log to Snowflake for enterprise monitoring
            await self._log_usage_to_snowflake(request, response, strategy)

            # Update internal metrics
            await self._update_metrics(response)

            logger.info(
                "âœ… LLM response generated",
                provider=response.provider.value,
                model=response.model,
                latency_ms=response.latency_ms,
                cost_usd=response.cost_usd,
                cache_hit=response.cache_hit,
            )

            return response

        except Exception as e:
            logger.error(f"Error generating LLM response: {e}")

            # Return error response
            return LLMResponse(
                content=f"Error: {str(e)}",
                provider=LLMProvider.FALLBACK,
                model="error",
                cost_usd=0.0,
                latency_ms=int((time.time() - start_time) * 1000),
                tokens_used=0,
                cache_hit=False,
                quality_score=0.0,
                error=str(e),
                request_id=request_id,
            )

    async def select_strategy(self, request: LLMRequest) -> dict[str, Any]:
        """
        Intelligent strategy selection with performance priority

        Args:
            request: LLM request to route

        Returns:
            Strategy configuration with provider, model, and reasoning
        """
        try:
            # Check for explicit model preference
            if request.model_preference:
                model = request.model_preference
                provider = self._get_provider_for_model(model)
                return {
                    "provider": provider,
                    "model": model,
                    "reasoning": f"Explicit model preference: {model}",
                }

            # Check strategic assignments (Unified-configurable)
            if request.task_type in self.strategic_assignments:
                model = self.strategic_assignments[request.task_type]
                provider = self._get_provider_for_model(model)
                return {
                    "provider": provider,
                    "model": model,
                    "reasoning": f"Strategic assignment for {request.task_type.value}",
                }

            # Performance-prioritized routing
            if request.performance_priority and request.cost_sensitivity > 0.7:
                # High performance requirement
                tier = PerformanceTier.TIER_1
                tier_config = self.model_tiers[tier]

                # Select best model for task type
                suitable_models = [
                    model
                    for model in tier_config["models"]
                    if request.task_type.value in tier_config["use_cases"]
                ]

                if suitable_models:
                    model = suitable_models[0]  # Best performance model
                else:
                    model = tier_config["models"][0]  # Default to first tier 1 model

                return {
                    "provider": tier_config["preferred_provider"],
                    "model": model,
                    "reasoning": f"Performance-prioritized: Tier 1 for high-stakes {request.task_type.value}",
                }

            # Experimental routing to OpenRouter
            if request.is_experimental:
                tier = PerformanceTier.COST_OPTIMIZED
                tier_config = self.model_tiers[tier]
                model = tier_config["models"][0]  # First cost-optimized model

                return {
                    "provider": LLMProvider.OPENROUTER,
                    "model": model,
                    "reasoning": "Experimental request routed to OpenRouter for model exploration",
                }

            # Cost-sensitive routing
            if request.cost_sensitivity < 0.3:
                tier = PerformanceTier.COST_OPTIMIZED
                tier_config = self.model_tiers[tier]
                model = tier_config["models"][0]

                return {
                    "provider": tier_config["preferred_provider"],
                    "model": model,
                    "reasoning": f"Cost-optimized routing for {request.task_type.value}",
                }

            # Balanced routing (default)
            tier = PerformanceTier.TIER_2
            tier_config = self.model_tiers[tier]

            # Select appropriate model for task
            suitable_models = [
                model
                for model in tier_config["models"]
                if request.task_type.value in tier_config["use_cases"]
            ]

            model = suitable_models[0] if suitable_models else tier_config["models"][0]

            return {
                "provider": tier_config["preferred_provider"],
                "model": model,
                "reasoning": f"Balanced performance/cost for {request.task_type.value}",
            }

        except Exception as e:
            logger.error(f"Error in strategy selection: {e}")
            # Fallback to safe default
            return {
                "provider": LLMProvider.PORTKEY,
                "model": "gpt-4o",
                "reasoning": f"Fallback due to error: {str(e)}",
            }

    async def _call_portkey(
        self, request: LLMRequest, strategy: dict[str, Any], request_id: str
    ) -> LLMResponse:
        """Call Portkey gateway with direct provider keys"""
        try:
            headers = {
                "Authorization": f"Bearer {self.portkey_config['api_key']}",
                "Content-Type": "application/json",
                "x-portkey-virtual-key": self.portkey_config["virtual_key"],
                "x-portkey-cache": (
                    "semantic" if strategy.get("use_cache", True) else "simple"
                ),
                "x-portkey-trace-id": request_id,
                "x-portkey-metadata": json.dumps(
                    {
                        "task_type": request.task_type.value,
                        "user_id": request.user_id,
                        "session_id": request.session_id,
                        "performance_priority": request.performance_priority,
                    }
                ),
            }

            payload = {
                "model": strategy["model"],
                "messages": request.messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "metadata": {
                    "request_id": request_id,
                    "task_type": request.task_type.value,
                    "routing_strategy": "portkey_direct",
                },
            }

            async with self.session.post(
                self.portkey_config["endpoint"], headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                result = await response.json()

                # Extract response data
                content = result["choices"][0]["message"]["content"]
                usage = result.get("usage", {})

                # Check for cache hit
                cache_hit = response.headers.get("x-portkey-cache-status") == "hit"

                # Calculate cost (estimated)
                cost_usd = await self._calculate_cost(strategy["model"], usage)

                return LLMResponse(
                    content=content,
                    provider=LLMProvider.PORTKEY,
                    model=strategy["model"],
                    cost_usd=cost_usd,
                    latency_ms=0,  # Will be set by caller
                    tokens_used=usage.get("total_tokens", 0),
                    cache_hit=cache_hit,
                    quality_score=0.9,  # High quality for Portkey
                    request_id=request_id,
                )

        except Exception as e:
            logger.error(f"Portkey call failed: {e}")
            # Fallback to OpenRouter
            return await self._call_openrouter(request, strategy, request_id)

    async def _call_openrouter(
        self, request: LLMRequest, strategy: dict[str, Any], request_id: str
    ) -> LLMResponse:
        """Call OpenRouter as parallel service"""
        try:
            headers = {
                "Authorization": f"Bearer {self.openrouter_config['api_key']}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://sophia-intel.ai",
                "X-Title": "Sophia AI Enterprise",
                "X-Request-ID": request_id,
            }

            # Map to OpenRouter model format if needed
            openrouter_model = self._map_to_openrouter_model(strategy["model"])

            payload = {
                "model": openrouter_model,
                "messages": request.messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "metadata": {
                    "request_id": request_id,
                    "task_type": request.task_type.value,
                    "routing_strategy": "openrouter_direct",
                },
            }

            async with self.session.post(
                self.openrouter_config["endpoint"], headers=headers, json=payload
            ) as response:
                response.raise_for_status()
                result = await response.json()

                # Extract response data
                content = result["choices"][0]["message"]["content"]
                usage = result.get("usage", {})

                # Calculate cost from OpenRouter response
                cost_usd = float(result.get("cost", 0.0))

                return LLMResponse(
                    content=content,
                    provider=LLMProvider.OPENROUTER,
                    model=openrouter_model,
                    cost_usd=cost_usd,
                    latency_ms=0,  # Will be set by caller
                    tokens_used=usage.get("total_tokens", 0),
                    cache_hit=False,  # OpenRouter doesn't report cache status
                    quality_score=0.8,  # Good quality for OpenRouter
                    request_id=request_id,
                )

        except Exception as e:
            logger.error(f"OpenRouter call failed: {e}")
            # Final fallback
            return await self._call_fallback(request, strategy, request_id)

    async def _call_fallback(
        self, request: LLMRequest, strategy: dict[str, Any], request_id: str
    ) -> LLMResponse:
        """Fallback to basic response when both gateways fail"""
        logger.warning("Using fallback response - both gateways failed")

        return LLMResponse(
            content="I apologize, but I'm experiencing technical difficulties. Please try again in a moment.",
            provider=LLMProvider.FALLBACK,
            model="fallback",
            cost_usd=0.0,
            latency_ms=0,
            tokens_used=0,
            cache_hit=False,
            quality_score=0.1,
            error="Gateway failures - using fallback",
            request_id=request_id,
        )

    async def _log_usage_to_snowflake(
        self, request: LLMRequest, response: LLMResponse, strategy: dict[str, Any]
    ) -> None:
        """Log comprehensive usage analytics to Snowflake"""
        try:
            if not self.snowflake_service:
                return

            # Prepare analytics record
            analytics_record = {
                "request_id": response.request_id,
                "timestamp": response.timestamp.isoformat(),
                "provider": response.provider.value,
                "model": response.model,
                "task_type": request.task_type.value,
                "user_id": request.user_id,
                "session_id": request.session_id,
                "cost_usd": response.cost_usd,
                "latency_ms": response.latency_ms,
                "tokens_used": response.tokens_used,
                "cache_hit": response.cache_hit,
                "quality_score": response.quality_score,
                "performance_priority": request.performance_priority,
                "cost_sensitivity": request.cost_sensitivity,
                "routing_reasoning": strategy.get("reasoning", ""),
                "error": response.error,
                "metadata": json.dumps(request.metadata or {}),
            }

            # Store in AI_USAGE_ANALYTICS table
            await self.snowflake_service.store_ai_usage_analytics(analytics_record)

        except Exception as e:
            logger.error(f"Failed to log usage to Snowflake: {e}")

    async def _calculate_cost(self, model: str, usage: dict[str, Any]) -> float:
        """Calculate estimated cost based on model and usage"""
        # Model cost per 1K tokens (input/output averaged)
        model_costs = {
            "gpt-4o": 0.015,
            "claude-3-opus": 0.075,
            "gemini-1.5-pro": 0.0125,
            "claude-3-haiku": 0.0025,
            "gpt-4-turbo": 0.01,
            "deepseek-v3": 0.0027,
            "llama-3-70b": 0.0009,
            "qwen2-72b": 0.0009,
            "mixtral-8x22b": 0.0012,
        }

        cost_per_1k = model_costs.get(model, 0.01)  # Default cost
        total_tokens = usage.get("total_tokens", 0)

        return (total_tokens / 1000) * cost_per_1k

    def _get_provider_for_model(self, model: str) -> LLMProvider:
        """Determine which provider typically hosts a model"""
        # Check tier configurations
        for tier_config in self.model_tiers.values():
            if model in tier_config["models"]:
                return tier_config["preferred_provider"]

        # Default to Portkey for unknown models
        return LLMProvider.PORTKEY

    def _map_to_openrouter_model(self, model: str) -> str:
        """Map internal model names to OpenRouter format"""
        openrouter_mapping = {
            "gpt-4o": "openai/gpt-4o",
            "claude-3-opus": "anthropic/claude-3-opus",
            "gemini-1.5-pro": "google/gemini-1.5-pro",
            "claude-3-haiku": "anthropic/claude-3-haiku",
            "deepseek-v3": "deepseek/deepseek-v3",
            "llama-3-70b": "meta-llama/llama-3-70b-instruct",
            "qwen2-72b": "qwen/qwen2-72b-instruct",
            "mixtral-8x22b": "mistralai/mixtral-8x22b-instruct",
        }

        return openrouter_mapping.get(model, model)

    async def _update_metrics(self, response: LLMResponse) -> None:
        """Update internal performance metrics"""
        model_key = f"{response.provider.value}:{response.model}"

        if model_key not in self.usage_metrics:
            self.usage_metrics[model_key] = {
                "total_requests": 0,
                "total_cost": 0.0,
                "total_latency": 0,
                "cache_hits": 0,
                "errors": 0,
                "avg_quality": 0.0,
            }

        metrics = self.usage_metrics[model_key]
        metrics["total_requests"] += 1
        metrics["total_cost"] += response.cost_usd
        metrics["total_latency"] += response.latency_ms

        if response.cache_hit:
            metrics["cache_hits"] += 1

        if response.error:
            metrics["errors"] += 1

        # Update rolling average quality score
        current_avg = metrics["avg_quality"]
        total_requests = metrics["total_requests"]
        metrics["avg_quality"] = (
            current_avg * (total_requests - 1) + response.quality_score
        ) / total_requests

    async def get_usage_analytics(self, time_period_hours: int = 24) -> dict[str, Any]:
        """Get comprehensive usage analytics"""
        try:
            # Calculate summary metrics
            total_requests = sum(
                m["total_requests"] for m in self.usage_metrics.values()
            )
            total_cost = sum(m["total_cost"] for m in self.usage_metrics.values())
            total_cache_hits = sum(m["cache_hits"] for m in self.usage_metrics.values())

            cache_hit_rate = (
                (total_cache_hits / total_requests) if total_requests > 0 else 0
            )
            avg_cost_per_request = (
                (total_cost / total_requests) if total_requests > 0 else 0
            )

            # Provider distribution
            provider_stats = {}
            for model_key, metrics in self.usage_metrics.items():
                provider = model_key.split(":")[0]
                if provider not in provider_stats:
                    provider_stats[provider] = {
                        "requests": 0,
                        "cost": 0.0,
                        "avg_latency": 0,
                    }

                provider_stats[provider]["requests"] += metrics["total_requests"]
                provider_stats[provider]["cost"] += metrics["total_cost"]

                if metrics["total_requests"] > 0:
                    provider_stats[provider]["avg_latency"] += (
                        metrics["total_latency"] / metrics["total_requests"]
                    )

            return {
                "period_hours": time_period_hours,
                "summary": {
                    "total_requests": total_requests,
                    "total_cost_usd": round(total_cost, 4),
                    "avg_cost_per_request": round(avg_cost_per_request, 4),
                    "cache_hit_rate": round(cache_hit_rate, 3),
                    "cost_savings_from_cache": round(
                        total_cost * cache_hit_rate * 0.8, 4
                    ),
                },
                "provider_distribution": provider_stats,
                "model_performance": self.usage_metrics,
                "strategic_assignments": {
                    k.value: v for k, v in self.strategic_assignments.items()
                },
                "gateway_health": {
                    "portkey_available": bool(self.portkey_config["api_key"]),
                    "openrouter_available": bool(self.openrouter_config["api_key"]),
                    "fallback_rate": sum(
                        m["errors"] for m in self.usage_metrics.values()
                    )
                    / max(total_requests, 1),
                },
            }

        except Exception as e:
            logger.error(f"Error generating usage analytics: {e}")
            return {"error": str(e)}

    async def update_strategic_assignment(
        self, task_type: TaskType, model: str
    ) -> bool:
        """Update strategic model assignment (Unified-configurable)"""
        try:
            self.strategic_assignments[task_type] = model
            logger.info(f"Updated strategic assignment: {task_type.value} -> {model}")
            return True
        except Exception as e:
            logger.error(f"Error updating strategic assignment: {e}")
            return False

    def get_available_models(self) -> dict[str, list[str]]:
        """Get available models by tier"""
        return {
            tier.value: config["models"] for tier, config in self.model_tiers.items()
        }


# Global instance
smart_ai_service = SmartAIService()


# Convenience functions for common use cases
async def generate_executive_insight(
    query: str, context: str | None = None, user_id: str = "executive"
) -> str:
    """Generate executive-level insights with premium models"""
    messages = [{"role": "user", "content": query}]
    if context:
        messages.insert(0, {"role": "system", "content": context})

    request = LLMRequest(
        messages=messages,
        task_type=TaskType.EXECUTIVE_INSIGHTS,
        performance_priority=True,
        cost_sensitivity=1.0,  # Performance over cost
        user_id=user_id,
    )

    response = await smart_ai_service.generate_response(request)
    return response.content


async def generate_competitive_analysis(prompt: str, user_id: str = "analyst") -> str:
    """Generate competitive analysis with specialized models"""
    request = LLMRequest(
        messages=[{"role": "user", "content": prompt}],
        task_type=TaskType.COMPETITIVE_ANALYSIS,
        performance_priority=True,
        cost_sensitivity=0.8,
        user_id=user_id,
    )

    response = await smart_ai_service.generate_response(request)
    return response.content


async def generate_code(
    prompt: str, language: str = "python", user_id: str = "developer"
) -> str:
    """Generate code with specialized coding models"""
    enhanced_prompt = f"Generate {language} code for: {prompt}"

    request = LLMRequest(
        messages=[{"role": "user", "content": enhanced_prompt}],
        task_type=TaskType.CODE_GENERATION,
        performance_priority=False,
        cost_sensitivity=0.4,
        user_id=user_id,
        temperature=0.2,  # Lower temperature for code
    )

    response = await smart_ai_service.generate_response(request)
    return response.content


async def experimental_query(
    prompt: str, model: str = "llama-3-70b", user_id: str = "researcher"
) -> str:
    """Run experimental queries on OpenRouter"""
    request = LLMRequest(
        messages=[{"role": "user", "content": prompt}],
        task_type=TaskType.EXPERIMENTAL,
        model_preference=model,
        is_experimental=True,
        performance_priority=False,
        cost_sensitivity=0.1,
        user_id=user_id,
    )

    response = await smart_ai_service.generate_response(request)
    return response.content
