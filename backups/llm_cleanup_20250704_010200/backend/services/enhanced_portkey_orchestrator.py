"""
Enhanced Portkey Orchestrator for Sophia AI
Leverages 11 LLM providers through Portkey virtual keys with intelligent routing

Providers: DeepSeek, OpenAI, Anthropic, Qwen, Grok (XAI), Perplexity,
          Together AI, Groq, Mistral, Cohere + fallbacks

Features:
- Intelligent model selection based on task complexity
- Cost optimization through provider tier routing
- Real-time fallback management
- Cursor IDE integration with MCP support
- Natural language chat interface optimization
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 775 lines

Recommended decomposition:
- enhanced_portkey_orchestrator_core.py - Core functionality
- enhanced_portkey_orchestrator_utils.py - Utility functions
- enhanced_portkey_orchestrator_models.py - Data models
- enhanced_portkey_orchestrator_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import json
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

import aiohttp
import structlog

from backend.core.simple_config import get_config_value

logger = structlog.get_logger()


class ProviderTier(str, Enum):
    """Provider tiers based on cost and capability"""

    ULTRA_FAST = "ultra_fast"  # Groq, Together AI - Speed optimized
    COST_EFFICIENT = "cost_efficient"  # DeepSeek, Qwen - Cost optimized
    PREMIUM = "premium"  # OpenAI GPT-4, Anthropic Claude - Quality
    RESEARCH = "research"  # Perplexity, Grok - Research/reasoning
    SPECIALIZED = "specialized"  # Mistral, Cohere - Domain-specific


class TaskComplexity(str, Enum):
    """Task complexity levels for intelligent routing"""

    SIMPLE = "simple"  # Basic chat, simple questions
    MODERATE = "moderate"  # Code generation, analysis
    COMPLEX = "complex"  # Business analysis, research
    EXPERT = "expert"  # Unified insights, critical decisions
    CREATIVE = "creative"  # Design, content creation
    RESEARCH = "research"  # Deep analysis, fact-checking


@dataclass
class ProviderConfig:
    """Configuration for each provider virtual key"""

    name: str
    virtual_key: str
    tier: ProviderTier
    models: list[str]
    cost_per_1k_tokens: float
    max_tokens: int
    strengths: list[str]
    best_for: list[TaskComplexity]
    rate_limit_rpm: int = 60
    fallback_priority: int = 5


@dataclass
class EnhancedLLMRequest:
    """Enhanced request with intelligent routing metadata"""

    messages: list[dict[str, str]]
    task_complexity: TaskComplexity = TaskComplexity.MODERATE
    preferred_providers: list[str] | None = None
    cost_preference: str = "balanced"  # "cost_optimized", "balanced", "quality_first"
    max_tokens: int = 2000
    temperature: float = 0.7
    stream: bool = False
    user_id: str = "sophia-ai"
    context_type: str = "general"  # "code", "business", "creative", "research"
    urgency: str = "normal"  # "low", "normal", "high", "critical"
    metadata: dict[str, Any] | None = None


@dataclass
class EnhancedLLMResponse:
    """Enhanced response with provider intelligence"""

    content: str
    provider_used: str
    model_used: str
    tokens_used: int
    cost_estimate: float
    processing_time_ms: int
    task_complexity: TaskComplexity
    quality_score: float = 0.0
    reasoning_path: list[str] = field(default_factory=list)
    fallbacks_attempted: list[str] = field(default_factory=list)
    success: bool = True
    error: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)


class EnhancedPortkeyOrchestrator:
    """
    Enhanced Portkey Orchestrator with 11-Provider Intelligence

    Capabilities:
    - Intelligent provider selection based on task complexity
    - Real-time cost optimization across providers
    - Automatic fallback management
    - Performance monitoring and learning
    - Cursor IDE integration with MCP support
    """

    def __init__(self):
        self.base_url = "https://api.portkey.ai/v1"
        self.session: aiohttp.ClientSession | None = None

        # Load virtual keys from config
        self.virtual_keys = self._load_virtual_keys()

        # Provider configurations based on your setup
        self.providers = self._initialize_providers()

        # Intelligence systems
        self.provider_performance = {}  # Track performance metrics
        self.cost_tracker = {}  # Track cost across providers
        self.fallback_history = {}  # Learn from fallback patterns

    def _load_virtual_keys(self) -> dict[str, str]:
        """Load all virtual keys from configuration"""
        return {
            "deepseek": get_config_value("portkey_virtual_key_deepseek"),
            "openai": get_config_value("portkey_virtual_key_openai"),
            "anthropic": get_config_value("portkey_virtual_key_anthropic"),
            "qwen": get_config_value("portkey_virtual_key_qwen"),
            "grok": get_config_value("portkey_virtual_key_grok"),
            "perplexity": get_config_value("portkey_virtual_key_perplexity"),
            "together": get_config_value("portkey_virtual_key_together"),
            "groq": get_config_value("portkey_virtual_key_groq"),
            "mistral": get_config_value("portkey_virtual_key_mistral"),
            "cohere": get_config_value("portkey_virtual_key_cohere"),
            "default": get_config_value("portkey_virtual_key_prod"),
        }

    def _initialize_providers(self) -> dict[str, ProviderConfig]:
        """Initialize provider configurations"""
        return {
            "groq": ProviderConfig(
                name="Groq",
                virtual_key=self.virtual_keys.get("groq", ""),
                tier=ProviderTier.ULTRA_FAST,
                models=["llama-3.1-70b-versatile"],
                cost_per_1k_tokens=0.0002,
                max_tokens=32768,
                strengths=["Ultra-fast inference", "Real-time responses"],
                best_for=[TaskComplexity.SIMPLE, TaskComplexity.MODERATE],
            ),
            "deepseek": ProviderConfig(
                name="DeepSeek",
                virtual_key=self.virtual_keys.get("deepseek", ""),
                tier=ProviderTier.COST_EFFICIENT,
                models=["deepseek-chat", "deepseek-coder"],
                cost_per_1k_tokens=0.00014,
                max_tokens=8192,
                strengths=["Extremely cost-effective", "Strong coding"],
                best_for=[TaskComplexity.SIMPLE, TaskComplexity.MODERATE],
            ),
            "qwen": ProviderConfig(
                name="Qwen",
                virtual_key=self.virtual_keys.get("qwen", ""),
                tier=ProviderTier.COST_EFFICIENT,
                models=["qwen2.5-72b-instruct", "qwen2.5-coder-32b-instruct"],
                cost_per_1k_tokens=0.0005,
                max_tokens=32768,
                strengths=["Excellent multilingual", "Strong reasoning"],
                best_for=[TaskComplexity.MODERATE, TaskComplexity.COMPLEX],
            ),
            "together": ProviderConfig(
                name="Together AI",
                virtual_key=self.virtual_keys.get("together", ""),
                tier=ProviderTier.ULTRA_FAST,
                models=["meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"],
                cost_per_1k_tokens=0.0009,
                max_tokens=8192,
                strengths=["Fast inference", "Open source models"],
                best_for=[TaskComplexity.SIMPLE, TaskComplexity.MODERATE],
            ),
            "anthropic": ProviderConfig(
                name="Anthropic",
                virtual_key=self.virtual_keys.get("anthropic", ""),
                tier=ProviderTier.PREMIUM,
                models=["claude-3.5-sonnet"],
                cost_per_1k_tokens=0.015,
                max_tokens=200000,
                strengths=["Superior reasoning", "Long context", "Safety"],
                best_for=[
                    TaskComplexity.COMPLEX,
                    TaskComplexity.EXPERT,
                    TaskComplexity.CREATIVE,
                ],
            ),
            "openai": ProviderConfig(
                name="OpenAI",
                virtual_key=self.virtual_keys.get("openai", ""),
                tier=ProviderTier.PREMIUM,
                models=["gpt-4o"],
                cost_per_1k_tokens=0.025,
                max_tokens=128000,
                strengths=["Versatile", "Reliable", "Function calling"],
                best_for=[
                    TaskComplexity.MODERATE,
                    TaskComplexity.COMPLEX,
                    TaskComplexity.EXPERT,
                ],
            ),
            "grok": ProviderConfig(
                name="Grok (XAI)",
                virtual_key=self.virtual_keys.get("grok", ""),
                tier=ProviderTier.RESEARCH,
                models=["grok-beta"],
                cost_per_1k_tokens=0.005,
                max_tokens=131072,
                strengths=["Real-time information", "Reasoning", "Humor"],
                best_for=[TaskComplexity.RESEARCH, TaskComplexity.COMPLEX],
            ),
            "perplexity": ProviderConfig(
                name="Perplexity",
                virtual_key=self.virtual_keys.get("perplexity", ""),
                tier=ProviderTier.RESEARCH,
                models=["llama-3.1-sonar-large-128k-online"],
                cost_per_1k_tokens=0.002,
                max_tokens=127072,
                strengths=["Web search", "Real-time data", "Research"],
                best_for=[TaskComplexity.RESEARCH, TaskComplexity.COMPLEX],
            ),
            "mistral": ProviderConfig(
                name="Mistral",
                virtual_key=self.virtual_keys.get("mistral", ""),
                tier=ProviderTier.SPECIALIZED,
                models=["mistral-large-latest", "codestral-latest"],
                cost_per_1k_tokens=0.008,
                max_tokens=32768,
                strengths=["European model", "Coding", "Efficiency"],
                best_for=[TaskComplexity.MODERATE, TaskComplexity.COMPLEX],
            ),
            "cohere": ProviderConfig(
                name="Cohere",
                virtual_key=self.virtual_keys.get("cohere", ""),
                tier=ProviderTier.SPECIALIZED,
                models=["command-r-plus"],
                cost_per_1k_tokens=0.003,
                max_tokens=128000,
                strengths=["RAG", "Embeddings", "Enterprise"],
                best_for=[TaskComplexity.MODERATE, TaskComplexity.COMPLEX],
            ),
        }

    async def initialize(self) -> bool:
        """Initialize the enhanced orchestrator"""
        try:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=60),
                headers={"Content-Type": "application/json"},
            )

            # Test connectivity to top providers
            connectivity_results = await self._test_provider_connectivity()
            active_providers = sum(
                1 for result in connectivity_results.values() if result
            )

            logger.info(
                "Enhanced Portkey Orchestrator initialized",
                active_providers=active_providers,
                total_providers=len(self.providers),
                connectivity=connectivity_results,
            )

            return active_providers > 0

        except Exception as e:
            logger.error(f"Failed to initialize orchestrator: {e}")
            return False

    async def _test_provider_connectivity(self) -> dict[str, bool]:
        """Test connectivity to all providers"""
        results = {}

        if not self.session:
            return dict.fromkeys(self.providers.keys(), False)

        for provider_name, config in self.providers.items():
            if not config.virtual_key:
                results[provider_name] = False
                continue

            try:
                async with self.session.get(
                    f"{self.base_url}/models",
                    headers={"x-portkey-virtual-key": config.virtual_key},
                    timeout=aiohttp.ClientTimeout(total=5),
                ) as response:
                    results[provider_name] = response.status == 200
            except Exception:
                results[provider_name] = False

        return results

    def _select_optimal_provider(self, request: EnhancedLLMRequest) -> str:
        """Intelligent provider selection based on request characteristics"""
        # Filter providers based on task complexity
        suitable_providers = [
            name
            for name, config in self.providers.items()
            if request.task_complexity in config.best_for and config.virtual_key
        ]

        if not suitable_providers:
            # Fallback to any available provider
            suitable_providers = [
                name for name, config in self.providers.items() if config.virtual_key
            ]

        # Apply cost preference
        if request.cost_preference == "cost_optimized":
            # Prefer cost-efficient providers
            cost_efficient = [
                p
                for p in suitable_providers
                if self.providers[p].tier == ProviderTier.COST_EFFICIENT
            ]
            if cost_efficient:
                suitable_providers = cost_efficient
        elif request.cost_preference == "quality_first":
            # Prefer premium providers
            premium = [
                p
                for p in suitable_providers
                if self.providers[p].tier == ProviderTier.PREMIUM
            ]
            if premium:
                suitable_providers = premium

        # Select based on performance history and current load
        return self._select_best_performing_provider(suitable_providers, request)

    def _select_best_performing_provider(
        self, candidates: list[str], request: EnhancedLLMRequest
    ) -> str:
        """Select provider based on performance metrics"""
        if not candidates:
            return "openai"  # Fallback

        # For urgency, prefer ultra-fast providers
        if request.urgency in ["high", "critical"]:
            ultra_fast = [
                p
                for p in candidates
                if self.providers[p].tier == ProviderTier.ULTRA_FAST
            ]
            if ultra_fast:
                return ultra_fast[0]

        # For research tasks, prefer research providers
        if request.task_complexity == TaskComplexity.RESEARCH:
            research = [
                p for p in candidates if self.providers[p].tier == ProviderTier.RESEARCH
            ]
            if research:
                return research[0]

        # Default to first suitable provider (can be enhanced with ML in future)
        return candidates[0]

    async def complete(self, request: EnhancedLLMRequest) -> EnhancedLLMResponse:
        """Enhanced completion with intelligent routing and fallbacks"""
        start_time = time.time()
        fallbacks_attempted = []

        # Select optimal provider
        selected_provider = self._select_optimal_provider(request)
        provider_config = self.providers[selected_provider]

        # Attempt completion with selected provider
        for _attempt in range(3):  # Try up to 3 providers
            try:
                response = await self._attempt_completion(
                    request, selected_provider, provider_config
                )

                if response.success:
                    # Update performance metrics
                    self._update_performance_metrics(
                        selected_provider,
                        time.time() - start_time,
                        response.tokens_used,
                        True,
                    )

                    response.fallbacks_attempted = fallbacks_attempted
                    return response

            except Exception as e:
                logger.warning(f"Provider {selected_provider} failed: {e}")
                fallbacks_attempted.append(f"{selected_provider}: {str(e)}")

                # Select next best provider
                remaining_providers = [
                    name
                    for name in self.providers
                    if name != selected_provider and name not in fallbacks_attempted
                ]

                if remaining_providers:
                    selected_provider = remaining_providers[0]
                    provider_config = self.providers[selected_provider]
                else:
                    break

        # All providers failed
        return EnhancedLLMResponse(
            content="",
            provider_used="none",
            model_used="none",
            tokens_used=0,
            cost_estimate=0.0,
            processing_time_ms=int((time.time() - start_time) * 1000),
            task_complexity=request.task_complexity,
            success=False,
            error="All providers failed",
            fallbacks_attempted=fallbacks_attempted,
        )

    async def _attempt_completion(
        self,
        request: EnhancedLLMRequest,
        provider_name: str,
        provider_config: ProviderConfig,
    ) -> EnhancedLLMResponse:
        """Attempt completion with specific provider"""
        start_time = time.time()

        # Build request
        portkey_request = {
            "messages": request.messages,
            "max_tokens": min(request.max_tokens, provider_config.max_tokens),
            "temperature": request.temperature,
            "stream": request.stream,
        }

        # Add provider-specific model selection
        if provider_config.models:
            portkey_request["model"] = provider_config.models[0]

        headers = {
            "x-portkey-virtual-key": provider_config.virtual_key,
            "x-portkey-cache": "semantic",
            "x-portkey-retry-count": "2",
            "x-portkey-trace-id": f"sophia-{provider_name}-{int(time.time())}",
        }

        if not self.session:
            raise Exception("Session not initialized")

        async with self.session.post(
            f"{self.base_url}/chat/completions", json=portkey_request, headers=headers
        ) as response:
            if response.status != 200:
                error_text = await response.text()
                raise Exception(f"HTTP {response.status}: {error_text}")

            result = await response.json()

            # Extract response data
            content = result["choices"][0]["message"]["content"]
            model_used = result.get(
                "model",
                provider_config.models[0] if provider_config.models else "unknown",
            )
            usage = result.get("usage", {})
            tokens_used = usage.get("total_tokens", 0)

            # Calculate cost
            cost_estimate = (tokens_used / 1000) * provider_config.cost_per_1k_tokens
            processing_time = int((time.time() - start_time) * 1000)

            # Calculate quality score (simplified - can be enhanced)
            quality_score = self._calculate_quality_score(
                content, request.task_complexity
            )

            return EnhancedLLMResponse(
                content=content,
                provider_used=provider_name,
                model_used=model_used,
                tokens_used=tokens_used,
                cost_estimate=cost_estimate,
                processing_time_ms=processing_time,
                task_complexity=request.task_complexity,
                quality_score=quality_score,
                success=True,
                metadata={
                    "provider_tier": provider_config.tier.value,
                    "provider_strengths": provider_config.strengths,
                },
            )

    def _calculate_quality_score(
        self, content: str, complexity: TaskComplexity
    ) -> float:
        """Calculate quality score based on content and task complexity"""
        # Simplified quality scoring - can be enhanced with ML models
        base_score = 0.8

        # Length appropriateness
        if len(content) > 100:
            base_score += 0.1
        if len(content) > 500:
            base_score += 0.1

        # Complexity appropriateness
        if complexity in [TaskComplexity.COMPLEX, TaskComplexity.EXPERT]:
            if len(content) > 300:
                base_score += 0.1

        return min(1.0, base_score)

    def _update_performance_metrics(
        self, provider: str, response_time: float, tokens: int, success: bool
    ):
        """Update performance metrics for provider"""
        if provider not in self.provider_performance:
            self.provider_performance[provider] = {
                "total_requests": 0,
                "successful_requests": 0,
                "total_response_time": 0.0,
                "total_tokens": 0,
                "avg_response_time": 0.0,
                "success_rate": 0.0,
            }

        metrics = self.provider_performance[provider]
        metrics["total_requests"] += 1

        if success:
            metrics["successful_requests"] += 1
            metrics["total_response_time"] += response_time
            metrics["total_tokens"] += tokens

        # Update averages
        if metrics["successful_requests"] > 0:
            metrics["avg_response_time"] = (
                metrics["total_response_time"] / metrics["successful_requests"]
            )
        metrics["success_rate"] = (
            metrics["successful_requests"] / metrics["total_requests"]
        )

    async def get_provider_status(self) -> dict[str, Any]:
        """Get comprehensive provider status"""
        connectivity = await self._test_provider_connectivity()

        status = {
            "total_providers": len(self.providers),
            "active_providers": sum(1 for active in connectivity.values() if active),
            "provider_details": {},
        }

        for name, config in self.providers.items():
            status["provider_details"][name] = {
                "tier": config.tier.value,
                "active": connectivity.get(name, False),
                "models": config.models,
                "cost_per_1k": config.cost_per_1k_tokens,
                "strengths": config.strengths,
                "performance": self.provider_performance.get(name, {}),
            }

        return status

    async def close(self):
        """Clean up resources"""
        if self.session:
            await self.session.close()


# Convenience classes for easy usage
class SophiaAI:
    """Enhanced Sophia AI interface with 11-provider intelligence"""

    _orchestrator: EnhancedPortkeyOrchestrator | None = None

    @classmethod
    async def _get_orchestrator(cls) -> EnhancedPortkeyOrchestrator:
        if cls._orchestrator is None:
            cls._orchestrator = EnhancedPortkeyOrchestrator()
            await cls._orchestrator.initialize()
        return cls._orchestrator

    @classmethod
    async def chat(
        cls,
        message: str,
        complexity: TaskComplexity = TaskComplexity.MODERATE,
        cost_preference: str = "balanced",
        context_type: str = "general",
        urgency: str = "normal",
    ) -> EnhancedLLMResponse:
        """Enhanced chat with intelligent provider routing"""
        orchestrator = await cls._get_orchestrator()

        request = EnhancedLLMRequest(
            messages=[{"role": "user", "content": message}],
            task_complexity=complexity,
            cost_preference=cost_preference,
            context_type=context_type,
            urgency=urgency,
        )

        return await orchestrator.complete(request)

    @classmethod
    async def code_expert(
        cls, requirements: str, language: str = "python"
    ) -> EnhancedLLMResponse:
        """Code generation optimized for best coding models"""
        enhanced_prompt = f"""
Generate high-quality {language} code for the following requirements:

{requirements}

Provide:
1. Complete, production-ready code
2. Comprehensive documentation
3. Error handling
4. Unit tests
5. Performance considerations
"""

        return await cls.chat(
            enhanced_prompt,
            complexity=TaskComplexity.COMPLEX,
            cost_preference="quality_first",
            context_type="code",
            urgency="normal",
        )

    @classmethod
    async def business_analyst(
        cls, query: str, context: dict | None = None
    ) -> EnhancedLLMResponse:
        """Business analysis with premium model routing"""
        enhanced_prompt = f"""
As a senior business analyst, provide comprehensive analysis:

Query: {query}

Context: {json.dumps(context, indent=2) if context else "None provided"}

Provide strategic insights including:
1. Executive summary
2. Key findings and metrics
3. Business implications
4. Risk assessment
5. Strategic recommendations
6. Implementation roadmap
"""

        return await cls.chat(
            enhanced_prompt,
            complexity=TaskComplexity.EXPERT,
            cost_preference="quality_first",
            context_type="business",
            urgency="normal",
        )

    @classmethod
    async def research_assistant(
        cls, topic: str, depth: str = "comprehensive"
    ) -> EnhancedLLMResponse:
        """Research with real-time data providers"""
        research_prompt = f"""
Conduct {depth} research on: {topic}

Provide:
1. Current state analysis
2. Recent developments and trends
3. Key stakeholders and players
4. Market implications
5. Future outlook
6. Data sources and citations
"""

        return await cls.chat(
            research_prompt,
            complexity=TaskComplexity.RESEARCH,
            cost_preference="balanced",
            context_type="research",
            urgency="normal",
        )

    @classmethod
    async def get_status(cls) -> dict[str, Any]:
        """Get comprehensive system status"""
        orchestrator = await cls._get_orchestrator()
        return await orchestrator.get_provider_status()


# Global instance for backward compatibility
enhanced_orchestrator = EnhancedPortkeyOrchestrator()


# Quick access functions
async def sophia_chat(message: str, complexity: str = "moderate") -> str:
    """Ultra-simple chat function"""
    try:
        complexity_enum = TaskComplexity(complexity)
        response = await SophiaAI.chat(message, complexity_enum)
        return response.content if response.success else f"Error: {response.error}"
    except ValueError:
        response = await SophiaAI.chat(message, TaskComplexity.MODERATE)
        return response.content if response.success else f"Error: {response.error}"


if __name__ == "__main__":

    async def test_enhanced_orchestrator():
        """Test the enhanced orchestrator"""

        # Test different complexity levels
        tests = [
            ("Simple chat test", TaskComplexity.SIMPLE, "cost_optimized"),
            (
                "Generate Python code for file processing",
                TaskComplexity.MODERATE,
                "balanced",
            ),
            (
                "Analyze Q4 business performance trends",
                TaskComplexity.EXPERT,
                "quality_first",
            ),
            ("Research latest AI developments", TaskComplexity.RESEARCH, "balanced"),
        ]

        for message, complexity, cost_pref in tests:
            response = await SophiaAI.chat(message, complexity, cost_pref)

            if response.success:
                pass
            else:
                pass

        # Test system status
        status = await SophiaAI.get_status()

        for _name, details in status["provider_details"].items():
            "ðŸŸ¢" if details["active"] else "ðŸ”´"

    asyncio.run(test_enhanced_orchestrator())
