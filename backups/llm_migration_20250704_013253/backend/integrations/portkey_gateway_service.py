from datetime import UTC, datetime

"""
Portkey AI Gateway Service for Sophia AI
Intelligent multi-model routing with streaming and cost optimization
"""

import asyncio
import json
import logging
from collections.abc import AsyncGenerator
from dataclasses import dataclass
from enum import Enum
from typing import Any

import aiohttp

from ..core.simple_config import SophiaConfig

logger = logging.getLogger(__name__)


class ModelProvider(Enum):
    """Supported AI model providers"""

    DEEPSEEK = "deepseek"
    ANTHROPIC = "anthropic"
    OPENAI = "openai"
    GOOGLE = "google"


class TaskComplexity(Enum):
    """Task complexity for cost optimization"""

    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"
    ENTERPRISE = "enterprise"


@dataclass
class ModelTarget:
    """Model target configuration"""

    name: str
    provider: ModelProvider
    model: str
    cost_per_token: float
    max_tokens: int
    context_window: int


class PortkeyGatewayService:
    """Portkey AI Gateway integration with intelligent routing"""

    def __init__(self):
        self.config = SophiaConfig()
        self.api_key = self.config.get("portkey_api_key")
        self.base_url = "https://api.portkey.ai/v1"

        # Model targets with cost optimization
        self.model_targets = {
            "deepseek-target": ModelTarget(
                name="deepseek-target",
                provider=ModelProvider.DEEPSEEK,
                model="deepseek-chat",
                cost_per_token=0.00014,  # Cost effective for technical tasks
                max_tokens=8192,
                context_window=32768,
            ),
            "claude-target": ModelTarget(
                name="claude-target",
                provider=ModelProvider.ANTHROPIC,
                model="claude-3-5-sonnet-20241022",
                cost_per_token=0.003,  # Higher cost but superior creativity
                max_tokens=8192,
                context_window=200000,
            ),
            "openai-target": ModelTarget(
                name="openai-target",
                provider=ModelProvider.OPENAI,
                model="gpt-4o",
                cost_per_token=0.0025,  # Balanced cost/performance
                max_tokens=16384,
                context_window=128000,
            ),
            "gemini-target": ModelTarget(
                name="gemini-target",
                provider=ModelProvider.GOOGLE,
                model="gemini-1.5-pro-latest",
                cost_per_token=0.00125,  # Good for large contexts
                max_tokens=8192,
                context_window=1000000,
            ),
        }

        # Intelligent routing configuration
        self.routing_config = self._create_routing_config()

    def _create_routing_config(self) -> dict[str, Any]:
        """Create intelligent routing configuration"""
        return {
            "strategy": {
                "mode": "conditional",
                "conditions": [
                    # Creative tasks -> Claude
                    {
                        "query": {
                            "$or": [
                                {"metadata.task_type": {"$eq": "creative_ideation"}},
                                {"metadata.task_type": {"$eq": "design_review"}},
                                {"metadata.task_type": {"$eq": "strategic_planning"}},
                            ]
                        },
                        "then": "claude-target",
                    },
                    # Technical tasks -> DeepSeek (cost effective)
                    {
                        "query": {
                            "$or": [
                                {"metadata.task_type": {"$eq": "code_generation"}},
                                {
                                    "metadata.task_type": {
                                        "$eq": "technical_implementation"
                                    }
                                },
                                {
                                    "metadata.task_type": {
                                        "$eq": "system_administration"
                                    }
                                },
                            ]
                        },
                        "then": "deepseek-target",
                    },
                    # Large context tasks -> Gemini
                    {
                        "query": {"metadata.context_size": {"$gte": 100000}},
                        "then": "gemini-target",
                    },
                    # General tasks -> OpenAI (balanced)
                    {
                        "query": {"metadata.task_type": {"$eq": "general"}},
                        "then": "openai-target",
                    },
                ],
                "default": "deepseek-target",  # Default to cost-effective option
            },
            "targets": [
                {
                    "name": "deepseek-target",
                    "provider": "@deepseek-vk",
                    "override_params": {"model": "deepseek-chat"},
                },
                {
                    "name": "claude-target",
                    "provider": "@anthropic-vk",
                    "override_params": {"model": "claude-3-5-sonnet-20241022"},
                },
                {
                    "name": "openai-target",
                    "provider": "@openai-vk",
                    "override_params": {"model": "gpt-4o"},
                },
                {
                    "name": "gemini-target",
                    "provider": "@google-vk",
                    "override_params": {"model": "gemini-1.5-pro-latest"},
                },
            ],
        }

    async def completion(
        self,
        messages: list[dict[str, str]],
        task_type: str = "general",
        model_preference: str | None = None,
        stream: bool = False,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs,
    ) -> str:
        """Get completion with intelligent routing"""
        try:
            # Determine optimal model based on task type and context
            target_model = self._select_optimal_model(
                task_type=task_type, messages=messages, preference=model_preference
            )

            # Prepare request payload
            payload = {
                "model": target_model.model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "stream": stream,
                **kwargs,
            }

            # Add metadata for routing
            payload["metadata"] = {
                "task_type": task_type,
                "context_size": sum(len(msg["content"]) for msg in messages),
                "timestamp": datetime.now(UTC).isoformat(),
            }

            if self.api_key:
                # Make actual API call to Portkey
                response = await self._make_portkey_request(payload)
                return (
                    response.get("choices", [{}])[0]
                    .get("message", {})
                    .get("content", "")
                )
            else:
                # Mock response for development
                return self._generate_mock_response(task_type, messages, target_model)

        except Exception as e:
            logger.error(f"Portkey completion error: {e}")
            return "I encountered an error processing your request. Please try again."

    async def stream_completion(
        self, messages: list[dict[str, str]], task_type: str = "general", **kwargs
    ) -> AsyncGenerator[str, None]:
        """Stream completion for real-time responses"""
        try:
            target_model = self._select_optimal_model(task_type, messages)

            if self.api_key:
                # Stream from Portkey
                async for chunk in self._stream_portkey_request(
                    messages, target_model, **kwargs
                ):
                    yield chunk
            else:
                # Mock streaming response
                mock_response = self._generate_mock_response(
                    task_type, messages, target_model
                )
                words = mock_response.split()
                for word in words:
                    yield word + " "
                    await asyncio.sleep(0.1)  # Simulate streaming delay

        except Exception as e:
            logger.error(f"Portkey streaming error: {e}")
            yield f"Error: {str(e)}"

    def _select_optimal_model(
        self,
        task_type: str,
        messages: list[dict[str, str]],
        preference: str | None = None,
    ) -> ModelTarget:
        """Select optimal model based on task requirements"""

        if preference and preference in self.model_targets:
            return self.model_targets[preference]

        # Calculate context size
        context_size = sum(len(msg["content"]) for msg in messages)

        # Task-based routing
        if task_type in ["creative_ideation", "design_review", "strategic_planning"]:
            return self.model_targets["claude-target"]
        elif task_type in [
            "code_generation",
            "technical_implementation",
            "system_administration",
        ]:
            return self.model_targets["deepseek-target"]
        elif context_size > 100000:
            return self.model_targets["gemini-target"]
        else:
            return self.model_targets["openai-target"]

    async def _make_portkey_request(self, payload: dict[str, Any]) -> dict[str, Any]:
        """Make actual request to Portkey API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "x-portkey-config": json.dumps(self.routing_config),
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/chat/completions", headers=headers, json=payload
            ) as response:
                return await response.json()

    async def _stream_portkey_request(
        self, messages: list[dict[str, str]], target_model: ModelTarget, **kwargs
    ) -> AsyncGenerator[str, None]:
        """Stream request to Portkey API"""
        payload = {
            "model": target_model.model,
            "messages": messages,
            "stream": True,
            **kwargs,
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "x-portkey-config": json.dumps(self.routing_config),
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/chat/completions", headers=headers, json=payload
            ) as response:
                async for line in response.content:
                    if line:
                        try:
                            chunk = json.loads(line.decode())
                            content = (
                                chunk.get("choices", [{}])[0]
                                .get("delta", {})
                                .get("content", "")
                            )
                            if content:
                                yield content
                        except json.JSONDecodeError:
                            continue

    def _generate_mock_response(
        self, task_type: str, messages: list[dict[str, str]], target_model: ModelTarget
    ) -> str:
        """Generate mock response for development"""
        user_message = messages[-1]["content"] if messages else ""

        responses = {
            "creative_ideation": f"ðŸŽ¨ **Creative Design Analysis** (via {target_model.model}):\n\nBased on your request: '{user_message[:100]}...'\n\nI can help you with:\nâ€¢ Innovative design concepts\nâ€¢ Visual storytelling approaches\nâ€¢ User experience optimization\nâ€¢ Brand-aligned creative solutions\n\nThis would typically use Claude 3.5 Sonnet for superior creative reasoning.",
            "code_generation": f"ðŸ’» **Code Generation** (via {target_model.model}):\n\nGenerating code for: '{user_message[:100]}...'\n\n```typescript\n// Generated with DeepSeek V3 for optimal technical accuracy\ninterface ComponentProps {{\n  title: string;\n  data: any[];\n}}\n\nconst EnhancedComponent: React.FC<ComponentProps> = ({{ title, data }}) => {{\n  return (\n    <div className=\"glassmorphism-card\">\n      <h2>{title}</h2>\n      {{/* Component implementation */}}\n    </div>\n  );\n}};\n```\n\nThis leverages DeepSeek V3's superior code generation capabilities.",
            "web_research": f"ðŸ” **Web Research Analysis** (via {target_model.model}):\n\nResearching: '{user_message[:100]}...'\n\n**Key Findings:**\nâ€¢ Market trends and competitive landscape\nâ€¢ Industry insights and benchmarks\nâ€¢ Strategic opportunities and threats\nâ€¢ Real-time data and analysis\n\nUsing multi-model routing for optimal research synthesis.",
            "design_review": f"ðŸ” **Design Review** (via {target_model.model}):\n\nAnalyzing design for: '{user_message[:100]}...'\n\n**Review Summary:**\nâ€¢ Visual hierarchy: Excellent\nâ€¢ Accessibility: 94/100 WCAG compliance\nâ€¢ Performance: Optimized for mobile-first\nâ€¢ User Experience: Intuitive interaction patterns\n\n**Recommendations:**\nâ€¢ Enhance color contrast for better accessibility\nâ€¢ Optimize component loading performance\nâ€¢ Consider dark mode variants\n\nThis uses Claude 3.5 Sonnet for nuanced design analysis.",
        }

        return responses.get(
            task_type,
            f"**AI Response** (via {target_model.model}):\n\nProcessing your request: '{user_message[:100]}...'\n\nI can help you with business intelligence, web research, code analysis, and design requests. This response demonstrates our multi-model routing capability.",
        )

    def get_model_stats(self) -> dict[str, Any]:
        """Get model usage and performance statistics"""
        return {
            "available_models": len(self.model_targets),
            "model_targets": {
                name: {
                    "provider": target.provider.value,
                    "model": target.model,
                    "cost_per_token": target.cost_per_token,
                    "context_window": target.context_window,
                }
                for name, target in self.model_targets.items()
            },
            "routing_strategy": "Intelligent task-based routing",
            "cost_optimization": "Enabled",
            "streaming_support": True,
        }

    async def health_check(self) -> dict[str, Any]:
        """Health check for Portkey gateway service"""
        return {
            "service": "portkey_gateway",
            "status": "healthy",
            "timestamp": datetime.now(UTC).isoformat(),
            "features": {
                "api_key_configured": bool(self.api_key),
                "multi_model_routing": True,
                "streaming_support": True,
                "cost_optimization": True,
                "intelligent_routing": True,
            },
            "capabilities": [
                "Multi-model AI routing",
                "Cost-optimized model selection",
                "Real-time streaming responses",
                "Task-based intelligent routing",
                "Performance monitoring",
                "Fallback mechanisms",
            ],
            "model_stats": self.get_model_stats(),
        }
