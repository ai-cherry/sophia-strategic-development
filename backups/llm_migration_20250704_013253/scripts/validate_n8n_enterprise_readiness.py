#!/usr/bin/env python3
"""
N8N Enterprise Enhancement Readiness Validator
Validates system readiness for enterprise deployment
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 1120 lines

Recommended decomposition:
- validate_n8n_enterprise_readiness_core.py - Core functionality
- validate_n8n_enterprise_readiness_utils.py - Utility functions
- validate_n8n_enterprise_readiness_models.py - Data models
- validate_n8n_enterprise_readiness_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import asyncio
import json
import logging
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ReadinessValidator:
    """Validates system readiness for N8N enterprise enhancement"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.validation_results = {
            "timestamp": datetime.now().isoformat(),
            "overall_ready": False,
            "readiness_score": 0,
            "categories": {},
            "recommendations": [],
        }

    async def validate_full_readiness(self) -> dict[str, Any]:
        """Perform comprehensive readiness validation"""

        logger.info("ðŸ” Starting N8N Enterprise Enhancement Readiness Validation")

        validation_categories = [
            (
                "infrastructure",
                "Infrastructure & Dependencies",
                self.validate_infrastructure,
            ),
            ("existing_n8n", "Existing N8N Integration", self.validate_existing_n8n),
            ("mcp_ecosystem", "MCP Ecosystem", self.validate_mcp_ecosystem),
            (
                "secrets_management",
                "Secrets Management",
                self.validate_secrets_management,
            ),
            ("kubernetes", "Kubernetes Readiness", self.validate_kubernetes),
            ("ai_integration", "AI Integration", self.validate_ai_integration),
            ("monitoring", "Monitoring Capabilities", self.validate_monitoring),
        ]

        total_score = 0
        max_score = 0

        for category_key, category_name, validator_func in validation_categories:
            logger.info(f"\nðŸ“‹ Validating: {category_name}")

            try:
                category_result = await validator_func()
                self.validation_results["categories"][category_key] = category_result

                category_score = category_result.get("score", 0)
                category_max = category_result.get("max_score", 100)

                total_score += category_score
                max_score += category_max

                if category_score >= category_max * 0.8:
                    logger.info(
                        f"âœ… {category_name}: {category_score}/{category_max} (Ready)"
                    )
                else:
                    logger.warning(
                        f"âš ï¸  {category_name}: {category_score}/{category_max} (Needs Attention)"
                    )

            except Exception as e:
                logger.error(f"âŒ {category_name}: Validation failed - {e}")
                self.validation_results["categories"][category_key] = {
                    "score": 0,
                    "max_score": 100,
                    "status": "failed",
                    "error": str(e),
                }

        # Calculate overall readiness
        if max_score > 0:
            overall_score = int((total_score / max_score) * 100)
        else:
            overall_score = 0

        self.validation_results["readiness_score"] = overall_score
        self.validation_results["overall_ready"] = overall_score >= 80

        # Generate recommendations
        self.generate_recommendations()

        # Save validation report
        await self.save_validation_report()

        logger.info(f"\nðŸŽ¯ Overall Readiness Score: {overall_score}/100")

        if self.validation_results["overall_ready"]:
            logger.info("ðŸŽ‰ System is READY for N8N Enterprise Enhancement!")
        else:
            logger.warning("âš ï¸  System needs improvements before deployment")

        return self.validation_results

    async def validate_infrastructure(self) -> dict[str, Any]:
        """Validate basic infrastructure readiness"""

        checks = []
        score = 0
        max_score = 100

        # Check Python version
        try:
            python_version = subprocess.check_output(
                ["python3", "--version"], text=True
            ).strip()
            if "3.11" in python_version or "3.12" in python_version:
                checks.append(
                    {
                        "check": "Python Version",
                        "status": "âœ…",
                        "details": python_version,
                    }
                )
                score += 15
            else:
                checks.append(
                    {
                        "check": "Python Version",
                        "status": "âš ï¸",
                        "details": f"{python_version} (Recommend 3.11+)",
                    }
                )
                score += 10
        except Exception as e:
            checks.append(
                {"check": "Python Version", "status": "âŒ", "details": f"Failed: {e}"}
            )

        # Check Docker
        try:
            docker_version = subprocess.check_output(
                ["docker", "--version"], text=True
            ).strip()
            checks.append({"check": "Docker", "status": "âœ…", "details": docker_version})
            score += 20
        except Exception as e:
            checks.append(
                {"check": "Docker", "status": "âŒ", "details": f"Docker not found: {e}"}
            )

        # Check kubectl
        try:
            kubectl_version = subprocess.check_output(
                ["kubectl", "version", "--client", "--short"], text=True
            ).strip()
            checks.append(
                {"check": "Kubectl", "status": "âœ…", "details": kubectl_version}
            )
            score += 20
        except Exception as e:
            checks.append(
                {
                    "check": "Kubectl",
                    "status": "âš ï¸",
                    "details": f"Kubectl not found: {e}",
                }
            )
            score += 10

        # Check Helm
        try:
            helm_version = subprocess.check_output(
                ["helm", "version", "--short"], text=True
            ).strip()
            checks.append({"check": "Helm", "status": "âœ…", "details": helm_version})
            score += 20
        except Exception as e:
            checks.append(
                {"check": "Helm", "status": "âš ï¸", "details": f"Helm not found: {e}"}
            )
            score += 10

        # Check required Python packages
        required_packages = ["httpx", "pydantic", "PyYAML", "kubernetes"]
        for package in required_packages:
            try:
                result = subprocess.run(
                    ["python3", "-c", f"import {package}"],
                    capture_output=True,
                    text=True,
                )
                if result.returncode == 0:
                    checks.append(
                        {
                            "check": f"Python {package}",
                            "status": "âœ…",
                            "details": "Available",
                        }
                    )
                    score += 5
                else:
                    checks.append(
                        {
                            "check": f"Python {package}",
                            "status": "âŒ",
                            "details": "Not installed",
                        }
                    )
            except Exception as e:
                checks.append(
                    {
                        "check": f"Python {package}",
                        "status": "âŒ",
                        "details": f"Check failed: {e}",
                    }
                )

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    async def validate_existing_n8n(self) -> dict[str, Any]:
        """Validate existing N8N integration"""

        checks = []
        score = 0
        max_score = 100

        # Check N8N directory structure
        n8n_dir = self.project_root / "n8n-integration"
        if n8n_dir.exists():
            checks.append(
                {"check": "N8N Directory", "status": "âœ…", "details": str(n8n_dir)}
            )
            score += 20

            # Check docker-compose.yml
            docker_compose = n8n_dir / "docker-compose.yml"
            if docker_compose.exists():
                checks.append(
                    {
                        "check": "Docker Compose",
                        "status": "âœ…",
                        "details": "Configuration exists",
                    }
                )
                score += 15
            else:
                checks.append(
                    {
                        "check": "Docker Compose",
                        "status": "âš ï¸",
                        "details": "No docker-compose.yml found",
                    }
                )
                score += 5

            # Check workflows directory
            workflows_dir = n8n_dir / "workflows"
            if workflows_dir.exists():
                workflow_count = len(list(workflows_dir.glob("*.json")))
                checks.append(
                    {
                        "check": "Workflows",
                        "status": "âœ…",
                        "details": f"{workflow_count} workflows found",
                    }
                )
                score += 15
            else:
                checks.append(
                    {
                        "check": "Workflows",
                        "status": "âš ï¸",
                        "details": "No workflows directory",
                    }
                )
                score += 5
        else:
            checks.append(
                {
                    "check": "N8N Directory",
                    "status": "âŒ",
                    "details": "N8N integration not found",
                }
            )

        # Check N8N bridge service
        bridge_service = self.project_root / "backend" / "n8n_bridge" / "main.py"
        if bridge_service.exists():
            checks.append(
                {
                    "check": "N8N Bridge Service",
                    "status": "âœ…",
                    "details": "Bridge service exists",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "N8N Bridge Service",
                    "status": "âš ï¸",
                    "details": "Bridge service not found",
                }
            )
            score += 10

        # Check enhanced CLI manager
        cli_manager = (
            self.project_root / "n8n-integration" / "enhanced_n8n_cli_manager.py"
        )
        if cli_manager.exists():
            checks.append(
                {
                    "check": "Enhanced CLI Manager",
                    "status": "âœ…",
                    "details": "CLI manager available",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Enhanced CLI Manager",
                    "status": "âš ï¸",
                    "details": "CLI manager not found",
                }
            )
            score += 10

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    async def validate_mcp_ecosystem(self) -> dict[str, Any]:
        """Validate MCP ecosystem readiness"""

        checks = []
        score = 0
        max_score = 100

        # Check MCP servers directory
        mcp_dir = self.project_root / "mcp-servers"
        if mcp_dir.exists():
            server_count = len(
                [
                    d
                    for d in mcp_dir.iterdir()
                    if d.is_dir() and not d.name.startswith(".")
                ]
            )
            checks.append(
                {
                    "check": "MCP Servers",
                    "status": "âœ…",
                    "details": f"{server_count} servers found",
                }
            )
            score += 30
        else:
            checks.append(
                {
                    "check": "MCP Servers",
                    "status": "âŒ",
                    "details": "MCP servers directory not found",
                }
            )

        # Check MCP orchestration service
        orchestration_service = (
            self.project_root / "backend" / "services" / "mcp_orchestration_service.py"
        )
        if orchestration_service.exists():
            checks.append(
                {
                    "check": "MCP Orchestration",
                    "status": "âœ…",
                    "details": "Orchestration service exists",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "MCP Orchestration",
                    "status": "âš ï¸",
                    "details": "Orchestration service not found",
                }
            )
            score += 10

        # Check MCP configuration
        mcp_config = self.project_root / "config" / "cursor_enhanced_mcp_config.json"
        if mcp_config.exists():
            try:
                with open(mcp_config) as f:
                    config_data = json.load(f)
                    server_count = len(config_data.get("mcpServers", {}))
                    checks.append(
                        {
                            "check": "MCP Configuration",
                            "status": "âœ…",
                            "details": f"{server_count} servers configured",
                        }
                    )
                    score += 25
            except Exception as e:
                checks.append(
                    {
                        "check": "MCP Configuration",
                        "status": "âš ï¸",
                        "details": f"Config error: {e}",
                    }
                )
                score += 10
        else:
            checks.append(
                {
                    "check": "MCP Configuration",
                    "status": "âš ï¸",
                    "details": "MCP config not found",
                }
            )
            score += 10

        # Check AI Memory MCP server
        ai_memory_server = self.project_root / "mcp-servers" / "ai_memory"
        if ai_memory_server.exists():
            checks.append(
                {
                    "check": "AI Memory Server",
                    "status": "âœ…",
                    "details": "AI Memory MCP server available",
                }
            )
            score += 20
        else:
            checks.append(
                {
                    "check": "AI Memory Server",
                    "status": "âš ï¸",
                    "details": "AI Memory server not found",
                }
            )
            score += 5

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    async def validate_secrets_management(self) -> dict[str, Any]:
        """Validate secrets management readiness"""

        checks = []
        score = 0
        max_score = 100

        # Check Pulumi ESC configuration
        auto_esc_config = self.project_root / "backend" / "core" / "auto_esc_config.py"
        if auto_esc_config.exists():
            checks.append(
                {
                    "check": "Auto ESC Config",
                    "status": "âœ…",
                    "details": "ESC integration available",
                }
            )
            score += 30
        else:
            checks.append(
                {
                    "check": "Auto ESC Config",
                    "status": "âŒ",
                    "details": "ESC config not found",
                }
            )

        # Check environment variables
        required_env_vars = ["ENVIRONMENT", "PULUMI_ORG"]
        for var in required_env_vars:
            if os.getenv(var):
                checks.append(
                    {"check": f"ENV {var}", "status": "âœ…", "details": f"{var} is set"}
                )
                score += 15
            else:
                checks.append(
                    {"check": f"ENV {var}", "status": "âš ï¸", "details": f"{var} not set"}
                )
                score += 5

        # Check ESC infrastructure
        esc_dir = self.project_root / "infrastructure" / "esc"
        if esc_dir.exists():
            esc_files = list(esc_dir.glob("*.yaml"))
            checks.append(
                {
                    "check": "ESC Infrastructure",
                    "status": "âœ…",
                    "details": f"{len(esc_files)} ESC files found",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "ESC Infrastructure",
                    "status": "âš ï¸",
                    "details": "ESC infrastructure not found",
                }
            )
            score += 10

        # Check secret sync scripts
        sync_script = self.project_root / "scripts" / "ci" / "sync_from_gh_to_pulumi.py"
        if sync_script.exists():
            checks.append(
                {
                    "check": "Secret Sync Script",
                    "status": "âœ…",
                    "details": "GitHubâ†’Pulumi sync available",
                }
            )
            score += 15
        else:
            checks.append(
                {
                    "check": "Secret Sync Script",
                    "status": "âš ï¸",
                    "details": "Sync script not found",
                }
            )
            score += 5

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    async def validate_kubernetes(self) -> dict[str, Any]:
        """Validate Kubernetes readiness"""

        checks = []
        score = 0
        max_score = 100

        # Check Kubernetes infrastructure directory
        k8s_dir = self.project_root / "infrastructure" / "kubernetes"
        if k8s_dir.exists():
            k8s_files = list(k8s_dir.glob("*.yaml"))
            checks.append(
                {
                    "check": "K8s Infrastructure",
                    "status": "âœ…",
                    "details": f"{len(k8s_files)} manifest files",
                }
            )
            score += 20
        else:
            checks.append(
                {
                    "check": "K8s Infrastructure",
                    "status": "âš ï¸",
                    "details": "K8s directory not found",
                }
            )
            score += 5

        # Check charts directory
        charts_dir = self.project_root / "charts"
        if charts_dir.exists():
            charts = [d for d in charts_dir.iterdir() if d.is_dir()]
            checks.append(
                {
                    "check": "Helm Charts",
                    "status": "âœ…",
                    "details": f"{len(charts)} charts available",
                }
            )
            score += 20
        else:
            checks.append(
                {
                    "check": "Helm Charts",
                    "status": "âš ï¸",
                    "details": "Charts directory not found",
                }
            )
            score += 5

        # Check kubectl connectivity
        try:
            result = subprocess.run(
                ["kubectl", "cluster-info"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                checks.append(
                    {
                        "check": "K8s Connectivity",
                        "status": "âœ…",
                        "details": "Cluster accessible",
                    }
                )
                score += 30
            else:
                checks.append(
                    {
                        "check": "K8s Connectivity",
                        "status": "âš ï¸",
                        "details": "Cluster not accessible",
                    }
                )
                score += 10
        except Exception as e:
            checks.append(
                {
                    "check": "K8s Connectivity",
                    "status": "âš ï¸",
                    "details": f"Cannot check: {e}",
                }
            )
            score += 10

        # Check namespace
        try:
            result = subprocess.run(
                ["kubectl", "get", "namespace", "sophia-ai"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                checks.append(
                    {
                        "check": "Sophia AI Namespace",
                        "status": "âœ…",
                        "details": "Namespace exists",
                    }
                )
                score += 15
            else:
                checks.append(
                    {
                        "check": "Sophia AI Namespace",
                        "status": "âš ï¸",
                        "details": "Namespace not found",
                    }
                )
                score += 5
        except Exception as e:
            checks.append(
                {
                    "check": "Sophia AI Namespace",
                    "status": "âš ï¸",
                    "details": f"Cannot check: {e}",
                }
            )
            score += 5

        # Check storage class
        try:
            result = subprocess.run(
                ["kubectl", "get", "storageclass"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                checks.append(
                    {
                        "check": "Storage Classes",
                        "status": "âœ…",
                        "details": "Storage available",
                    }
                )
                score += 15
            else:
                checks.append(
                    {
                        "check": "Storage Classes",
                        "status": "âš ï¸",
                        "details": "Storage check failed",
                    }
                )
                score += 5
        except Exception as e:
            checks.append(
                {
                    "check": "Storage Classes",
                    "status": "âš ï¸",
                    "details": f"Cannot check: {e}",
                }
            )
            score += 5

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    async def validate_ai_integration(self) -> dict[str, Any]:
        """Validate AI integration readiness"""

        checks = []
        score = 0
        max_score = 100

        # Check Portkey gateway service
        portkey_service = (
            self.project_root / "backend" / "services" / "portkey_ai_gateway.py"
        )
        if portkey_service.exists():
            checks.append(
                {
                    "check": "Portkey Gateway Service",
                    "status": "âœ…",
                    "details": "Service implementation ready",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Portkey Gateway Service",
                    "status": "âš ï¸",
                    "details": "Service not implemented",
                }
            )
            score += 10

        # Check AI cost optimization config
        cost_config = self.project_root / "config" / "ai_cost_optimization.json"
        if cost_config.exists():
            checks.append(
                {
                    "check": "Cost Optimization Config",
                    "status": "âœ…",
                    "details": "Cost config available",
                }
            )
            score += 20
        else:
            checks.append(
                {
                    "check": "Cost Optimization Config",
                    "status": "âš ï¸",
                    "details": "Cost config not found",
                }
            )
            score += 5

        # Check Snowflake Cortex integration
        cortex_service = (
            self.project_root / "backend" / "services" / "snowflake_cortex_service.py"
        )
        if cortex_service.exists():
            checks.append(
                {
                    "check": "Snowflake Cortex",
                    "status": "âœ…",
                    "details": "Cortex integration available",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Snowflake Cortex",
                    "status": "âš ï¸",
                    "details": "Cortex service not found",
                }
            )
            score += 10

        # Check executive intelligence workflows
        executive_workflow = (
            self.project_root
            / "n8n-integration"
            / "workflows"
            / "executive_intelligence_enhanced.json"
        )
        if executive_workflow.exists():
            checks.append(
                {
                    "check": "Executive Workflows",
                    "status": "âœ…",
                    "details": "Executive intelligence ready",
                }
            )
            score += 30
        else:
            checks.append(
                {
                    "check": "Executive Workflows",
                    "status": "âš ï¸",
                    "details": "Executive workflows not found",
                }
            )
            score += 10

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    async def validate_monitoring(self) -> dict[str, Any]:
        """Validate monitoring capabilities"""

        checks = []
        score = 0
        max_score = 100

        # Check monitoring configuration
        monitoring_dir = self.project_root / "monitoring"
        if monitoring_dir.exists():
            monitoring_files = list(monitoring_dir.rglob("*.json")) + list(
                monitoring_dir.rglob("*.yaml")
            )
            checks.append(
                {
                    "check": "Monitoring Config",
                    "status": "âœ…",
                    "details": f"{len(monitoring_files)} monitoring files",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Monitoring Config",
                    "status": "âš ï¸",
                    "details": "Monitoring directory not found",
                }
            )
            score += 10

        # Check Grafana dashboards
        dashboards_dir = self.project_root / "monitoring" / "dashboards"
        if dashboards_dir.exists():
            dashboard_count = len(list(dashboards_dir.glob("*.json")))
            checks.append(
                {
                    "check": "Grafana Dashboards",
                    "status": "âœ…",
                    "details": f"{dashboard_count} dashboards",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Grafana Dashboards",
                    "status": "âš ï¸",
                    "details": "Dashboards not found",
                }
            )
            score += 10

        # Check performance monitoring
        perf_monitor = self.project_root / "backend" / "monitoring"
        if perf_monitor.exists():
            perf_files = list(perf_monitor.glob("*.py"))
            checks.append(
                {
                    "check": "Performance Monitoring",
                    "status": "âœ…",
                    "details": f"{len(perf_files)} monitoring modules",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Performance Monitoring",
                    "status": "âš ï¸",
                    "details": "Performance monitoring not found",
                }
            )
            score += 10

        # Check health check endpoints
        health_checks = []
        for service_dir in ["backend/app", "backend/services", "mcp-servers"]:
            service_path = self.project_root / service_dir
            if service_path.exists():
                health_files = list(service_path.rglob("*health*"))
                health_checks.extend(health_files)

        if health_checks:
            checks.append(
                {
                    "check": "Health Checks",
                    "status": "âœ…",
                    "details": f"{len(health_checks)} health check implementations",
                }
            )
            score += 25
        else:
            checks.append(
                {
                    "check": "Health Checks",
                    "status": "âš ï¸",
                    "details": "Health checks not found",
                }
            )
            score += 10

        return {
            "score": min(score, max_score),
            "max_score": max_score,
            "status": "ready" if score >= 80 else "needs_attention",
            "checks": checks,
        }

    def generate_recommendations(self):
        """Generate actionable recommendations based on validation results"""

        recommendations = []

        for category_key, category_data in self.validation_results[
            "categories"
        ].items():
            if (
                category_data.get("score", 0)
                < category_data.get("max_score", 100) * 0.8
            ):
                if category_key == "infrastructure":
                    recommendations.append(
                        {
                            "priority": "high",
                            "category": "Infrastructure",
                            "action": "Install missing dependencies",
                            "details": "Run: pip install httpx pydantic PyYAML kubernetes && install kubectl helm",
                        }
                    )

                elif category_key == "kubernetes":
                    recommendations.append(
                        {
                            "priority": "high",
                            "category": "Kubernetes",
                            "action": "Setup Kubernetes cluster access",
                            "details": "Configure kubectl and create sophia-ai namespace: kubectl create namespace sophia-ai",
                        }
                    )

                elif category_key == "existing_n8n":
                    recommendations.append(
                        {
                            "priority": "medium",
                            "category": "N8N Integration",
                            "action": "Deploy basic N8N integration",
                            "details": "Run: cd n8n-integration && ./quick_deploy_n8n.sh",
                        }
                    )

                elif category_key == "secrets_management":
                    recommendations.append(
                        {
                            "priority": "high",
                            "category": "Secrets",
                            "action": "Configure secret management",
                            "details": "Set ENVIRONMENT=prod and PULUMI_ORG=scoobyjava-org environment variables",
                        }
                    )

                elif category_key == "ai_integration":
                    recommendations.append(
                        {
                            "priority": "medium",
                            "category": "AI Integration",
                            "action": "Setup AI gateway services",
                            "details": "Configure Portkey credentials and deploy AI gateway components",
                        }
                    )

                elif category_key == "monitoring":
                    recommendations.append(
                        {
                            "priority": "low",
                            "category": "Monitoring",
                            "action": "Deploy monitoring stack",
                            "details": "Setup Prometheus and Grafana for comprehensive monitoring",
                        }
                    )

        # Add general recommendations
        if self.validation_results["readiness_score"] < 60:
            recommendations.insert(
                0,
                {
                    "priority": "critical",
                    "category": "General",
                    "action": "Address critical infrastructure gaps",
                    "details": "Focus on infrastructure, Kubernetes, and secrets management first",
                },
            )

        self.validation_results["recommendations"] = recommendations

    async def save_validation_report(self):
        """Save validation report to file"""

        report_file = (
            self.project_root
            / f"N8N_ENTERPRISE_READINESS_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        with open(report_file, "w") as f:
            json.dump(self.validation_results, f, indent=2)

        logger.info(f"ðŸ“„ Validation report saved: {report_file}")

    def print_summary(self):
        """Print validation summary"""

        print("\n" + "=" * 80)
        print("ðŸ” N8N ENTERPRISE ENHANCEMENT READINESS VALIDATION SUMMARY")
        print("=" * 80)

        print(
            f"\nðŸ“Š Overall Readiness Score: {self.validation_results['readiness_score']}/100"
        )

        if self.validation_results["overall_ready"]:
            print("ðŸŽ‰ Status: READY FOR DEPLOYMENT")
        else:
            print("âš ï¸  Status: NEEDS ATTENTION BEFORE DEPLOYMENT")

        print("\nðŸ“‹ Category Breakdown:")
        for category_key, category_data in self.validation_results[
            "categories"
        ].items():
            score = category_data.get("score", 0)
            max_score = category_data.get("max_score", 100)
            status = category_data.get("status", "unknown")

            status_emoji = (
                "âœ…"
                if status == "ready"
                else "âš ï¸"
                if status == "needs_attention"
                else "âŒ"
            )
            print(
                f"  {status_emoji} {category_key.replace('_', ' ').title()}: {score}/{max_score}"
            )

        if self.validation_results["recommendations"]:
            print("\nðŸŽ¯ Recommendations:")
            for i, rec in enumerate(self.validation_results["recommendations"][:5], 1):
                priority_emoji = (
                    "ðŸ”´"
                    if rec["priority"] == "critical"
                    else "ðŸŸ¡"
                    if rec["priority"] == "high"
                    else "ðŸŸ¢"
                )
                print(f"  {i}. {priority_emoji} {rec['action']}")
                print(f"     {rec['details']}")

        print("\nðŸš€ Next Steps:")
        if self.validation_results["overall_ready"]:
            print("  1. Run: python scripts/deploy_n8n_enterprise_enhancement.py")
            print("  2. Monitor deployment progress")
            print("  3. Validate deployment success")
        else:
            print("  1. Address high-priority recommendations")
            print(
                "  2. Re-run validation: python scripts/validate_n8n_enterprise_readiness.py"
            )
            print("  3. Proceed with deployment when ready")

        print("\n" + "=" * 80)


async def main():
    """Main validation function"""

    validator = ReadinessValidator()

    # Run comprehensive validation
    results = await validator.validate_full_readiness()

    # Print summary
    validator.print_summary()

    return results


if __name__ == "__main__":
    asyncio.run(main())
