## ðŸŒŸ **NEW: The System Handbook is the Ultimate Source of Truth**

**CRITICAL DIRECTIVE:** The `docs/system_handbook/` directory, and specifically `00_SOPHIA_AI_SYSTEM_HANDBOOK.md`, is the **single, definitive source of truth** for the Sophia AI platform's architecture, standards, and roadmap.

### **Your Core Responsibilities Regarding Documentation:**

1.  **CONSULT FIRST, CODE LATER:** Before answering any architectural question or implementing any new feature, you **MUST** first consult the System Handbook to understand the established patterns and principles.
2.  **PROACTIVELY UPDATE:** If you make a significant architectural change (e.g., add a new service, change a data flow, modify the MCP gateway), you **MUST** update the relevant section of the System Handbook within the same session.
3.  **MAINTAIN STRUCTURE:** When adding documentation, you **MUST** follow the existing structure and format. If a new deep-dive document is needed, create it within the `system_handbook` directory and link to it from the master file.
4.  **CITE YOUR SOURCE:** When you use information from the handbook to make a decision, you should briefly mention it (e.g., "Based on the Phoenix Plan architecture outlined in the System Handbook, I will...").

This ensures our documentation remains a living, breathing, and accurate representation of the platform, empowering both human and AI developers.

# Sophia AI Pay Ready Platform - Cursor AI Rules

## Project Overview
You are working on Sophia AI, an AI assistant orchestrator for Pay Ready company. Sophia serves as the central "Pay Ready Brain" that orchestrates multiple AI agents and integrates with business systems.

## ðŸŒŸ **NEW: The System Handbook is the Ultimate Source of Truth**

**CRITICAL DIRECTIVE:** The `docs/system_handbook/` directory, and specifically `00_SOPHIA_AI_SYSTEM_HANDBOOK.md`, is the **single, definitive source of truth** for the Sophia AI platform's architecture, standards, and roadmap.

### **Your Core Responsibilities Regarding Documentation:**

1.  **CONSULT FIRST, CODE LATER:** Before answering any architectural question or implementing any new feature, you **MUST** first consult the System Handbook to understand the established patterns and principles.
2.  **PROACTIVELY UPDATE:** If you make a significant architectural change (e.g., add a new service, change a data flow, modify the MCP gateway), you **MUST** update the relevant section of the System Handbook within the same session.
3.  **MAINTAIN STRUCTURE:** When adding documentation, you **MUST** follow the existing structure and format. If a new deep-dive document is needed, create it within the `system_handbook` directory and link to it from the master file.
4.  **CITE YOUR SOURCE:** When you use information from the handbook to make a decision, you should briefly mention it (e.g., "Based on the Phoenix Plan architecture outlined in the System Handbook, I will...").

This ensures our documentation remains a living, breathing, and accurate representation of the platform, empowering both human and AI developers.

## Architecture Context
- **Type:** Multi-agent AI orchestrator with flat-to-hierarchical evolution
- **Primary Role:** Business intelligence and automation for Pay Ready
- **Core Integrations:** HubSpot CRM, Gong.io call analysis, Slack communication
- **Data Stack:** PostgreSQL, Redis, Pinecone, Weaviate
- **Infrastructure:** Lambda Labs servers, Vercel frontend deployment
- **ðŸ†• External Repository Collection:** 11 strategic MCP servers (22k+ combined stars)

## ðŸ” **PERMANENT SECRET MANAGEMENT SOLUTION**

### **CRITICAL: GitHub Organization Secrets Integration**
Sophia AI uses a **PERMANENT** secret management solution that eliminates manual `.env` management:

```
GitHub Organization Secrets (ai-cherry)
           â†“
    GitHub Actions (automatic sync)
           â†“
    Pulumi ESC Environments
           â†“
    Sophia AI Backend (automatic loading)
```

### **âœ… What's Automated**
- **Zero Manual Secret Management**: No more `.env` file management
- **Organization-Level Secrets**: All secrets in [GitHub ai-cherry org](https://github.com/ai-cherry)
- **Automatic Sync**: GitHub Actions â†’ Pulumi ESC â†’ Backend
- **Enterprise Security**: No exposed credentials anywhere
- **Forever Solution**: Works automatically without intervention

### **ðŸ”‘ Secret Access Pattern**
```python
# Backend automatically loads from Pulumi ESC
from backend.core.auto_esc_config import config

# Secrets are automatically available
openai_key = config.openai_api_key
gong_key = config.gong_access_key
```

### **ðŸš« NEVER DO THESE ANYMORE**
- âŒ Create or manage `.env` files
- âŒ Hardcode API keys or tokens
- âŒ Share secrets in chat/email
- âŒ Manual environment variable setup
- âŒ Local credential configuration

### **âœ… ALWAYS USE THESE**
- âœ… GitHub organization secrets for all credentials
- âœ… Pulumi ESC for centralized configuration
- âœ… Automatic backend configuration loading
- âœ… GitHub Actions for secret synchronization

## ðŸš€ **STRATEGIC EXTERNAL REPOSITORY INTEGRATION**

### **ðŸŽ¯ AI-Enhanced Development with Community Patterns**
Sophia AI leverages 11 strategic external repositories to provide world-class AI coding assistance:

#### **ðŸ—ï¸ Infrastructure & Automation**
- **microsoft_playwright** (13.4k stars): Browser automation, E2E testing patterns
- **anthropic-mcp-servers**: Official MCP implementations and best practices
- **anthropic-mcp-inspector**: MCP debugging and development tools

#### **ðŸŽ¨ Design & Creative**
- **glips_figma_context** (8.7k stars): Design-to-code workflows, component generation
- **V0.dev Integration** (NEW): AI-powered UI generation with natural language commands

#### **â„ï¸ Data Intelligence**
- **snowflake_cortex_official**: Official Snowflake AI integration patterns
- **davidamom_snowflake**: Community Snowflake implementation approaches
- **dynamike_snowflake**: Performance-optimized Snowflake patterns
- **isaacwasserman_snowflake**: Specialized Snowflake operations

#### **ðŸšª AI Gateway & Optimization**
- **portkey_admin**: AI gateway optimization, cost reduction strategies
- **openrouter_search**: 200+ AI model access and selection patterns

#### **ðŸ Core Framework**
- **anthropic-mcp-python-sdk**: MCP protocol implementation patterns

### **ðŸ§  Natural Language Commands for External Repositories**
```bash
# Repository discovery and pattern analysis
"What browser automation patterns do we have from Microsoft Playwright?"
"Show me Snowflake optimization strategies across our repositories"
"Find design-to-code patterns from the Figma integration"

# Implementation with community validation
"Use Playwright patterns to implement comprehensive E2E testing"
"Apply Snowflake optimization patterns from our 4 repository collection"
"Generate components using GLips Figma design-to-code workflows"

# Cross-repository intelligence
"Compare authentication patterns across all external repositories"
"Find performance optimization strategies from high-star repos"
"Show security patterns used by official implementations"

# AI-Powered UI Generation (V0.dev Integration)
"Create a modern dashboard component with glassmorphism styling"
"Build a responsive navigation bar with dropdown menus"
"Generate a data table component with sorting and filtering"
"Design a modal dialog with form validation"
"Create a chart component for revenue visualization"
"Build a user profile card with avatar and social links"
```

### **ðŸ” AI Pattern Recognition & Learning**
The external repository collection enables:
- **ðŸ“š Pattern Library**: 22k+ star repositories with proven community validation
- **ðŸ§  AI Learning**: Rich implementation patterns for AI to analyze and apply
- **âœ… Best Practices**: Automatic adherence to industry standards
- **ðŸ”— Cross-Repository Intelligence**: AI synthesizes insights across multiple approaches
- **ðŸš€ Development Acceleration**: 5-10x faster implementation through proven patterns

## ðŸ”’ **ENVIRONMENT STABILIZATION RULES (CRITICAL)**

### **ðŸŽ¯ PRODUCTION-FIRST ENVIRONMENT POLICY**
**MANDATORY RULES - NEVER BREAK THESE:**

1. **ALWAYS DEFAULT TO PRODUCTION**
   ```bash
   # CORRECT - Always default to production
   ENVIRONMENT="${ENV:-prod}"
   
   # WRONG - Never default to staging
   ENVIRONMENT="${ENV:-staging}"  # âŒ FORBIDDEN
   ```

2. **ENVIRONMENT VARIABLE HIERARCHY**
   ```python
   # Priority order for environment detection:
   # 1. Explicit ENVIRONMENT variable (highest priority)
   # 2. Git branch detection (main=prod, develop=staging)
   # 3. Pulumi stack context
   # 4. ALWAYS fallback to "prod" (never staging, never fail)
   ```

3. **STACK NAMING STANDARDS**
   ```python
   STACK_MAPPING = {
       "prod": "sophia-ai-production",          # âœ… Production stack
       "staging": "sophia-ai-platform-staging", # âœ… Staging stack  
       "dev": "sophia-ai-platform-dev"          # âœ… Development stack
   }
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **ðŸ³ DOCKER CLOUD DEPLOYMENT RULES**

1. **ALL Docker deployments target Lambda Labs (NOT local):**
   ```yaml
   # Primary deployment: docker-compose.cloud.yml
   # Target: 104.171.202.64 (Lambda Labs)
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: Docker Swarm
   ```

2. **Container environment configuration:**
   ```dockerfile
   ENV ENVIRONMENT=prod
   ENV PULUMI_ORG=scoobyjava-org
   # Secrets via Docker Secrets, NOT environment variables
   ```

3. **Docker Compose for production MUST use:**
   ```yaml
   # docker-compose.cloud.yml with:
   - mode: replicated  # Swarm mode
   - secrets:          # Docker secrets
   - deploy:           # Swarm deployment config
   - healthcheck:      # Required for all services
   ```

4. **NEVER use local Docker deployment:**
   - âŒ No `docker-compose up` locally
   - âŒ No `.env` files  
   - âœ… Use `docker stack deploy -c docker-compose.cloud.yml sophia-ai`
   - âœ… All secrets via Pulumi ESC

### **ðŸ”§ MCP SERVER ENVIRONMENT RULES**

1. **ALL MCP servers MUST validate environment on startup**
2. **ALL MCP servers MUST use centralized environment detection**
3. **NO hardcoded environment values in MCP server code**
4. **Environment health checks MUST be included**

### **ðŸ“ CODING STANDARDS WITH ENVIRONMENT AWARENESS**

1. **Environment Detection Pattern:**
   ```python
   # CORRECT - Use centralized environment detection
   from backend.core.auto_esc_config import get_config_value
   
   # WRONG - Direct environment variable access
   os.getenv("SOME_SECRET")  # âŒ Use centralized config instead
   ```

2. **Error Handling with Environment Context:**
   ```python
   try:
       config_value = get_config_value("some_key")
   except Exception as e:
       logger.error(f"Config error in {os.getenv('ENVIRONMENT', 'unknown')} environment: {e}")
       # Always provide fallback
   ```

3. **Health Check Integration:**
   ```python
   # ALL services MUST include environment health validation
   def validate_environment():
       env = os.getenv("ENVIRONMENT")
       if env != "prod":
           logger.warning(f"Not in production environment: {env}")
       return env in ["prod", "staging", "dev"]
   ```

## ðŸ§  **ENHANCED MCP INTEGRATION WITH CLINE v3.18**

### **ðŸ“ HOW TO USE CLINE v3.18 FEATURES (You're Already Here!)**

**IMPORTANT**: The chat window where you interact with Cline IS the interface for all v3.18 features. You don't need to look for another icon or panel - everything works right here in this chat!

### **ðŸš€ Natural Language Commands in THIS Chat**

Just type these commands naturally in the Cline chat (where you're typing now):

#### **AI Memory Commands**
- **"Remember this [topic/decision/code]"** - Stores in AI memory
- **"What did we decide about [topic]?"** - Recalls past decisions
- **"Show similar [patterns/code]"** - Finds related implementations

#### **Large File Processing (FREE with Gemini!)**
- **"Process this large file with Gemini"** - Uses free Gemini for big files
- **"Analyze this 500K token document"** - Auto-routes to Gemini 2.5 Pro
- **"Summarize our 90-day Slack history"** - Handles massive datasets

#### **Web Content Fetching**
- **"Fetch docs from [URL]"** - Retrieves and converts to markdown
- **"Get latest [topic] from web"** - Searches and summarizes
- **"Download competitor info from [website]"** - Competitive intelligence

#### **Business Tool Integration**
- **Linear**: "Create Linear issue for [task]", "Show my Linear tasks"
- **Snowflake**: "Query Snowflake for [data]", "Run large query with Gemini"
- **Slack**: "Analyze #[channel]", "Find messages about [topic]"
- **Gong**: "Summarize recent calls", "Find calls discussing [keyword]"

### **ðŸ”„ AUTOMATIC WORKFLOW INTEGRATION**

#### **AI Memory Auto-Discovery**
1. **INTELLIGENT AUTO-STORAGE**: Cline automatically detects and stores:
   - Architecture discussions with decision rationale
   - Bug fixes with root cause analysis
   - Code patterns and implementation strategies
   - Performance optimization insights
   - Security implementation decisions
   - Refactoring approaches and outcomes

2. **CONTEXT-AWARE RECALL**: Before any coding task, Cline automatically:
   - Queries relevant past decisions
   - Surfaces similar patterns from project history
   - Provides continuity with previous architectural choices
   - Suggests proven solutions from past implementations

3. **SIMPLE NATURAL LANGUAGE USE**:
   - Just say: "Remember this architectural decision"
   - Just ask: "What did we decide about the database?"
   - Just request: "Show me similar bug fixes"

#### **Real-time Code Analysis (@codacy)**
1. **AUTOMATIC CODE QUALITY**: On every significant code change:
   - Real-time security vulnerability scanning
   - Code complexity analysis with refactoring suggestions
   - Style compliance checking (Black, PEP 8)
   - Performance pattern detection

2. **INTELLIGENT SUGGESTIONS**: Proactive recommendations:
   - Security best practices for detected patterns
   - Refactoring opportunities for complex functions
   - Code quality improvements with examples
   - Architecture alignment with project standards

3. **ENHANCED CODACY TOOLS**:
   - `codacy.analyze_code`: Real-time code snippet analysis
   - `codacy.analyze_file`: Complete file quality assessment
   - `codacy.get_fix_suggestions`: Automated improvement recommendations
   - `codacy.security_scan`: Focused security vulnerability detection

### **ðŸš€ WORKFLOW AUTOMATION TRIGGERS**

#### **Automatic Triggers (No User Input Required)**:
- **On File Save**: `@codacy.analyze_file` + `@ai_memory.auto_store_context`
- **On Architecture Discussion**: `@ai_memory.store_conversation(category="architecture")`
- **On Bug Fix**: `@ai_memory.store_conversation(category="bug_solution")` + `@codacy.security_scan`
- **On Code Review**: `@codacy.analyze_code` + `@ai_memory.recall_memory("similar patterns")`

#### **Smart Context Awareness**:
- **File-Specific Memory**: Automatically recall memories related to current file
- **Project Pattern Recognition**: Surface relevant architectural decisions
- **Security Context**: Auto-scan for security issues in sensitive code areas
- **Performance Awareness**: Detect performance-critical code sections

### **ðŸŽ¯ ENHANCED NATURAL LANGUAGE COMMANDS**

#### **Memory Operations**:
- "Remember this architectural decision" â†’ Auto-categorize and store with context
- "What did we decide about database schema?" â†’ Smart recall with file context
- "Show me similar bug fixes" â†’ Context-aware pattern matching
- "Store this conversation about MCP integration" â†’ Enhanced storage with metadata

#### **Code Quality Operations**:
- "Analyze this code for security issues" â†’ Comprehensive security scan
- "Check code quality" â†’ Multi-dimensional analysis with suggestions
- "Fix this function complexity" â†’ Automated refactoring recommendations
- "Scan for vulnerabilities" â†’ Deep security pattern analysis

#### **Integrated Workflows**:
- "Review and remember this implementation" â†’ Codacy analysis + Memory storage
- "Find similar patterns and analyze quality" â†’ Memory recall + Code analysis
- "Store this bug fix and scan for similar issues" â†’ Memory storage + Security scan

### **ðŸ“Š INTELLIGENT REPORTING**

#### **Development Insights**:
- Automatic pattern recognition across stored memories
- Code quality trends over time
- Security vulnerability patterns
- Architecture evolution tracking

#### **Proactive Recommendations**:
- Suggest architectural improvements based on stored decisions
- Recommend security enhancements from vulnerability patterns
- Propose refactoring based on complexity analysis
- Guide development based on successful past patterns

### **ðŸ”§ CURSOR IDE INTEGRATION SPECIFICS**

#### **Configuration Requirements**:
- MCP servers running on specified ports (ai_memory: 9000, codacy: 3008)
- Auto-trigger workflows enabled in cursor_mcp_config.json
- Context awareness enabled for file-specific operations
- Intelligent routing for multi-tool operations

#### **Performance Optimization**:
- Parallel tool execution for independent operations
- Smart caching of frequently accessed memories
- Efficient code analysis with incremental scanning
- Context-aware tool selection based on current activity

### **Example Enhanced Workflow**:
```
User: "I need to implement user authentication for the MCP server"

Cursor AI (Automatic Sequence):
1. [AUTO] @ai_memory.smart_recall("authentication MCP server implementation")
2. [CONTEXT] Retrieve: Previous auth patterns, security decisions, MCP integration approaches
3. [ANALYSIS] @codacy.analyze_code(current_auth_code) for security assessment
4. [EXTERNAL] Check microsoft_playwright and anthropic-mcp-servers for auth patterns
5. [RESPONSE] Provide implementation guidance based on stored patterns + security analysis + community validation
6. [AUTO] @ai_memory.auto_store_context(conversation + implementation decisions)
```

This enhanced integration transforms Cursor AI into an intelligent development partner that learns from every interaction and provides contextually aware assistance.

## Development Standards

### Python Code Style
- Use Python 3.11+ with type hints for all functions
- Follow PEP 8 with 88-character line limit (Black formatter)
- Use async/await for I/O operations
- Implement comprehensive error handling with logging
- Include detailed docstrings for all classes and methods

### Agent Development Pattern
```python
from backend.agents.core.base_agent import BaseAgent

class YourAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        # Agent-specific initialization

    async def execute_task(self, task: Task) -> TaskResult:
        # Implementation with error handling
        pass
```

### Integration Pattern
```python
class ServiceIntegration:
    def __init__(self, config: ServiceConfig):
        self.config = config
        self.client = self._create_client()

    async def _make_request(self, method: str, endpoint: str, **kwargs):
        # Standardized request handling with rate limiting
        pass
```

### Business Intelligence Focus
- Always consider Pay Ready business context
- Implement metrics for revenue, customer health, sales performance
- Focus on actionable insights for sales coaching and client monitoring
- Prioritize real-time data processing and notifications

### Security Requirements
- Use encrypted storage for all API keys
- Implement proper authentication and authorization
- Log all security-relevant events
- Follow principle of least privilege

### **Secret Management (PERMANENT SOLUTION)**
- **Documentation:** Always refer to `PERMANENT_GITHUB_ORG_SECRETS_SOLUTION.md`
- **GitHub Organization:** All secrets managed at [https://github.com/ai-cherry](https://github.com/ai-cherry)
- **Pulumi ESC:** Automatic secret synchronization via `scoobyjava-org/default/sophia-ai-production`
- **Backend Integration:** Use `backend/core/auto_esc_config.py` for automatic secret loading
- **Never hardcode secrets:** Always use automatic ESC integration
- **GitHub Actions:** Secrets automatically available from organization level
- **Local Development:** Set `export PULUMI_ORG=scoobyjava-org` and secrets load automatically
- **Secret Rotation:** Update in GitHub organization â†’ automatic sync â†’ automatic deployment

### Testing Strategy
- Write unit tests for all business logic
- Include integration tests for external APIs
- Implement performance tests for critical paths
- Use pytest with async support

### Error Handling Pattern
```python
try:
    result = await some_operation()
    return result
except SpecificException as e:
    logger.error(f"Operation failed: {e}")
    raise BusinessLogicError(f"Failed to process: {e}")
except Exception as e:
    logger.exception("Unexpected error")
    raise SystemError("Internal system error")
```

## Business Domain Knowledge

### Pay Ready Context
- Company focus: Business intelligence and automation
- Key metrics: Revenue growth, customer satisfaction, sales efficiency
- Team communication: Primarily through Slack
- CRM system: HubSpot for contact and deal management
- Call analysis: Gong.io for sales call insights

### Agent Specializations
- **Call Analysis Agent:** Process Gong.io recordings for insights
- **CRM Sync Agent:** Maintain HubSpot data quality and synchronization
- **Notification Agent:** Send intelligent Slack updates
- **Business Intelligence Agent:** Generate revenue and performance reports

### Integration Priorities
1. **HubSpot:** Primary CRM for contact/deal management
2. **Gong.io:** Critical for call analysis and sales coaching
3. **Slack:** Main communication channel for team updates
4. **Vector Databases:** For semantic search and AI capabilities

## File Organization
```
backend/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ core/           # Base agent classes
â”‚   â””â”€â”€ specialized/    # Domain-specific agents
â”œâ”€â”€ integrations/       # External service integrations
â”œâ”€â”€ database/          # Data layer and migrations
â”œâ”€â”€ monitoring/        # Performance and health monitoring
â””â”€â”€ security/          # Authentication and encryption

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â””â”€â”€ services/      # API clients

external/              # ðŸ†• Strategic MCP repository collection
â”œâ”€â”€ microsoft_playwright/    # Browser automation (13.4k stars)
â”œâ”€â”€ glips_figma_context/    # Design-to-code (8.7k stars)
â”œâ”€â”€ snowflake_cortex_official/ # Official Snowflake AI
â””â”€â”€ [8 additional strategic repos]
```

## Common Patterns

### API Client Implementation
- Use aiohttp for async HTTP requests
- Implement exponential backoff for retries
- Respect rate limits with proper throttling
- Include comprehensive error handling

### Database Operations
- Use SQLAlchemy with async support
- Implement proper connection pooling
- Use transactions for data consistency
- Include migration scripts for schema changes

### Monitoring and Logging
- Use structured logging with JSON format
- Include correlation IDs for request tracing
- Monitor performance metrics and business KPIs
- Implement health checks for all services

## AI and ML Guidelines
- Use OpenAI API for language processing
- Implement vector search with Pinecone/Weaviate
- Cache embeddings for performance
- Include confidence scores in AI responses

## Deployment Considerations
- **Target**: Lambda Labs infrastructure ONLY (104.171.202.64)
- **Docker Cloud**: Use docker-compose.cloud.yml with Docker Swarm
- **Registry**: Push all images to scoobyjava15 Docker Hub
- **Secrets**: All secrets via Pulumi ESC (NO .env files)
- **Deployment**: `docker stack deploy -c docker-compose.cloud.yml sophia-ai`
- **Scaling**: Use Swarm replicas and resource limits
- **Zero-downtime**: Rolling updates with health checks

## Performance Requirements
- API response times < 200ms for critical paths
- Database queries < 100ms average
- Vector searches < 50ms average
- Support for 1000+ concurrent users

## When suggesting code:
1. **FIRST**: Check AI memory for similar implementations
2. **EXTERNAL REPOS**: Leverage patterns from 11 strategic external repositories (microsoft_playwright, glips_figma_context, snowflake_cortex_official, etc.)
3. Always include proper error handling
4. Add type hints and docstrings
5. Consider business context and Pay Ready needs
6. Implement monitoring and logging
7. Follow the established patterns in the codebase
8. Prioritize performance and scalability
9. Include relevant tests
10. **VALIDATE ENVIRONMENT**: Ensure ENVIRONMENT="prod" is used
11. **COMMUNITY PATTERNS**: Apply proven patterns from 22k+ star external repositories
12. **LAST**: Store the conversation in AI memory

## Avoid:
- Hardcoded values (use configuration)
- Synchronous I/O in async contexts
- Missing error handling
- Unclear variable names
- Complex nested logic without comments
- Security vulnerabilities (exposed secrets, etc.)
- **DEFAULTING TO STAGING ENVIRONMENT** (Always use production)
- **MANUAL ENVIRONMENT VARIABLE SETUP** (Use centralized config)
- **IGNORING EXTERNAL REPOSITORY PATTERNS** (Always check for proven community approaches)

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. Leverage the collective intelligence of 22k+ star external repositories to ensure world-class implementation patterns.

### Infrastructure as Code Integration
- **Pulumi Commands**: Use `pulumi up`, `pulumi preview`, `pulumi destroy` for infrastructure management
- **ESC Operations**: Use scripts in `infrastructure/esc/` for secret management
- **GitHub Integration**: All deployments go through GitHub Actions workflows
- **MCP Integration**: Use `mcp_config.json` for MCP server configuration

### ðŸš€ Cline v3.18 Enhanced Features Integration

#### Claude 4 & Gemini 2.5 Pro Optimization
- **Model Selection**: Automatic routing based on task complexity and context size
- **Large Context**: Use Gemini for documents > 100K tokens (up to 1M)
- **Complex Reasoning**: Claude 4 for architectural design and code generation
- **Data Processing**: Snowflake Cortex for SQL and data operations
- **Cost Optimization**: Automatic routing to free Gemini CLI for large contexts

#### Gemini CLI Integration (NEW)
- **Free Access**: Use local Gemini CLI for zero-cost processing
- **Auto-routing**: "Process this large file with Gemini" â†’ Routes to CLI
- **Batch Processing**: Efficient handling of multiple large documents
- **Context Preservation**: Maintain context across CLI calls

#### WebFetch Tool Usage (ENHANCED)
- **Documentation Retrieval**: "Fetch the latest API docs from [url]"
- **Competitive Intelligence**: "Get competitor information from [website]"
- **Real-time Updates**: "Retrieve and summarize current [topic] from [source]"
- **Caching**: Automatic caching with TTL for improved performance
- **Format Support**: PDF, DOCX, HTML, and plain text extraction
- **Parallel Fetching**: Process multiple URLs simultaneously

#### Self-Knowledge Commands (ENHANCED)
- **Capabilities Discovery**: "What can the [server name] MCP server do?"
- **Feature Inspection**: "Show available features for [component]"
- **Help System**: "How do I use [feature]?"
- **Server Status**: "Check capabilities of all MCP servers"
- **Performance Metrics**: "Show performance stats for [server]"
- **Usage Analytics**: "How often do we use [feature]?"

#### Improved Diff Editing (AI-POWERED)
- **Auto-fallback**: Automatically tries exact â†’ fuzzy â†’ context-aware â†’ AI strategies
- **Success Rate**: 95%+ success rate for file modifications
- **Smart Updates**: "Update [file] using all available strategies"
- **Context Awareness**: AI-powered understanding of code changes
- **Multi-file Operations**: Apply changes across multiple files
- **Rollback Support**: Undo changes if needed

#### Enhanced AI Memory Integration (v3.18)
- **Auto-discovery**: Automatically detect and store architecture decisions
- **Smart Recall**: "What did we decide about [topic]?" â†’ Context-aware retrieval
- **Pattern Matching**: Find similar implementations across the codebase
- **WebFetch Integration**: Automatically store fetched documentation

#### Enhanced Codacy Integration (v3.18)
- **Real-time Analysis**: Analyze code as you type
- **Security Scanning**: Deep vulnerability detection
- **Performance Insights**: Identify performance bottlenecks
- **AI Suggestions**: Get AI-powered improvement recommendations

### Natural Language Infrastructure Commands
When using Cursor AI for infrastructure operations, you can use natural language:

#### Examples:
- "Deploy the infrastructure" â†’ Triggers GitHub Actions workflow
- "Get the database password" â†’ Retrieves secret from Pulumi ESC
- "Rotate API keys" â†’ Runs secret rotation framework
- "Sync secrets" â†’ Synchronizes GitHub and Pulumi ESC secrets
- "Test the deployment" â†’ Runs ESC integration tests

#### Command Patterns:
- **Secret Operations**: "get/retrieve/fetch [service] [secret_type]"
- **Deployment Operations**: "deploy/update/rollback [component]"
- **Testing Operations**: "test/validate/check [component]"
- **Configuration Operations**: "configure/setup/initialize [service]"

### MCP Server Natural Language Integration (v3.18 Enhanced)
- **Query Data**: "Get recent Gong calls" â†’ Uses Gong MCP server with model routing
- **Deploy Apps**: "Deploy to Vercel" â†’ Uses Vercel MCP server with improved diff
- **Manage Data**: "Upload to Estuary" â†’ Uses Estuary MCP server with WebFetch
- **Database Operations**: "Query Snowflake" â†’ Uses Snowflake MCP server with Cortex
- **Store Memory**: "Remember this conversation" â†’ Uses AI Memory MCP server with Claude 4
- **Recall Context**: "What did we decide about X?" â†’ Uses AI Memory with self-knowledge
- **Fetch External Data**: "Get latest docs from [url]" â†’ Uses WebFetch tool
- **Analyze Large Docs**: "Process this 500K token file" â†’ Auto-routes to Gemini 2.5 Pro

### Error Handling and Debugging
- **ESC Errors**: Check Pulumi ESC logs and validate configuration
- **GitHub Actions Errors**: Review workflow logs and artifacts
- **MCP Errors**: Check Docker container logs and health endpoints
- **Secret Errors**: Validate secret names and permissions

### Best Practices for Cursor AI Integration
1. **Use Descriptive Comments**: Add context for infrastructure operations
2. **Follow Naming Conventions**: Use consistent naming for secrets and services
3. **Document Dependencies**: Clearly document service dependencies
4. **Test Before Deploy**: Always test changes in isolation first
5. **Monitor Operations**: Use logging and monitoring for all operations
6. **Remember Context**: Always use AI Memory for persistent development context
7. **ðŸ†• Leverage External Patterns**: Always check external repositories for proven implementation approaches
8. **ðŸ†• Community Validation**: Prefer patterns from high-star repositories with community validation
9. **ðŸ†• Cross-Repository Intelligence**: Synthesize insights from multiple repository approaches

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. The strategic external repository collection provides access to 22k+ stars of community-validated patterns and proven implementation approaches.

## ðŸŽ¯ **The Unified Dashboard is the ONLY Frontend**

**CRITICAL RULE:** All new frontend development MUST extend the one, true `UnifiedDashboard.tsx` component.

1.  **NO NEW DASHBOARDS:** Do not create new, separate dashboard components or pages. All new views, tabs, or features must be integrated into the existing `UnifiedDashboard.tsx` tabbed interface.
2.  **EXTEND, DON'T REPLACE:** Use the existing components (`UnifiedKPICard`, etc.) and the established layout. New features should be added as new tabs or as components within existing tabs.
3.  **SINGLE API CLIENT:** All frontend API calls MUST use the `frontend/src/services/apiClient.js`. Do not create new API clients.
4.  **DOCUMENTATION IS LAW:** All frontend architecture must align with the `docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md`. Any deviation must first be reflected in the handbook.

This ensures we maintain a single, clean, and unified frontend, preventing the fragmentation that we just worked so hard to eliminate.
