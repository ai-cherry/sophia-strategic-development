## âš ï¸ **MONOREPO TRANSITION IN PROGRESS**

**CRITICAL FOR AI CODERS**:
- We are transitioning from the old structure (`backend/`, `frontend/`) to a new monorepo structure (`apps/`, `libs/`)
- **Continue using the OLD structure** for all new code until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for details
- Target completion: February 2025

## ðŸš€ **THE GOLDEN RULE OF DEPLOYMENT (MANDATORY)**

**ALL DEPLOYMENTS ARE AUTOMATED VIA GITHUB ACTIONS. NEVER DEPLOY FROM A LOCAL MACHINE.**

### **Core Principles (Non-Negotiable):**
1.  **Single Point of Truth**: The `main` branch on GitHub is the single source of truth for what is deployed.
2.  **Push-to-Deploy**: All deployments to any environment (Production, Staging) are triggered **exclusively** by a `git push` to the relevant branch.
3.  **No Local Operations**: You MUST NOT run `docker build`, `docker push`, or `docker stack deploy` from your local machine for deployment purposes. Your role is to write the code and the GitHub Actions workflows; the cloud does the rest.
4.  **Automated Secret Management**: All secrets (`DOCKER_HUB_ACCESS_TOKEN`, `LAMBDA_LABS_SSH_KEY`, etc.) are managed in GitHub Organization Secrets and injected into the GitHub Actions workflow at runtime. You MUST NOT handle credentials locally.

### **The Correct Workflow:**
1.  **Write Code**: Implement the feature or fix.
2.  **Write/Update Workflow**: Ensure the `.github/workflows/` configuration is correct for building and deploying the service.
3.  **Push to GitHub**: `git push origin main`.
4.  **Monitor Action**: Observe the deployment progress in the GitHub Actions tab.

This is the **only** approved method. Any deviation from this (e.g., suggesting `docker login` locally) is a violation of core operating principles.

## ðŸŒŸ **NEW: The System Handbook is the Ultimate Source of Truth**

**CRITICAL DIRECTIVE:** The `docs/system_handbook/` directory, and specifically `00_SOPHIA_AI_SYSTEM_HANDBOOK.md`, is the **single, definitive source of truth** for the Sophia AI platform's architecture, standards, and roadmap.

### **Your Core Responsibilities Regarding Documentation:**

1.  **CONSULT FIRST, CODE LATER:** Before answering any architectural question or implementing any new feature, you **MUST** first consult the System Handbook to understand the established patterns and principles.
2.  **PROACTIVELY UPDATE:** If you make a significant architectural change (e.g., add a new service, change a data flow, modify the MCP gateway), you **MUST** update the relevant section of the System Handbook within the same session.
3.  **MAINTAIN STRUCTURE:** When adding documentation, you **MUST** follow the existing structure and format. If a new deep-dive document is needed, create it within the `system_handbook` directory and link to it from the master file.
4.  **CITE YOUR SOURCE:** When you use information from the handbook to make a decision, you should briefly mention it (e.g., "Based on the Phoenix Plan architecture outlined in the System Handbook, I will...").

This ensures our documentation remains a living, breathing, and accurate representation of the platform, empowering both human and AI developers.

# Sophia AI Pay Ready Platform - Cursor AI Rules

## Project Overview
You are working on Sophia AI, an AI assistant orchestrator for Pay Ready company. Sophia serves as the central "Pay Ready Brain" that orchestrates multiple AI agents and integrates with business systems.

### Company Context
- **Company Size:** 80 employees total
- **Initial Users:** 1 (CEO - primary developer and user)
- **Rollout Plan:** CEO only â†’ Few super users (2-3 months) â†’ Full company (6+ months)
- **Development Team:** CEO (sole human developer) + AI assistants

## ðŸŽ¯ **Development Priorities (CRITICAL)**

### **Priority Order - NEVER COMPROMISE:**
1. **QUALITY & CORRECTNESS** - Every line of code must be correct and well-structured
2. **STABILITY & RELIABILITY** - System must be rock-solid for CEO usage
3. **MAINTAINABILITY** - Code must be clear and easy to modify
4. **PERFORMANCE** - Important but secondary to quality
5. **COST & SECURITY** - Consider but don't over-optimize at this stage

### **Quality Standards:**
- **Zero Duplication:** Never duplicate code or functionality
- **Clear Dependencies:** All dependencies must be explicit and documented
- **Conflict Prevention:** Check for conflicts before implementing
- **Structure First:** Plan structure to avoid future issues
- **Review Everything:** Always review context before coding

### **Tool Selection Principle:**
> **Only add new tools when there's a clear gap that existing tools cannot fill.**

This principle prevents:
- Tool proliferation and complexity creep
- Duplicate functionality across different tools
- Maintenance burden from unnecessary dependencies
- Migration overhead from overlapping solutions

Before adding any new tool or framework:
1. Check if existing tools can solve the problem
2. Document the specific gap the new tool fills
3. Consider the long-term maintenance cost
4. Prefer enhancing existing tools over adding new ones

## ðŸ§¹ **Code Hygiene Rules (MANDATORY)**

### **One-Time Scripts and Documents:**
1. **ALWAYS DELETE after use:**
   - Migration scripts after successful migration
   - Test scripts after tests pass
   - Setup scripts after setup complete
   - Temporary analysis documents
   - One-time reports

2. **NEVER DELETE:**
   - Reusable utilities
   - Documentation in `docs/`
   - Configuration files
   - Test suites
   - Core scripts

3. **Before Creating Files:**
   - Ask: "Will this be used more than once?"
   - If NO â†’ Plan to delete after use
   - If YES â†’ Place in appropriate permanent location

4. **Cleanup Pattern:**
   ```python
   # At the end of any one-time script:
   print("âœ… Task completed successfully")
   print("ðŸ§¹ This script can now be deleted: rm scripts/one_time_script.py")
   ```

## ðŸ“… **Timeline and Budget Guidelines (CRITICAL)**

### **For Coding Tasks - NEVER INCLUDE:**
- âŒ **NO specific time estimates** (hours, days, weeks, months)
- âŒ **NO budget estimates** for development work
- âŒ **NO duration predictions** for implementation
- âŒ **NO cost calculations** for coding effort
- âŒ **NO timeline-based milestones**

### **For Coding Tasks - ALWAYS USE:**
- âœ… **Phase numbers** (Phase 1, Phase 2.5, etc.)
- âœ… **Functional milestones** (Foundation Complete, Integration Ready)
- âœ… **Feature-based progress** (Intent Classification âœ…, Code Modification âœ…)
- âœ… **Technical dependencies** (Requires X before Y)
- âœ… **Complexity indicators** (Simple, Moderate, Complex)

### **Why This Matters:**
- Time estimates for AI-assisted coding are meaningless and wasteful
- Focus should be on technical implementation, not scheduling
- Quality and correctness matter more than speed
- Each coding session is unique and unpredictable

### **Exception - Business Planning Only:**
- Timeline estimates ONLY when explicitly requested for business planning
- Infrastructure cost estimates when evaluating services
- ROI calculations for business decisions
- Keep business metrics completely separate from coding tasks

## Architecture Context
- **Type:** Multi-agent AI orchestrator for CEO-level business intelligence
- **Primary User:** Pay Ready CEO (initial sole user)
- **Core Integrations:** HubSpot CRM, Gong.io call analysis, Slack communication
- **Data Stack:** PostgreSQL, Redis, Pinecone, Weaviate
- **Infrastructure:** Lambda Labs servers, Vercel frontend deployment
- **External Repository Collection:** 11 strategic MCP servers (22k+ combined stars)

## ðŸ” **PERMANENT SECRET MANAGEMENT SOLUTION**

### **CRITICAL: GitHub Organization Secrets Integration**
Sophia AI uses a **PERMANENT** secret management solution that eliminates manual `.env` management:

```
GitHub Organization Secrets (ai-cherry)
           â†“
    GitHub Actions (automatic sync)
           â†“
    Pulumi ESC Environments
           â†“
    Sophia AI Backend (automatic loading)
```

### **âœ… What's Automated**
- **Zero Manual Secret Management**: No more `.env` file management
- **Organization-Level Secrets**: All secrets in [GitHub ai-cherry org](https://github.com/ai-cherry)
- **Automatic Sync**: GitHub Actions â†’ Pulumi ESC â†’ Backend
- **Enterprise Security**: No exposed credentials anywhere
- **Forever Solution**: Works automatically without intervention

### **ðŸ”‘ Secret Access Pattern**
```python
# Backend automatically loads from Pulumi ESC
from backend.core.auto_esc_config import config

# Secrets are automatically available
openai_key = config.openai_api_key
gong_key = config.gong_access_key
```

### **ðŸš« NEVER DO THESE ANYMORE**
- âŒ Create or manage `.env` files
- âŒ Hardcode API keys or tokens
- âŒ Share secrets in chat/email
- âŒ Manual environment variable setup
- âŒ Local credential configuration

### **âœ… ALWAYS USE THESE**
- âœ… GitHub organization secrets for all credentials
- âœ… Pulumi ESC for centralized configuration
- âœ… Automatic backend configuration loading
- âœ… GitHub Actions for secret synchronization

## ðŸ¤– **UNIFIED AI AGENT AUTHENTICATION SYSTEM**

### **ðŸš€ REVOLUTIONARY CAPABILITY: AI Agents Can Make REAL CHANGES**

Sophia AI features a **REVOLUTIONARY** Unified AI Agent Authentication System that enables AI coding agents to make **REAL CHANGES** across the entire technology stack with enterprise-grade security.

### **ðŸ—ï¸ Three-Tier Security Architecture**

#### **Tier 1: CLI-Based Authentication (Highest Security)**
Services that use CLI-based authentication with secure credential storage:
- **GitHub**: `gh auth login` with secure token storage
- **Pulumi**: `pulumi login` with organization access
- **Docker**: `docker login` for registry operations
- **Vercel**: `vercel login` for frontend deployments

#### **Tier 2: Enhanced API Authentication**
Services with enhanced security patterns:
- **Snowflake**: Secure connection strings with role-based access
- **Lambda Labs**: API key management with instance control
- **Estuary Flow**: Service account authentication

#### **Tier 3: Secure API Key Management**
Standard API integrations with secure key management:
- **OpenAI, Anthropic, Slack, Linear, HubSpot**: Automatic ESC integration

### **ðŸ¤– Agent Permission Matrix**

#### **Infrastructure Agent (CRITICAL Risk)**
Can perform infrastructure-level operations:
```python
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager

auth_manager = UnifiedServiceAuthManager()

# Deploy infrastructure
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="pulumi",
    operation="infrastructure_deployment",
    params={"stack": "production", "config": "updated_resources"}
)

# Manage containers
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="docker",
    operation="container_management",
    params={"action": "deploy", "service": "sophia-ai"}
)
```

#### **Data Agent (HIGH Risk)**
Can perform data operations:
```python
# Execute database operations
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="snowflake",
    operation="query_execution",
    params={"query": "CREATE SCHEMA AI_AGENT_TEST", "warehouse": "SOPHIA_AI_COMPUTE_WH"}
)

# Manage data flows
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="estuary_flow",
    operation="flow_management",
    params={"action": "create", "collection": "ai_agent_data"}
)
```

#### **Integration Agent (MEDIUM Risk)**
Can perform business tool operations:
```python
# Create project tickets
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="linear",
    operation="ticket_creation",
    params={"title": "AI Agent Test", "team": "sophia-dev"}
)

# Send notifications
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="slack",
    operation="message_send",
    params={"channel": "#ai-agents", "message": "Deployment completed"}
)
```

### **ðŸ›¡ï¸ Enterprise Security Features**

#### **Zero Trust Authentication**
- Every operation requires explicit authentication
- No persistent credentials in AI agent memory
- All operations logged with full audit trail

#### **Risk-Based Confirmation Workflows**
```python
# CRITICAL operations require explicit confirmation
confirmation = await auth_manager.get_operation_confirmation(
    operation="infrastructure_deployment",
    risk_level=RiskLevel.CRITICAL,
    details="Deploying new Pulumi stack to production"
)

if confirmation.approved:
    await auth_manager.execute_operation(...)
```

#### **Complete Audit Trail**
- All AI agent operations logged to database
- Risk assessment recorded for each operation
- User confirmation workflows tracked
- Security compliance reporting

### **ðŸš€ Natural Language Commands for AI Agents**

#### **Infrastructure Operations**
```bash
"Deploy the updated infrastructure to production"
"Scale up the Snowflake warehouse for the analytics job"
"Create a new Docker service for the MCP gateway"
"Update the Vercel deployment with latest frontend changes"
```

#### **Data Operations**
```bash
"Create a new schema for the AI agent testing"
"Run the quarterly revenue analysis query"
"Set up a new data flow from HubSpot to Snowflake"
"Backup the production database before the migration"
```

#### **Business Tool Integration**
```bash
"Create a Linear ticket for the authentication bug"
"Send a Slack message about the deployment status"
"Update the HubSpot deal with the latest information"
"Schedule a GitHub Action workflow for the nightly build"
```

### **ðŸ“‹ Setup Commands**

#### **Phase 1: CLI Authentication Setup**
```bash
# Run the setup script
python scripts/setup_unified_ai_agent_auth.py

# Verify authentication
python -c "from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager; auth = UnifiedServiceAuthManager(); print('âœ… Authentication system ready')"
```

#### **Phase 2: Permission Configuration**
```bash
# Configure agent permissions
python -c "
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager
auth = UnifiedServiceAuthManager()
auth.configure_agent_permissions()
print('âœ… Agent permissions configured')
"
```

### **ðŸŽ¯ Business Value**

#### **Revolutionary Capabilities**
- **AI agents can make REAL infrastructure changes** with enterprise security
- **Natural language interface** for complex technical operations
- **Zero credential exposure** through secure CLI authentication
- **Complete audit compliance** for all AI-driven changes

#### **Enterprise Benefits**
- **100% Automated Operations**: AI agents handle routine deployments
- **Zero Manual Credential Management**: All authentication automated
- **Complete Security Compliance**: Enterprise-grade audit trails
- **Real-Time Infrastructure Management**: Immediate response to operational needs

### **ðŸ“š Documentation Reference**
- **Complete Guide**: `docs/99-reference/UNIFIED_AI_AGENT_AUTHENTICATION.md`
- **Security Manager**: `backend/security/unified_service_auth_manager.py`
- **Setup Script**: `scripts/setup_unified_ai_agent_auth.py`

**ðŸš¨ CRITICAL**: This system enables AI agents to make REAL changes across your entire technology stack. All operations are secured, audited, and require appropriate confirmations based on risk levels.

## ðŸš€ **STRATEGIC EXTERNAL REPOSITORY INTEGRATION**

### **ðŸŽ¯ AI-Enhanced Development with Community Patterns**
Sophia AI leverages 11 strategic external repositories to provide world-class AI coding assistance:

#### **ðŸ—ï¸ Infrastructure & Automation**
- **microsoft_playwright** (13.4k stars): Browser automation, E2E testing patterns
- **anthropic-mcp-servers**: Official MCP implementations and best practices
- **anthropic-mcp-inspector**: MCP debugging and development tools

#### **ðŸŽ¨ Design & Creative**
- **glips_figma_context** (8.7k stars): Design-to-code workflows, component generation
- **V0.dev Integration** (NEW): AI-powered UI generation with natural language commands

#### **â„ï¸ Data Intelligence**
- **snowflake_cortex_official**: Official Snowflake AI integration patterns
- **davidamom_snowflake**: Community Snowflake implementation approaches
- **dynamike_snowflake**: Performance-optimized Snowflake patterns
- **isaacwasserman_snowflake**: Specialized Snowflake operations

#### **ðŸšª AI Gateway & Optimization**
- **portkey_admin**: AI gateway optimization, cost reduction strategies
- **openrouter_search**: 200+ AI model access and selection patterns

#### **ðŸ Core Framework**
- **anthropic-mcp-python-sdk**: MCP protocol implementation patterns

### **ðŸ§  Natural Language Commands for External Repositories**
```bash
# Repository discovery and pattern analysis
"What browser automation patterns do we have from Microsoft Playwright?"
"Show me Snowflake optimization strategies across our repositories"
"Find design-to-code patterns from the Figma integration"

# Implementation with community validation
"Use Playwright patterns to implement comprehensive E2E testing"
"Apply Snowflake optimization patterns from our 4 repository collection"
"Generate components using GLips Figma design-to-code workflows"

# Cross-repository intelligence
"Compare authentication patterns across all external repositories"
"Find performance optimization strategies from high-star repos"
"Show security patterns used by official implementations"

# AI-Powered UI Generation (V0.dev Integration via Unified Chat)
# Note: Type these directly in the unified chat - no @ commands needed
"Create a modern dashboard component with glassmorphism styling"
"Build a responsive navigation bar with dropdown menus"
"Generate a data table component with sorting and filtering"
"Design a modal dialog with form validation"
"Create a chart component for revenue visualization"
"Build a user profile card with avatar and social links"
```

### **ðŸ” AI Pattern Recognition & Learning**
The external repository collection enables:
- **ðŸ“š Pattern Library**: 22k+ star repositories with proven community validation
- **ðŸ§  AI Learning**: Rich implementation patterns for AI to analyze and apply
- **âœ… Best Practices**: Automatic adherence to industry standards
- **ðŸ”— Cross-Repository Intelligence**: AI synthesizes insights across multiple approaches
- **ðŸš€ Development Acceleration**: 5-10x faster implementation through proven patterns

## ðŸ”’ **ENVIRONMENT STABILIZATION RULES (CRITICAL)**

### **ðŸŽ¯ PRODUCTION-FIRST ENVIRONMENT POLICY**
**MANDATORY RULES - NEVER BREAK THESE:**

1. **ALWAYS DEFAULT TO PRODUCTION**
   ```bash
   # CORRECT - Always default to production
   ENVIRONMENT="${ENV:-prod}"

   # WRONG - Never default to staging
   ENVIRONMENT="${ENV:-staging}"  # âŒ FORBIDDEN
   ```

2. **ENVIRONMENT VARIABLE HIERARCHY**
   ```python
   # Priority order for environment detection:
   # 1. Explicit ENVIRONMENT variable (highest priority)
   # 2. Git branch detection (main=prod, develop=staging)
   # 3. Pulumi stack context
   # 4. ALWAYS fallback to "prod" (never staging, never fail)
   ```

3. **STACK NAMING STANDARDS**
   ```python
   STACK_MAPPING = {
       "prod": "sophia-ai-production",          # âœ… Production stack
       "staging": "sophia-ai-platform-staging", # âœ… Staging stack
       "dev": "sophia-ai-platform-dev"          # âœ… Development stack
   }
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **ðŸ³ DOCKER CLOUD DEPLOYMENT RULES**

1. **ALL Docker deployments target Lambda Labs (NOT local):**
   ```yaml
   # Primary deployment: docker-compose.cloud.yml
   # Target: 146.235.200.1 (Lambda Labs)
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: Docker Swarm
   ```

2. **Container environment configuration:**
   ```dockerfile
   ENV ENVIRONMENT=prod
   ENV PULUMI_ORG=scoobyjava-org
   # Secrets via Docker Secrets, NOT environment variables
   ```

3. **Docker Compose for production MUST use:**
   ```yaml
   # docker-compose.cloud.yml with:
   - mode: replicated  # Swarm mode
   - secrets:          # Docker secrets
   - deploy:           # Swarm deployment config
   - healthcheck:      # Required for all services
   ```

4. **NEVER use local Docker deployment:**
   - âŒ No `docker-compose up` locally
   - âŒ No `.env` files
   - âœ… Use `docker stack deploy -c docker-compose.cloud.yml sophia-ai`
   - âœ… All secrets via Pulumi ESC

### **ðŸ”§ MCP SERVER ENVIRONMENT RULES**

1. **ALL MCP servers MUST validate environment on startup**
2. **ALL MCP servers MUST use centralized environment detection**
3. **NO hardcoded environment values in MCP server code**
4. **Environment health checks MUST be included**

### **ðŸ“ CODING STANDARDS WITH ENVIRONMENT AWARENESS**

1. **Environment Detection Pattern:**
   ```python
   # CORRECT - Use centralized environment detection
   from backend.core.auto_esc_config import get_config_value

   # WRONG - Direct environment variable access
   os.getenv("SOME_SECRET")  # âŒ Use centralized config instead
   ```

2. **Error Handling with Environment Context:**
   ```python
   try:
       config_value = get_config_value("some_key")
   except Exception as e:
       logger.error(f"Config error in {os.getenv('ENVIRONMENT', 'unknown')} environment: {e}")
       # Always provide fallback
   ```

3. **Health Check Integration:**
   ```python
   # ALL services MUST include environment health validation
   def validate_environment():
       env = os.getenv("ENVIRONMENT")
       if env != "prod":
           logger.warning(f"Not in production environment: {env}")
       return env in ["prod", "staging", "dev"]
   ```

## ðŸ§  **ENHANCED MCP INTEGRATION WITH CLINE v3.18**

### **ðŸ“ HOW TO USE CLINE v3.18 FEATURES (You're Already Here!)**

**IMPORTANT**: The chat window where you interact with Cline IS the interface for all v3.18 features. You don't need to look for another icon or panel - everything works right here in this chat!

### **ðŸš€ Natural Language Commands in THIS Chat**

Just type these commands naturally in the Cline chat (where you're typing now):

#### **AI Memory Commands**
- **"Remember this [topic/decision/code]"** - Stores in AI memory
- **"What did we decide about [topic]?"** - Recalls past decisions
- **"Show similar [patterns/code]"** - Finds related implementations

#### **Large File Processing (FREE with Gemini!)**
- **"Process this large file with Gemini"** - Uses free Gemini for big files
- **"Analyze this 500K token document"** - Auto-routes to Gemini 2.5 Pro
- **"Summarize our 90-day Slack history"** - Handles massive datasets

#### **Web Content Fetching**
- **"Fetch docs from [URL]"** - Retrieves and converts to markdown
- **"Get latest [topic] from web"** - Searches and summarizes
- **"Download competitor info from [website]"** - Competitive intelligence

#### **Business Tool Integration**
- **Linear**: "Create Linear issue for [task]", "Show my Linear tasks"
- **Snowflake**: "Query Snowflake for [data]", "Run large query with Gemini"
- **Slack**: "Analyze #[channel]", "Find messages about [topic]"
- **Gong**: "Summarize recent calls", "Find calls discussing [keyword]"

### **ðŸ”„ AUTOMATIC WORKFLOW INTEGRATION**

#### **AI Memory Auto-Discovery**
1. **INTELLIGENT AUTO-STORAGE**: Cline automatically detects and stores:
   - Architecture discussions with decision rationale
   - Bug fixes with root cause analysis
   - Code patterns and implementation strategies
   - Performance optimization insights
   - Security implementation decisions
   - Refactoring approaches and outcomes

2. **CONTEXT-AWARE RECALL**: Before any coding task, Cline automatically:
   - Queries relevant past decisions
   - Surfaces similar patterns from project history
   - Provides continuity with previous architectural choices
   - Suggests proven solutions from past implementations

3. **SIMPLE NATURAL LANGUAGE USE**:
   - Just say: "Remember this architectural decision"
   - Just ask: "What did we decide about the database?"
   - Just request: "Show me similar bug fixes"

#### **Real-time Code Analysis (@codacy)**
1. **AUTOMATIC CODE QUALITY**: On every significant code change:
   - Real-time security vulnerability scanning
   - Code complexity analysis with refactoring suggestions
   - Style compliance checking (Black, PEP 8)
   - Performance pattern detection

2. **INTELLIGENT SUGGESTIONS**: Proactive recommendations:
   - Security best practices for detected patterns
   - Refactoring opportunities for complex functions
   - Code quality improvements with examples
   - Architecture alignment with project standards

3. **ENHANCED CODACY TOOLS**:
   - `codacy.analyze_code`: Real-time code snippet analysis
   - `codacy.analyze_file`: Complete file quality assessment
   - `codacy.get_fix_suggestions`: Automated improvement recommendations
   - `codacy.security_scan`: Focused security vulnerability detection

### **ðŸš€ WORKFLOW AUTOMATION TRIGGERS**

#### **Automatic Triggers (No User Input Required)**:
- **On File Save**: `@codacy.analyze_file` + `@ai_memory.auto_store_context`
- **On Architecture Discussion**: `@ai_memory.store_conversation(category="architecture")`
- **On Bug Fix**: `@ai_memory.store_conversation(category="bug_solution")` + `@codacy.security_scan`
- **On Code Review**: `@codacy.analyze_code` + `@ai_memory.recall_memory("similar patterns")`

#### **Smart Context Awareness**:
- **File-Specific Memory**: Automatically recall memories related to current file
- **Project Pattern Recognition**: Surface relevant architectural decisions
- **Security Context**: Auto-scan for security issues in sensitive code areas
- **Performance Awareness**: Detect performance-critical code sections

### **ðŸŽ¯ ENHANCED NATURAL LANGUAGE COMMANDS**

#### **Memory Operations**:
- "Remember this architectural decision" â†’ Auto-categorize and store with context
- "What did we decide about database schema?" â†’ Smart recall with file context
- "Show me similar bug fixes" â†’ Context-aware pattern matching
- "Store this conversation about MCP integration" â†’ Enhanced storage with metadata

#### **Code Quality Operations**:
- "Analyze this code for security issues" â†’ Comprehensive security scan
- "Check code quality" â†’ Multi-dimensional analysis with suggestions
- "Fix this function complexity" â†’ Automated refactoring recommendations
- "Scan for vulnerabilities" â†’ Deep security pattern analysis

#### **Integrated Workflows**:
- "Review and remember this implementation" â†’ Codacy analysis + Memory storage
- "Find similar patterns and analyze quality" â†’ Memory recall + Code analysis
- "Store this bug fix and scan for similar issues" â†’ Memory storage + Security scan

### **ðŸ“Š INTELLIGENT REPORTING**

#### **Development Insights**:
- Automatic pattern recognition across stored memories
- Code quality trends over time
- Security vulnerability patterns
- Architecture evolution tracking

#### **Proactive Recommendations**:
- Suggest architectural improvements based on stored decisions
- Recommend security enhancements from vulnerability patterns
- Propose refactoring based on complexity analysis
- Guide development based on successful past patterns

### **ðŸ”§ CURSOR IDE INTEGRATION SPECIFICS**

#### **Configuration Requirements**:
- MCP servers running on specified ports (ai_memory: 9000, codacy: 3008)
- Auto-trigger workflows enabled in cursor_mcp_config.json
- Context awareness enabled for file-specific operations
- Intelligent routing for multi-tool operations

#### **Performance Optimization**:
- Parallel tool execution for independent operations
- Smart caching of frequently accessed memories
- Efficient code analysis with incremental scanning
- Context-aware tool selection based on current activity

### **Example Enhanced Workflow**:
```
User: "I need to implement user authentication for the MCP server"

Cursor AI (Automatic Sequence):
1. [AUTO] @ai_memory.smart_recall("authentication MCP server implementation")
2. [CONTEXT] Retrieve: Previous auth patterns, security decisions, MCP integration approaches
3. [ANALYSIS] @codacy.analyze_code(current_auth_code) for security assessment
4. [EXTERNAL] Check microsoft_playwright and anthropic-mcp-servers for auth patterns
5. [RESPONSE] Provide implementation guidance based on stored patterns + security analysis + community validation
6. [AUTO] @ai_memory.auto_store_context(conversation + implementation decisions)
```

This enhanced integration transforms Cursor AI into an intelligent development partner that learns from every interaction and provides contextually aware assistance.

## Development Standards

### Python Code Style
- Use Python 3.11+ with type hints for all functions
- Follow PEP 8 with 88-character line limit (Black formatter)
- Use async/await for I/O operations
- Implement comprehensive error handling with logging
- Include detailed docstrings for all classes and methods

### Agent Development Pattern

**During Transition - USE THIS**:
```python
from backend.agents.core.base_agent import BaseAgent

class YourAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        # Agent-specific initialization

    async def execute_task(self, task: Task) -> TaskResult:
        # Implementation with error handling
        pass
```

**After Migration (DO NOT USE YET)**:
```python
from libs.core.agents.base_agent import BaseAgent

class YourAgent(BaseAgent):
    # Same implementation, different import
```

### Integration Pattern
```python
class ServiceIntegration:
    def __init__(self, config: ServiceConfig):
        self.config = config
        self.client = self._create_client()

    async def _make_request(self, method: str, endpoint: str, **kwargs):
        # Standardized request handling with rate limiting
        pass
```

### Business Intelligence Focus
- Always consider Pay Ready business context
- Implement metrics for revenue, customer health, sales performance
- Focus on actionable insights for sales coaching and client monitoring
- Prioritize real-time data processing and notifications

### Security Requirements
- Use encrypted storage for all API keys
- Implement proper authentication and authorization
- Log all security-relevant events
- Follow principle of least privilege

### **Secret Management (PERMANENT SOLUTION)**
- **Documentation:** Always refer to `PERMANENT_GITHUB_ORG_SECRETS_SOLUTION.md`
- **GitHub Organization:** All secrets managed at [https://github.com/ai-cherry](https://github.com/ai-cherry)
- **Pulumi ESC:** Automatic secret synchronization via `scoobyjava-org/default/sophia-ai-production`
- **Backend Integration:** Use `backend/core/auto_esc_config.py` for automatic secret loading
- **Never hardcode secrets:** Always use automatic ESC integration
- **GitHub Actions:** Secrets automatically available from organization level
- **Local Development:** Set `export PULUMI_ORG=scoobyjava-org` and secrets load automatically
- **Secret Rotation:** Update in GitHub organization â†’ automatic sync â†’ automatic deployment

### Testing Strategy
- Write unit tests for all business logic
- Include integration tests for external APIs
- Implement performance tests for critical paths
- Use pytest with async support

### Error Handling Pattern
```python
try:
    result = await some_operation()
    return result
except SpecificException as e:
    logger.error(f"Operation failed: {e}")
    raise BusinessLogicError(f"Failed to process: {e}")
except Exception as e:
    logger.exception("Unexpected error")
    raise SystemError("Internal system error")
```

## Business Domain Knowledge

### Pay Ready Context
- Company focus: Business intelligence and automation
- Key metrics: Revenue growth, customer satisfaction, sales efficiency
- Team communication: Primarily through Slack
- CRM system: HubSpot for contact and deal management
- Call analysis: Gong.io for sales call insights

### Agent Specializations
- **Call Analysis Agent:** Process Gong.io recordings for insights
- **CRM Sync Agent:** Maintain HubSpot data quality and synchronization
- **Notification Agent:** Send intelligent Slack updates
- **Business Intelligence Agent:** Generate revenue and performance reports

### Integration Priorities
1. **HubSpot:** Primary CRM for contact/deal management
2. **Gong.io:** Critical for call analysis and sales coaching
3. **Slack:** Main communication channel for team updates
4. **Vector Databases:** For semantic search and AI capabilities

## ðŸš¨ MONOREPO TRANSITION IN PROGRESS

**CRITICAL**: We are transitioning to a monorepo structure. During this transition:
- **Continue using the OLD structure** for new code (`backend/`, `frontend/`, etc.)
- **DO NOT use the new structure** (`apps/`, `libs/`) until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for current status

### Current File Organization (USE THIS)
```
backend/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ core/           # Base agent classes
â”‚   â””â”€â”€ specialized/    # Domain-specific agents
â”œâ”€â”€ integrations/       # External service integrations
â”œâ”€â”€ database/          # Data layer and migrations
â”œâ”€â”€ monitoring/        # Performance and health monitoring
â””â”€â”€ security/          # Authentication and encryption

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â””â”€â”€ services/      # API clients

external/              # ðŸ†• Strategic MCP repository collection
â”œâ”€â”€ microsoft_playwright/    # Browser automation (13.4k stars)
â”œâ”€â”€ glips_figma_context/    # Design-to-code (8.7k stars)
â”œâ”€â”€ snowflake_cortex_official/ # Official Snowflake AI
â””â”€â”€ [8 additional strategic repos]
```

### Future File Organization (DO NOT USE YET)
```
apps/                  # Monorepo applications
â”œâ”€â”€ api/              # Backend API (from backend/api)
â”œâ”€â”€ frontend/         # React frontend
â”œâ”€â”€ mcp-servers/      # All MCP servers
â””â”€â”€ n8n-bridge/       # N8N integration

libs/                  # Shared libraries
â”œâ”€â”€ ui/               # Shared UI components
â”œâ”€â”€ utils/            # Shared utilities
â”œâ”€â”€ types/            # Shared TypeScript types
â””â”€â”€ core/             # Core business logic

config/               # Centralized configurations
â”œâ”€â”€ eslint/
â”œâ”€â”€ prettier/
â”œâ”€â”€ typescript/
â””â”€â”€ ruff/
```

## Common Patterns

### API Client Implementation
- Use aiohttp for async HTTP requests
- Implement exponential backoff for retries
- Respect rate limits with proper throttling
- Include comprehensive error handling

### Database Operations
- Use SQLAlchemy with async support
- Implement proper connection pooling
- Use transactions for data consistency
- Include migration scripts for schema changes

### Monitoring and Logging
- Use structured logging with JSON format
- Include correlation IDs for request tracing
- Monitor performance metrics and business KPIs
- Implement health checks for all services

## AI and ML Guidelines
- Use OpenAI API for language processing
- Implement vector search with Pinecone/Weaviate
- Cache embeddings for performance
- Include confidence scores in AI responses

## Deployment Considerations
- **Target**: Lambda Labs infrastructure ONLY (146.235.200.1)
- **Docker Cloud**: Use docker-compose.cloud.yml with Docker Swarm
- **Registry**: Push all images to scoobyjava15 Docker Hub
- **Secrets**: All secrets via Pulumi ESC (NO .env files)
- **Deployment**: `docker stack deploy -c docker-compose.cloud.yml sophia-ai`
- **Scaling**: Use Swarm replicas and resource limits
- **Zero-downtime**: Rolling updates with health checks

## Performance Requirements
- API response times < 200ms for critical paths
- Database queries < 100ms average
- Vector searches < 50ms average
- Support for 1000+ concurrent users

## When suggesting code:
1. **FIRST**: Check AI memory for similar implementations
2. **EXTERNAL REPOS**: Leverage patterns from 11 strategic external repositories (microsoft_playwright, glips_figma_context, snowflake_cortex_official, etc.)
3. Always include proper error handling
4. Add type hints and docstrings
5. Consider business context and Pay Ready needs
6. Implement monitoring and logging
7. Follow the established patterns in the codebase
8. Prioritize performance and scalability
9. Include relevant tests
10. **VALIDATE ENVIRONMENT**: Ensure ENVIRONMENT="prod" is used
11. **COMMUNITY PATTERNS**: Apply proven patterns from 22k+ star external repositories
12. **LAST**: Store the conversation in AI memory

## Avoid:
- Hardcoded values (use configuration)
- Synchronous I/O in async contexts
- Missing error handling
- Unclear variable names
- Complex nested logic without comments
- Security vulnerabilities (exposed secrets, etc.)
- **DEFAULTING TO STAGING ENVIRONMENT** (Always use production)
- **MANUAL ENVIRONMENT VARIABLE SETUP** (Use centralized config)
- **IGNORING EXTERNAL REPOSITORY PATTERNS** (Always check for proven community approaches)

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. Leverage the collective intelligence of 22k+ star external repositories to ensure world-class implementation patterns.

### Infrastructure as Code Integration
- **Pulumi Commands**: Use `pulumi up`, `pulumi preview`, `pulumi destroy` for infrastructure management
- **ESC Operations**: Use scripts in `infrastructure/esc/` for secret management
- **GitHub Integration**: All deployments go through GitHub Actions workflows
- **MCP Integration**: Use `mcp_config.json` for MCP server configuration

### ðŸš€ Cline v3.18 Enhanced Features Integration

#### Claude 4 & Gemini 2.5 Pro Optimization
- **Model Selection**: Automatic routing based on task complexity and context size
- **Large Context**: Use Gemini for documents > 100K tokens (up to 1M)
- **Complex Reasoning**: Claude 4 for architectural design and code generation
- **Data Processing**: Snowflake Cortex for SQL and data operations
- **Cost Optimization**: Automatic routing to free Gemini CLI for large contexts

#### Gemini CLI Integration (NEW)
- **Free Access**: Use local Gemini CLI for zero-cost processing
- **Auto-routing**: "Process this large file with Gemini" â†’ Routes to CLI
- **Batch Processing**: Efficient handling of multiple large documents
- **Context Preservation**: Maintain context across CLI calls

#### WebFetch Tool Usage (ENHANCED)
- **Documentation Retrieval**: "Fetch the latest API docs from [url]"
- **Competitive Intelligence**: "Get competitor information from [website]"
- **Real-time Updates**: "Retrieve and summarize current [topic] from [source]"
- **Caching**: Automatic caching with TTL for improved performance
- **Format Support**: PDF, DOCX, HTML, and plain text extraction
- **Parallel Fetching**: Process multiple URLs simultaneously

#### Self-Knowledge Commands (ENHANCED)
- **Capabilities Discovery**: "What can the [server name] MCP server do?"
- **Feature Inspection**: "Show available features for [component]"
- **Help System**: "How do I use [feature]?"
- **Server Status**: "Check capabilities of all MCP servers"
- **Performance Metrics**: "Show performance stats for [server]"
- **Usage Analytics**: "How often do we use [feature]?"

#### Improved Diff Editing (AI-POWERED)
- **Auto-fallback**: Automatically tries exact â†’ fuzzy â†’ context-aware â†’ AI strategies
- **Success Rate**: 95%+ success rate for file modifications
- **Smart Updates**: "Update [file] using all available strategies"
- **Context Awareness**: AI-powered understanding of code changes
- **Multi-file Operations**: Apply changes across multiple files
- **Rollback Support**: Undo changes if needed

#### Enhanced AI Memory Integration (v3.18)
- **Auto-discovery**: Automatically detect and store architecture decisions
- **Smart Recall**: "What did we decide about [topic]?" â†’ Context-aware retrieval
- **Pattern Matching**: Find similar implementations across the codebase
- **WebFetch Integration**: Automatically store fetched documentation

#### Enhanced Codacy Integration (v3.18)
- **Real-time Analysis**: Analyze code as you type
- **Security Scanning**: Deep vulnerability detection
- **Performance Insights**: Identify performance bottlenecks
- **AI Suggestions**: Get AI-powered improvement recommendations

### Natural Language Infrastructure Commands
When using Cursor AI for infrastructure operations, you can use natural language:

#### Examples:
- "Deploy the infrastructure" â†’ Triggers GitHub Actions workflow
- "Get the database password" â†’ Retrieves secret from Pulumi ESC
- "Rotate API keys" â†’ Runs secret rotation framework
- "Sync secrets" â†’ Synchronizes GitHub and Pulumi ESC secrets
- "Test the deployment" â†’ Runs ESC integration tests

#### Command Patterns:
- **Secret Operations**: "get/retrieve/fetch [service] [secret_type]"
- **Deployment Operations**: "deploy/update/rollback [component]"
- **Testing Operations**: "test/validate/check [component]"
- **Configuration Operations**: "configure/setup/initialize [service]"

### MCP Server Natural Language Integration (v3.18 Enhanced)
- **Query Data**: "Get recent Gong calls" â†’ Uses Gong MCP server with model routing
- **Deploy Apps**: "Deploy to Vercel" â†’ Uses Vercel MCP server with improved diff
- **Manage Data**: "Upload to Estuary" â†’ Uses Estuary MCP server with WebFetch
- **Database Operations**: "Query Snowflake" â†’ Uses Snowflake MCP server with Cortex
- **Store Memory**: "Remember this conversation" â†’ Uses AI Memory MCP server with Claude 4
- **Recall Context**: "What did we decide about X?" â†’ Uses AI Memory with self-knowledge
- **Fetch External Data**: "Get latest docs from [url]" â†’ Uses WebFetch tool
- **Analyze Large Docs**: "Process this 500K token file" â†’ Auto-routes to Gemini 2.5 Pro

### Error Handling and Debugging
- **ESC Errors**: Check Pulumi ESC logs and validate configuration
- **GitHub Actions Errors**: Review workflow logs and artifacts
- **MCP Errors**: Check Docker container logs and health endpoints
- **Secret Errors**: Validate secret names and permissions

### Best Practices for Cursor AI Integration
1. **Use Descriptive Comments**: Add context for infrastructure operations
2. **Follow Naming Conventions**: Use consistent naming for secrets and services
3. **Document Dependencies**: Clearly document service dependencies
4. **Test Before Deploy**: Always test changes in isolation first
5. **Monitor Operations**: Use logging and monitoring for all operations
6. **Remember Context**: Always use AI Memory for persistent development context
7. **ðŸ†• Leverage External Patterns**: Always check external repositories for proven implementation approaches
8. **ðŸ†• Community Validation**: Prefer patterns from high-star repositories with community validation
9. **ðŸ†• Cross-Repository Intelligence**: Synthesize insights from multiple repository approaches

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. The strategic external repository collection provides access to 22k+ stars of community-validated patterns and proven implementation approaches.

## ðŸŽ¯ **The Unified Dashboard is the ONLY Frontend**

**CRITICAL RULE:** All new frontend development MUST extend the one, true `UnifiedDashboard.tsx` component.

1.  **NO NEW DASHBOARDS:** Do not create new, separate dashboard components or pages. All new views, tabs, or features must be integrated into the existing `UnifiedDashboard.tsx` tabbed interface.
2.  **EXTEND, DON'T REPLACE:** Use the existing components (`UnifiedKPICard`, etc.) and the established layout. New features should be added as new tabs or as components within existing tabs.
3.  **SINGLE API CLIENT:** All frontend API calls MUST use the `frontend/src/services/apiClient.js`. Do not create new API clients.
4.  **DOCUMENTATION IS LAW:** All frontend architecture must align with the `docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md`. Any deviation must first be reflected in the handbook.

This ensures we maintain a single, clean, and unified frontend, preventing the fragmentation that we just worked so hard to eliminate.

# Sophia AI Development Rules

PROJECT_CONTEXT: |
  Sophia AI - Executive AI Orchestrator for Pay Ready
  Initial User: CEO only (80-employee company)
  Priority: Quality > Stability > Maintainability > Performance > Cost
  Current Phase: Building enhanced chat with citation system

CODING_STANDARDS: |
  Python:
    - Use Python 3.11+ with type hints for all functions
    - Follow async/await patterns for I/O operations
    - Use Black formatter (88 char line limit)
    - Include comprehensive docstrings
    - Error handling with proper logging

  TypeScript:
    - Strict mode enabled, no 'any' types
    - ESLint + Prettier configured
    - Interfaces over types where possible
    - JSDoc for all exported functions
    - React functional components only

  Testing:
    - TDD approach - write tests first
    - pytest for Python, Jest for TypeScript
    - Minimum 80% code coverage
    - Integration tests for all API endpoints
    - Unit tests for business logic

AI_RULES: |
  Planning:
    - Plan-Then-Act: 70% planning, 30% execution
    - Break tasks into micro-tasks (< 5 min each)
    - Define clear success criteria before coding
    - Create tests before implementation

  Development:
    - Make minimal, focused changes
    - Micro-commits with descriptive messages
    - Update documentation with each change
    - Run tests after each change
    - Review diffs before committing

  Context:
    - Always reference @docs/PROJECT_CONTEXT.md
    - Check @docs/architecture/ for patterns
    - Update @docs/AI_MEMORY.md with learnings
    - Cite all data sources in responses

ARCHITECTURE_PATTERNS: |
  Backend:
    - FastAPI for async REST APIs
    - Service layer pattern for business logic
    - Repository pattern for data access
    - Dependency injection for testability
    - Event-driven communication via Redis

  Frontend:
    - React 18 with TypeScript
    - Component composition over inheritance
    - Custom hooks for shared logic
    - Context API for global state
    - TailwindCSS for styling

  AI Integration:
    - Snowflake Cortex for all LLM operations
    - Model routing based on task complexity
    - Citation system for transparency
    - Memory system with Mem0
    - Cost tracking for all AI operations

SECURITY_RULES: |
  - Never hardcode secrets or API keys
  - Use environment variables via auto_esc_config
  - Validate all user inputs
  - Sanitize LLM outputs
  - Log security-relevant events
  - Follow principle of least privilege

QUALITY_STANDARDS: |
  - No code duplication
  - Clear variable and function names
  - Comprehensive error handling
  - Performance monitoring for all endpoints
  - Document all architectural decisions
  - Regular code reviews (even for AI-generated code)

TIMELINE_AND_BUDGET_GUIDELINES: |
  Coding Tasks:
    - NO specific timeline estimates (hours, days, weeks)
    - NO budget estimates for development work
    - Focus on technical implementation details only
    - Break down tasks by complexity, not time
    - Describe phases by functionality, not duration

  Business Planning:
    - Timeline estimates only when explicitly requested for business planning
    - Budget discussions only for infrastructure/service costs
    - Keep business metrics separate from coding tasks

  Documentation:
    - Use phase numbers or functional milestones
    - Describe dependencies and prerequisites
    - Focus on "what" and "how", not "when"
    - Technical complexity over time estimates
