## ⚠️ **MONOREPO TRANSITION IN PROGRESS**

**CRITICAL FOR AI CODERS**:
- We are transitioning from the old structure (`backend/`, `frontend/`) to a new monorepo structure (`apps/`, `libs/`)
- **Continue using the OLD structure** for all new code until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for details
- Target completion: February 2025

## 🚀 **THE GOLDEN RULE OF DEPLOYMENT (MANDATORY)**

**ALL DEPLOYMENTS ARE AUTOMATED VIA GITHUB ACTIONS. NEVER DEPLOY FROM A LOCAL MACHINE.**

### **Core Principles (Non-Negotiable):**
1.  **Single Point of Truth**: The `main` branch on GitHub is the single source of truth for what is deployed.
2.  **Push-to-Deploy**: All deployments to any environment (Production, Staging) are triggered **exclusively** by a `git push` to the relevant branch.
3.  **No Local Operations**: You MUST NOT run `docker build`, `docker push`, or `kubectl apply` from your local machine for deployment purposes. Your role is to write the code and the GitHub Actions workflows; the cloud does the rest.
4.  **Automated Secret Management**: All secrets (`DOCKER_HUB_ACCESS_TOKEN`, `LAMBDA_LABS_SSH_KEY`, etc.) are managed in GitHub Organization Secrets and injected into the GitHub Actions workflow at runtime. You MUST NOT handle credentials locally.

### **The Correct Workflow:**
1.  **Write Code**: Implement the feature or fix.
2.  **Write/Update Workflow**: Ensure the `.github/workflows/` configuration is correct for building and deploying the service.
3.  **Push to GitHub**: `git push origin main`.
4.  **Monitor Action**: Observe the deployment progress in the GitHub Actions tab.

This is the **only** approved method. Any deviation from this (e.g., suggesting `docker login` locally) is a violation of core operating principles.

## 🌟 **NEW: The System Handbook is the Ultimate Source of Truth**

**CRITICAL DIRECTIVE:** The `docs/system_handbook/` directory, and specifically `00_SOPHIA_AI_SYSTEM_HANDBOOK.md`, is the **single, definitive source of truth** for the Sophia AI platform's architecture, standards, and roadmap.

### **Your Core Responsibilities Regarding Documentation:**

1.  **CONSULT FIRST, CODE LATER:** Before answering any architectural question or implementing any new feature, you **MUST** first consult the System Handbook to understand the established patterns and principles.
2.  **PROACTIVELY UPDATE:** If you make a significant architectural change (e.g., add a new service, change a data flow, modify the MCP gateway), you **MUST** update the relevant section of the System Handbook within the same session.
3.  **MAINTAIN STRUCTURE:** When adding documentation, you **MUST** follow the existing structure and format. If a new deep-dive document is needed, create it within the `system_handbook` directory and link to it from the master file.
4.  **CITE YOUR SOURCE:** When you use information from the handbook to make a decision, you should briefly mention it (e.g., "Based on the Phoenix Plan architecture outlined in the System Handbook, I will...").

This ensures our documentation remains a living, breathing, and accurate representation of the platform, empowering both human and AI developers.

# Sophia AI Pay Ready Platform - Cursor AI Rules

## Project Overview
You are working on Sophia AI, an AI assistant orchestrator for Pay Ready company. Sophia serves as the central "Pay Ready Brain" that orchestrates multiple AI agents and integrates with business systems.

### Company Context
- **Company Size:** 80 employees total
- **Initial Users:** 1 (CEO - primary developer and user)
- **Rollout Plan:** CEO only → Few super users (2-3 months) → Full company (6+ months)
- **Development Team:** CEO (sole human developer) + AI assistants

## 🎯 **Development Priorities (CRITICAL)**

### **Priority Order - NEVER COMPROMISE:**
1. **QUALITY & CORRECTNESS** - Every line of code must be correct and well-structured
2. **STABILITY & RELIABILITY** - System must be rock-solid for CEO usage
3. **MAINTAINABILITY** - Code must be clear and easy to modify
4. **PERFORMANCE** - Important but secondary to quality
5. **COST & SECURITY** - Consider but don't over-optimize at this stage

### **Quality Standards:**
- **Zero Duplication:** Never duplicate code or functionality
- **Clear Dependencies:** All dependencies must be explicit and documented
- **Conflict Prevention:** Check for conflicts before implementing
- **Structure First:** Plan structure to avoid future issues
- **Review Everything:** Always review context before coding

### **Tool Selection Principle:**
> **Only add new tools when there's a clear gap that existing tools cannot fill.**

This principle prevents:
- Tool proliferation and complexity creep
- Duplicate functionality across different tools
- Maintenance burden from unnecessary dependencies
- Migration overhead from overlapping solutions

Before adding any new tool or framework:
1. Check if existing tools can solve the problem
2. Document the specific gap the new tool fills
3. Consider the long-term maintenance cost
4. Prefer enhancing existing tools over adding new ones

## 🧹 **ENHANCED CODE HYGIENE RULES (MANDATORY)**

### **Automated Technical Debt Prevention:**
Our "Clean by Design" framework prevents the accumulation of technical debt through:

1. **Automated Daily Cleanup**: Removes expired one-time scripts, backup files, and empty archive directories
2. **Pre-Commit Blocking**: Prevents commits that would introduce technical debt patterns
3. **Documentation Lifecycle**: Automatic archiving of temporary documentation
4. **Zero Tolerance Enforcement**: Immediate rejection of forbidden patterns

### **One-Time Script Management (Enhanced):**
```python
# REQUIRED: All one-time scripts must use this pattern
"""
🚨 ONE-TIME SCRIPT - DELETE AFTER USE
Purpose: [specific purpose]
Created: [date]
DELETE AFTER: [date]
Usage: python scripts/one_time/script_name_DELETE_YYYY_MM_DD.py

🧹 CLEANUP: This script will be auto-deleted after expiration date
"""

# At the end of any one-time script:
print("✅ Task completed successfully")
print(f"🧹 This script will auto-delete on: {DELETE_DATE}")
print("📍 Located in: scripts/one_time/ for automatic cleanup")
```

### **File Creation Decision Tree:**
```
Creating a file? → Ask: "Will this be used more than once?"
├── NO (One-time use)
│   ├── Script → scripts/one_time/name_DELETE_YYYY_MM_DD.py
│   ├── Doc → Mark for auto-archive with retention period
│   └── Data → /tmp/ or delete immediately after use
└── YES (Permanent)
    ├── Utility → scripts/utils/
    ├── Monitoring → scripts/monitoring/
    ├── Reference Doc → docs/99-reference/
    └── Core Logic → appropriate permanent directory
```

### **Automated Enforcement:**
- **Daily**: `python scripts/utils/daily_cleanup.py` (runs automatically)
- **Pre-commit**: `python scripts/utils/pre_push_debt_check.py` (blocks bad commits)
- **Monitoring**: Technical debt score tracking and alerting

## 📅 **Timeline and Budget Guidelines (CRITICAL)**

### **For Coding Tasks - NEVER INCLUDE:**
- ❌ **NO specific time estimates** (hours, days, weeks, months)
- ❌ **NO budget estimates** for development work
- ❌ **NO duration predictions** for implementation
- ❌ **NO cost calculations** for coding effort
- ❌ **NO timeline-based milestones**

### **For Coding Tasks - ALWAYS USE:**
- ✅ **Phase numbers** (Phase 1, Phase 2.5, etc.)
- ✅ **Functional milestones** (Foundation Complete, Integration Ready)
- ✅ **Feature-based progress** (Intent Classification ✅, Code Modification ✅)
- ✅ **Technical dependencies** (Requires X before Y)
- ✅ **Complexity indicators** (Simple, Moderate, Complex)

### **Why This Matters:**
- Time estimates for AI-assisted coding are meaningless and wasteful
- Focus should be on technical implementation, not scheduling
- Quality and correctness matter more than speed
- Each coding session is unique and unpredictable

### **Exception - Business Planning Only:**
- Timeline estimates ONLY when explicitly requested for business planning
- Infrastructure cost estimates when evaluating services
- ROI calculations for business decisions
- Keep business metrics completely separate from coding tasks

## Architecture Context
- **Type:** Multi-agent AI orchestrator for CEO-level business intelligence
- **Primary User:** Pay Ready CEO (initial sole user)
- **Core Integrations:** HubSpot CRM, Gong.io call analysis, Slack communication
- **Data Stack:** PostgreSQL, Redis, Pinecone, Qdrant (Weaviate eliminated)
- **Infrastructure:** Lambda Labs servers, Lambda Labs frontend deployment
- **External Repository Collection:** 11 strategic MCP servers (22k+ combined stars)

## 🔐 **PERMANENT SECRET MANAGEMENT SOLUTION - MANDATORY REFERENCE**

**CRITICAL**: Before suggesting ANY secret management solution, you MUST:

1. **READ**: `docs/99-reference/PERMANENT_SECRET_MANAGEMENT_SOLUTION.md`
2. **VERIFY**: The secret exists in GitHub Organization Secrets
3. **USE**: `get_config_value()` from `backend/core/auto_esc_config.py`
4. **NEVER**: Create manual secret management

### **Docker Hub Credentials - ALWAYS AVAILABLE**
```python
from backend.core.auto_esc_config import get_docker_hub_config
config = get_docker_hub_config()
# config["username"] = "scoobyjava15"
# config["access_token"] contains the real Docker Hub token
# config["registry"] = "docker.io"
```

### **Secret Priority Order**
1. **Pulumi ESC** (automatic via `get_config_value()`)
2. **Environment Variables** (fallback)
3. **Defaults** (hardcoded fallbacks only)

**NEVER** skip to manual solutions without checking the automated infrastructure first.

### **Common Secret Access Patterns**
```python
# Docker Hub
from backend.core.auto_esc_config import get_docker_hub_config
docker = get_docker_hub_config()

# qdrant
from backend.core.auto_esc_config import get_qdrant_config
qdrant = get_qdrant_config()

# Lambda Labs
from backend.core.auto_esc_config import get_lambda_labs_config
lambda_labs = get_lambda_labs_config()

# Any other secret
from backend.core.auto_esc_config import get_config_value
secret = get_config_value("secret_name")
```

### **GitHub Actions Already Has Access**
All secrets are automatically available in GitHub Actions workflows:
- `${{ secrets.DOCKER_HUB_USERNAME }}`
- `${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}`
- `${{ secrets.LAMBDA_PRIVATE_SSH_KEY }}`
- etc.

**DO NOT** suggest manual authentication when using GitHub Actions!

## 🤖 **UNIFIED AI AGENT AUTHENTICATION SYSTEM**

### **🚀 REVOLUTIONARY CAPABILITY: AI Agents Can Make REAL CHANGES**

Sophia AI features a **REVOLUTIONARY** Unified AI Agent Authentication System that enables AI coding agents to make **REAL CHANGES** across the entire technology stack with enterprise-grade security.

### **🏗️ Three-Tier Security Architecture**

#### **Tier 1: CLI-Based Authentication (Highest Security)**
Services that use CLI-based authentication with secure credential storage:
- **GitHub**: `gh auth login` with secure token storage
- **Pulumi**: `pulumi login` with organization access
- **Docker**: `docker login` for registry operations
- **Lambda Labs**: Direct deployment to Lambda Labs servers

#### **Tier 2: Enhanced API Authentication**
Services with enhanced security patterns:
- **qdrant**: Secure connection strings with role-based access
- **Lambda Labs**: API key management with instance control
- **Estuary Flow**: Service account authentication

#### **Tier 3: Secure API Key Management**
Standard API integrations with secure key management:
- **OpenAI, Anthropic, Slack, Linear, HubSpot**: Automatic ESC integration

### **🤖 Agent Permission Matrix**

#### **Infrastructure Agent (CRITICAL Risk)**
Can perform infrastructure-level operations:
```python
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager

auth_manager = UnifiedServiceAuthManager()

# Deploy infrastructure
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="pulumi",
    operation="infrastructure_deployment",
    params={"stack": "production", "config": "updated_resources"}
)

# Manage containers
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="docker",
    operation="container_management",
    params={"action": "deploy", "service": "sophia-ai"}
)
```

#### **Data Agent (HIGH Risk)**
Can perform data operations:
```python
# Execute database operations
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="qdrant",
    operation="query_execution",
    params={"query": "CREATE SCHEMA AI_AGENT_TEST", "warehouse": "SOPHIA_AI_COMPUTE_WH"}
)

# Manage data flows
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="estuary_flow",
    operation="flow_management",
    params={"action": "create", "collection": "ai_agent_data"}
)
```

#### **Integration Agent (MEDIUM Risk)**
Can perform business tool operations:
```python
# Create project tickets
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="linear",
    operation="ticket_creation",
    params={"title": "AI Agent Test", "team": "sophia-dev"}
)

# Send notifications
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="slack",
    operation="message_send",
    params={"channel": "#ai-agents", "message": "Deployment completed"}
)
```

### **🛡️ Enterprise Security Features**

#### **Zero Trust Authentication**
- Every operation requires explicit authentication
- No persistent credentials in AI agent memory
- All operations logged with full audit trail

#### **Risk-Based Confirmation Workflows**
```python
# CRITICAL operations require explicit confirmation
confirmation = await auth_manager.get_operation_confirmation(
    operation="infrastructure_deployment",
    risk_level=RiskLevel.CRITICAL,
    details="Deploying new Pulumi stack to production"
)

if confirmation.approved:
    await auth_manager.execute_operation(...)
```

#### **Complete Audit Trail**
- All AI agent operations logged to database
- Risk assessment recorded for each operation
- User confirmation workflows tracked
- Security compliance reporting

### **🚀 Natural Language Commands for AI Agents**

#### **Infrastructure Operations**
```bash
"Deploy the updated infrastructure to production"
"Scale up the qdrant warehouse for the analytics job"
"Create a new Docker service for the MCP gateway"
"Update the Lambda Labs deployment with latest frontend changes"
```

#### **Data Operations**
```bash
"Create a new schema for the AI agent testing"
"Run the quarterly revenue analysis query"
"Set up a new data flow from HubSpot to qdrant"
"Backup the production database before the migration"
```

#### **Business Tool Integration**
```bash
"Create a Linear ticket for the authentication bug"
"Send a Slack message about the deployment status"
"Update the HubSpot deal with the latest information"
"Schedule a GitHub Action workflow for the nightly build"
```

### **📋 Setup Commands**

#### **Phase 1: CLI Authentication Setup**
```bash
# Run the setup script
python scripts/setup_unified_ai_agent_auth.py

# Verify authentication
python -c "from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager; auth = UnifiedServiceAuthManager(); print('✅ Authentication system ready')"
```

#### **Phase 2: Permission Configuration**
```bash
# Configure agent permissions
python -c "
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager
auth = UnifiedServiceAuthManager()
auth.configure_agent_permissions()
print('✅ Agent permissions configured')
"
```

### **🎯 Business Value**

#### **Revolutionary Capabilities**
- **AI agents can make REAL infrastructure changes** with enterprise security
- **Natural language interface** for complex technical operations
- **Zero credential exposure** through secure CLI authentication
- **Complete audit compliance** for all AI-driven changes

#### **Enterprise Benefits**
- **100% Automated Operations**: AI agents handle routine deployments
- **Zero Manual Credential Management**: All authentication automated
- **Complete Security Compliance**: Enterprise-grade audit trails
- **Real-Time Infrastructure Management**: Immediate response to operational needs

### **📚 Documentation Reference**
- **Complete Guide**: `docs/99-reference/UNIFIED_AI_AGENT_AUTHENTICATION.md`
- **Security Manager**: `backend/security/unified_service_auth_manager.py`
- **Setup Script**: `scripts/setup_unified_ai_agent_auth.py`

**🚨 CRITICAL**: This system enables AI agents to make REAL changes across your entire technology stack. All operations are secured, audited, and require appropriate confirmations based on risk levels.

## 🚀 **STRATEGIC EXTERNAL REPOSITORY INTEGRATION**

### **🎯 AI-Enhanced Development with Community Patterns**
Sophia AI leverages 11 strategic external repositories to provide world-class AI coding assistance:

#### **🏗️ Infrastructure & Automation**
- **microsoft_playwright** (13.4k stars): Browser automation, E2E testing patterns
- **anthropic-mcp-servers**: Official MCP implementations and best practices
- **anthropic-mcp-inspector**: MCP debugging and development tools

#### **🎨 Design & Creative**
- **glips_figma_context** (8.7k stars): Design-to-code workflows, component generation
- **V0.dev Integration** (NEW): AI-powered UI generation with natural language commands

#### **❄️ Data Intelligence**
- **qdrant_memory_official**: Official qdrant AI integration patterns
- **davidamom_qdrant**: Community qdrant implementation approaches
- **dynamike_qdrant**: Performance-optimized qdrant patterns
- **isaacwasserman_qdrant**: Specialized qdrant operations

#### **🚪 AI Gateway & Optimization**
- **portkey_admin**: AI gateway optimization, cost reduction strategies
- **openrouter_search**: 200+ AI model access and selection patterns

#### **🐍 Core Framework**
- **anthropic-mcp-python-sdk**: MCP protocol implementation patterns

### **🧠 Natural Language Commands for External Repositories**
```bash
# Repository discovery and pattern analysis
"What browser automation patterns do we have from Microsoft Playwright?"
"Show me qdrant optimization strategies across our repositories"
"Find design-to-code patterns from the Figma integration"

# Implementation with community validation
"Use Playwright patterns to implement comprehensive E2E testing"
"Apply qdrant optimization patterns from our 4 repository collection"
"Generate components using GLips Figma design-to-code workflows"

# Cross-repository intelligence
"Compare authentication patterns across all external repositories"
"Find performance optimization strategies from high-star repos"
"Show security patterns used by official implementations"

# AI-Powered UI Generation (V0.dev Integration via Unified Chat)
# Note: Type these directly in the unified chat - no @ commands needed
"Create a modern dashboard component with glassmorphism styling"
"Build a responsive navigation bar with dropdown menus"
"Generate a data table component with sorting and filtering"
"Design a modal dialog with form validation"
"Create a chart component for revenue visualization"
"Build a user profile card with avatar and social links"
```

### **🔍 AI Pattern Recognition & Learning**
The external repository collection enables:
- **📚 Pattern Library**: 22k+ star repositories with proven community validation
- **🧠 AI Learning**: Rich implementation patterns for AI to analyze and apply
- **✅ Best Practices**: Automatic adherence to industry standards
- **🔗 Cross-Repository Intelligence**: AI synthesizes insights across multiple approaches
- **🚀 Development Acceleration**: 5-10x faster implementation through proven patterns

## 🔒 **ENVIRONMENT STABILIZATION RULES (CRITICAL)**

### **🎯 PRODUCTION-FIRST ENVIRONMENT POLICY**
**MANDATORY RULES - NEVER BREAK THESE:**

1. **ALWAYS DEFAULT TO PRODUCTION**
   ```bash
   # CORRECT - Always default to production
   ENVIRONMENT="${ENV:-prod}"

   # WRONG - Never default to staging
   ENVIRONMENT="${ENV:-staging}"  # ❌ FORBIDDEN
   ```

2. **ENVIRONMENT VARIABLE HIERARCHY**
   ```python
   # Priority order for environment detection:
   # 1. Explicit ENVIRONMENT variable (highest priority)
   # 2. Git branch detection (main=prod, develop=staging)
   # 3. Pulumi stack context
   # 4. ALWAYS fallback to "prod" (never staging, never fail)
   ```

3. **STACK NAMING STANDARDS**
   ```python
   STACK_MAPPING = {
       "prod": "sophia-ai-production",          # ✅ Production stack
       "staging": "sophia-ai-platform-staging", # ✅ Staging stack
       "dev": "sophia-ai-platform-dev"          # ✅ Development stack
   }
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   export KUBECONFIG="$HOME/.kube/k3s-lambda-labs"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **🐳 DOCKER CLOUD DEPLOYMENT RULES**

1. **ALL Docker deployments target Lambda Labs (NOT local):**
   ```yaml
   # Primary deployment: K3s cluster on Lambda Labs
   # Target: 192.222.58.232 (Lambda Labs)
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: K3s Kubernetes
   ```

2. **Container environment configuration:**
   ```dockerfile
   ENV ENVIRONMENT=prod
   ENV PULUMI_ORG=scoobyjava-org
   # Secrets via Docker Secrets, NOT environment variables
   ```

3. **K3s deployment patterns:**
   ```bash
   # All deployments via K3s manifests
   kubectl apply -f k8s/
   # Secrets via Pulumi ESC integration
   # No manual kubectl from local machine
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   export KUBECONFIG="$HOME/.kube/k3s-lambda-labs"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **☸️ K3S DEPLOYMENT RULES**

1. **ALL Kubernetes deployments target Lambda Labs K3s cluster:**
   ```yaml
   # Primary cluster: k3s.lambda-labs.sophia-ai.com
   # Control plane: 192.222.58.232:6443
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: K3s (lightweight Kubernetes)
   ```

2. **K3s namespace organization:**
   ```yaml
   # Production namespace
   namespace: sophia-ai-prod
   # MCP servers namespace  
   namespace: mcp-servers
   # Monitoring namespace
   namespace: monitoring
   ```

3. **Deployment patterns:**
   ```bash
   # GitHub Actions deploys via:
   kubectl apply -k k8s/overlays/production
   # Kustomize for environment management
   # Helm charts for complex deployments
   ```

4. **Secret management:**
   ```yaml
   # All secrets via Pulumi ESC
   # Automatic sync to K3s secrets
   # No manual secret creation
   ```

### **🔧 MCP SERVER ENVIRONMENT RULES**

1. **ALL MCP servers MUST validate environment on startup**
2. **ALL MCP servers MUST use centralized environment detection**
3. **NO hardcoded environment values in MCP server code**
4. **Environment health checks MUST be included**

### **📝 CODING STANDARDS WITH ENVIRONMENT AWARENESS**

1. **Environment Detection Pattern:**
   ```python
   # CORRECT - Use centralized environment detection
   from backend.core.auto_esc_config import get_config_value

   # WRONG - Direct environment variable access
   os.getenv("SOME_SECRET")  # ❌ Use centralized config instead
   ```

2. **Error Handling with Environment Context:**
   ```python
   try:
       config_value = get_config_value("some_key")
   except Exception as e:
       logger.error(f"Config error in {os.getenv('ENVIRONMENT', 'unknown')} environment: {e}")
       # Always provide fallback
   ```

3. **Health Check Integration:**
   ```python
   # ALL services MUST include environment health validation
   def validate_environment():
       env = os.getenv("ENVIRONMENT")
       if env != "prod":
           logger.warning(f"Not in production environment: {env}")
       return env in ["prod", "staging", "dev"]
   ```

## 🧠 **UNIFIED MEMORY ARCHITECTURE RULES (CRITICAL - JULY 10, 2025)**

### **🚀 FLEXIBLE MEMORY ARCHITECTURE RULES 🚀**

**THE DATE IS JULY 10, 2025 - REMEMBER THIS!**

1. **SUPPORTED VECTOR DATABASES:**
   - ✅ **Qdrant** - Primary vector store for AI-native search
   - ✅ **PostgreSQL pgvector** - Hybrid SQL + vector queries
   - ✅ **Redis** - Sub-millisecond caching layer
   - ✅ **qdrant Cortex** - Legacy support during migration
   - ✅ **Mem0** - Agent conversational memory

2. **RECOMMENDED STACK:**
   - ✅ **UnifiedMemoryService** for ALL memory operations
   - ✅ **Lambda Labs GPU** for embeddings (<50ms latency)
   - ✅ **Qdrant** for primary vector storage
   - ✅ **Redis** for hot data caching
   - ✅ **PostgreSQL pgvector** for hybrid queries
   - ✅ **Mem0** for conversational context

3. **THE 6-TIER MEMORY ARCHITECTURE:**
   ```
   L0: GPU Cache (Lambda Labs) - Hardware acceleration
   L1: Redis (Hot cache) - <10ms session data
   L2: Qdrant (Vectors) - <50ms semantic search
   L3: PostgreSQL pgvector - <100ms hybrid queries
   L4: Mem0 (Conversations) - Agent memory
   L5: Legacy Storage (Deprecated) - Migration complete
   ```

4. **IMPORT PATTERNS:**
   ```python
   # ✅ CORRECT - The flexible way to use memory
   from backend.services.unified_memory_service import get_unified_memory_service
   memory = get_unified_memory_service()
   
   # ✅ ALSO CORRECT - Direct usage when needed
   from qdrant_client import QdrantClient
   from pgvector.asyncpg import register_vector
   import redis
   ```

5. **MEMORY OPERATIONS:**
   ```python
   # ✅ CORRECT - Unified service (recommended)
   results = memory.search_knowledge(
       query="What is the revenue forecast?",
       limit=10,
       metadata_filter={"department": "sales"}
   )
   
   # ✅ ALSO CORRECT - Direct Qdrant for performance
   qdrant_client.search(
       collection_name="knowledge",
       query_vector=embeddings,
       limit=10
   )
   ```

6. **PERFORMANCE TARGETS:**
   ```python
   # Embedding generation: <50ms (Lambda GPU)
   # Vector search: <100ms (Qdrant)
   # Cache hit: <10ms (Redis)
   # Hybrid query: <150ms (PostgreSQL)
   ```

### **🛡️ MIGRATION SUPPORT**

1. **Dual-Mode Operation:**
   - ELIMINATED: Weaviate has been completely removed
   - Services now use Qdrant as primary vector database
   - Migration to Qdrant complete

2. **Migration Helpers:**
   ```python
   # ELIMINATED: Weaviate migration complete
   # All services now use Qdrant directly
   return await memory.search_qdrant(query)
   ```

### **📋 BEST PRACTICES**

1. **Choose the Right Store:**
   - Hot data → Redis
   - Semantic search → Qdrant (Weaviate eliminated)
   - Hybrid queries → PostgreSQL pgvector
   - Conversations → Mem0
   - Analytics → Qdrant (primary)

2. **Optimize for Performance:**
   - Batch embeddings on GPU
   - Use connection pooling
   - Cache frequently accessed data
   - Parallelize searches when possible

### **🚀 PERFORMANCE WINS**

With Qdrant architecture:
- 10x faster embeddings (500ms → 50ms)
- 5x faster search (500ms → 100ms)  
- 70% cost reduction ($3.5k → $1k/month)
- No vendor lock-in
- Full control over models

Remember: **Lambda Labs + Qdrant = Enterprise Performance** for Sophia AI!

## 📦 **UV DEPENDENCY GOVERNANCE (MANDATORY)**

### **🎯 CORE PRINCIPLES**

1. **Single Lock File of Truth**: `uv.lock` is canonical
2. **Group-Based Isolation**: Dependencies in exactly ONE group
3. **SemVer Pinning**: All direct deps use `==` versions
4. **Zero Unvetted Wheels**: Only PyPI or approved sources
5. **No Direct Transitive Fixes**: Use `tool.uv.transitive-overrides`

### **🔧 UV WORKFLOW**

```bash
# CORRECT - Add dependency to group
uv add package==1.0.0 --group core

# CORRECT - Sync environment
uv sync --strict --require-hashes

# CORRECT - Run with UV
uv run pytest

# WRONG - Never use pip directly
pip install package  # ❌ FORBIDDEN
```

### **📋 DEPENDENCY GROUPS**

```toml
[tool.uv.dependency-groups]
core = ["fastapi==0.111.0", "redis==5.0.4", "qdrant-client==3.10.0"]
mcp-servers = ["anthropic-mcp-python-sdk==1.2.4"]
ai-enhanced = ["openai==1.30.0", "anthropic==0.25.6", "langchain==0.2.0"]
automation = ["n8n-python-client==0.2.0", "temporal-sdk==1.5.0"]
dev = ["pytest==8.2.2", "ruff==0.4.4", "mypy==1.10.0", "black==24.4.0"]
```

### **🛡️ CONTINUOUS HYGIENE**

1. **Pre-commit Hooks**: `uv sync --check`
2. **CI/CD Security**: `uv audit` on every PR
3. **Renovate Bot**: Weekly dependency updates
4. **Nightly Drift Hunter**: Automated vulnerability scanning

### **📊 SUCCESS METRICS**

- Mean `uv sync` duration: < 35s
- High/Critical vulns open > 7 days: 0
- Duplicate direct deps: 0
- Build reproducibility: ≥ 99.9%
- License compliance: 100%

## 🚀 **HIGH-PERFORMANCE LLM STRATEGY (MANDATORY)**

### **🏗️ ARCHITECTURE**

```
Request → Portkey Gateway → Policy Engine → OpenRouter → Best Model
             ↓                    ↓              ↓
         Trace/Metrics      Scoring Logic   200+ Models
```

### **📊 MODEL SCORING POLICY**

| Criterion | Weight | Rule |
|-----------|--------|------|
| Freshness | 40% | Release < 90d = 40pts, else 20pts |
| Latency | 25% | p95 < 800ms = 25pts, else 15pts |
| Quality | 25% | Benchmark percentile → linear score |
| Cost | 10% | Only penalize if > $0.01/1k tokens |

### **🎯 ROUTING PATTERNS**

```python
# PERFORMANCE FIRST
response = await portkey.invoke_llm(
    prompt="Quick task",
    preferences={"prefer_fast": True}  # Boosts latency weight
)

# QUALITY CRITICAL
response = await portkey.invoke_llm(
    prompt="Complex analysis",
    preferences={"prefer_quality": True}  # Boosts quality weight
)

# DEFAULT - BALANCED
response = await portkey.invoke_llm(prompt="Standard request")
```

### **📈 PERFORMANCE TARGETS**

- **First Token**: < 150ms with streaming
- **Total Latency**: p95 < 2s
- **Availability**: > 99.5% with failover
- **Model Currency**: New models < 72h
- **Route Score**: > 80/100

### **🚫 FORBIDDEN PATTERNS**

```python
# ❌ NEVER direct SDK usage
import openai
client = openai.Client()  # FORBIDDEN

# ❌ NEVER bypass Portkey
response = requests.post("https://api.openai.com/...")  # FORBIDDEN

# ✅ ALWAYS use Portkey
from backend.services.portkey_gateway import invoke_llm
response = await invoke_llm(prompt)
```

## 🔄 **N8N WORKFLOW AUTOMATION**

### **📋 WORKFLOW PATTERNS**

```yaml
# Workflow Structure
name: Business Process
triggers:
  - schedule: "0 9 * * *"
  - webhook: "/webhook/process"
  - event: "gong.call_completed"
nodes:
  - data: ["qdrant_query", "api_fetch"]
  - ai: ["llm_analysis", "embedding_generation"]
  - action: ["slack_notify", "linear_create_task"]
  - condition: ["if_threshold", "switch_sentiment"]
```

### **🎯 PRE-BUILT WORKFLOWS**

1. **Daily Business Intelligence**
   - Query qdrant metrics
   - AI analysis and insights
   - Executive summary to Slack

2. **Customer Health Monitoring**
   - Gong sentiment tracking
   - HubSpot deal analysis
   - Automated alerts

3. **Code Quality Gates**
   - GitHub PR triggers
   - Codacy security scans
   - AI code review

### **🚀 NATURAL LANGUAGE COMMANDS**

```bash
"Create a workflow to monitor revenue anomalies"
"Set up daily customer health reports"
"Automate PR review process with AI"
"Build workflow for lead scoring"
```

## 🧠 **ENHANCED MCP INTEGRATION WITH CLINE v3.18**

### **📍 HOW TO USE CLINE v3.18 FEATURES (You're Already Here!)**

**IMPORTANT**: The chat window where you interact with Cline IS the interface for all v3.18 features. You don't need to look for another icon or panel - everything works right here in this chat!

### **🚀 Natural Language Commands in THIS Chat**

Just type these commands naturally in the Cline chat (where you're typing now):

#### **AI Memory Commands**
- **"Remember this [topic/decision/code]"** - Stores in AI memory
- **"What did we decide about [topic]?"** - Recalls past decisions
- **"Show similar [patterns/code]"** - Finds related implementations

#### **Large File Processing (FREE with Gemini!)**
- **"Process this large file with Gemini"** - Uses free Gemini for big files
- **"Analyze this 500K token document"** - Auto-routes to Gemini 2.5 Pro
- **"Summarize our 90-day Slack history"** - Handles massive datasets

#### **Web Content Fetching**
- **"Fetch docs from [URL]"** - Retrieves and converts to markdown
- **"Get latest [topic] from web"** - Searches and summarizes
- **"Download competitor info from [website]"** - Competitive intelligence

#### **Business Tool Integration**
- **Linear**: "Create Linear issue for [task]", "Show my Linear tasks"
- **qdrant**: "Query qdrant for [data]", "Run large query with Gemini"
- **Slack**: "Analyze #[channel]", "Find messages about [topic]"
- **Gong**: "Summarize recent calls", "Find calls discussing [keyword]"

### **🔄 AUTOMATIC WORKFLOW INTEGRATION**

#### **AI Memory Auto-Discovery**
1. **INTELLIGENT AUTO-STORAGE**: Cline automatically detects and stores:
   - Architecture discussions with decision rationale
   - Bug fixes with root cause analysis
   - Code patterns and implementation strategies
   - Performance optimization insights
   - Security implementation decisions
   - Refactoring approaches and outcomes

2. **CONTEXT-AWARE RECALL**: Before any coding task, Cline automatically:
   - Queries relevant past decisions
   - Surfaces similar patterns from project history
   - Provides continuity with previous architectural choices
   - Suggests proven solutions from past implementations

3. **SIMPLE NATURAL LANGUAGE USE**:
   - Just say: "Remember this architectural decision"
   - Just ask: "What did we decide about the database?"
   - Just request: "Show me similar bug fixes"

#### **Real-time Code Analysis (@codacy)**
1. **AUTOMATIC CODE QUALITY**: On every significant code change:
   - Real-time security vulnerability scanning
   - Code complexity analysis with refactoring suggestions
   - Style compliance checking (Black, PEP 8)
   - Performance pattern detection

2. **INTELLIGENT SUGGESTIONS**: Proactive recommendations:
   - Security best practices for detected patterns
   - Refactoring opportunities for complex functions
   - Code quality improvements with examples
   - Architecture alignment with project standards

3. **ENHANCED CODACY TOOLS**:
   - `codacy.analyze_code`: Real-time code snippet analysis
   - `codacy.analyze_file`: Complete file quality assessment
   - `codacy.get_fix_suggestions`: Automated improvement recommendations
   - `codacy.security_scan`: Focused security vulnerability detection

### **🚀 WORKFLOW AUTOMATION TRIGGERS**

#### **Automatic Triggers (No User Input Required)**:
- **On File Save**: `@codacy.analyze_file` + `@ai_memory.auto_store_context`
- **On Architecture Discussion**: `@ai_memory.store_conversation(category="architecture")`
- **On Bug Fix**: `@ai_memory.store_conversation(category="bug_solution")` + `@codacy.security_scan`
- **On Code Review**: `@codacy.analyze_code` + `@ai_memory.recall_memory("similar patterns")`

#### **Smart Context Awareness**:
- **File-Specific Memory**: Automatically recall memories related to current file
- **Project Pattern Recognition**: Surface relevant architectural decisions
- **Security Context**: Auto-scan for security issues in sensitive code areas
- **Performance Awareness**: Detect performance-critical code sections

### **🎯 ENHANCED NATURAL LANGUAGE COMMANDS**

#### **Memory Operations**:
- "Remember this architectural decision" → Auto-categorize and store with context
- "What did we decide about database schema?" → Smart recall with file context
- "Show me similar bug fixes" → Context-aware pattern matching
- "Store this conversation about MCP integration" → Enhanced storage with metadata

#### **Code Quality Operations**:
- "Analyze this code for security issues" → Comprehensive security scan
- "Check code quality" → Multi-dimensional analysis with suggestions
- "Fix this function complexity" → Automated refactoring recommendations
- "Scan for vulnerabilities" → Deep security pattern analysis

#### **Integrated Workflows**:
- "Review and remember this implementation" → Codacy analysis + Memory storage
- "Find similar patterns and analyze quality" → Memory recall + Code analysis
- "Store this bug fix and scan for similar issues" → Memory storage + Security scan

### **📊 INTELLIGENT REPORTING**

#### **Development Insights**:
- Automatic pattern recognition across stored memories
- Code quality trends over time
- Security vulnerability patterns
- Architecture evolution tracking

#### **Proactive Recommendations**:
- Suggest architectural improvements based on stored decisions
- Recommend security enhancements from vulnerability patterns
- Propose refactoring based on complexity analysis
- Guide development based on successful past patterns

### **🔧 CURSOR IDE INTEGRATION SPECIFICS**

#### **Configuration Requirements**:
- MCP servers running on specified ports (ai_memory: 9000, codacy: 3008)
- Auto-trigger workflows enabled in cursor_mcp_config.json
- Context awareness enabled for file-specific operations
- Intelligent routing for multi-tool operations

#### **Performance Optimization**:
- Parallel tool execution for independent operations
- Smart caching of frequently accessed memories
- Efficient code analysis with incremental scanning
- Context-aware tool selection based on current activity

### **Example Enhanced Workflow**:
```
User: "I need to implement user authentication for the MCP server"

Cursor AI (Automatic Sequence):
1. [AUTO] @ai_memory.smart_recall("authentication MCP server implementation")
2. [CONTEXT] Retrieve: Previous auth patterns, security decisions, MCP integration approaches
3. [ANALYSIS] @codacy.analyze_code(current_auth_code) for security assessment
4. [EXTERNAL] Check microsoft_playwright and anthropic-mcp-servers for auth patterns
5. [RESPONSE] Provide implementation guidance based on stored patterns + security analysis + community validation
6. [AUTO] @ai_memory.auto_store_context(conversation + implementation decisions)
```

This enhanced integration transforms Cursor AI into an intelligent development partner that learns from every interaction and provides contextually aware assistance.

## Development Standards

### Python Code Style
- Use Python 3.11+ with type hints for all functions
- Follow PEP 8 with 88-character line limit (Black formatter)
- Use async/await for I/O operations
- Implement comprehensive error handling with logging
- Include detailed docstrings for all classes and methods

### Agent Development Pattern

**During Transition - USE THIS**:
```python
from backend.agents.core.base_agent import BaseAgent

class YourAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        # Agent-specific initialization

    async def execute_task(self, task: Task) -> TaskResult:
        # Implementation with error handling
        pass
```

**After Migration (DO NOT USE YET)**:
```python
from libs.core.agents.base_agent import BaseAgent

class YourAgent(BaseAgent):
    # Same implementation, different import
```

### Integration Pattern
```python
class ServiceIntegration:
    def __init__(self, config: ServiceConfig):
        self.config = config
        self.client = self._create_client()

    async def _make_request(self, method: str, endpoint: str, **kwargs):
        # Standardized request handling with rate limiting
        pass
```

### Business Intelligence Focus
- Always consider Pay Ready business context
- Implement metrics for revenue, customer health, sales performance
- Focus on actionable insights for sales coaching and client monitoring
- Prioritize real-time data processing and notifications

### Security Requirements
- Use encrypted storage for all API keys
- Implement proper authentication and authorization
- Log all security-relevant events
- Follow principle of least privilege

### **Secret Management (PERMANENT SOLUTION)**
- **Documentation:** Always refer to `PERMANENT_GITHUB_ORG_SECRETS_SOLUTION.md`
- **GitHub Organization:** All secrets managed at [https://github.com/ai-cherry](https://github.com/ai-cherry)
- **Pulumi ESC:** Automatic secret synchronization via `scoobyjava-org/default/sophia-ai-production`
- **Backend Integration:** Use `backend/core/auto_esc_config.py` for automatic secret loading
- **Never hardcode secrets:** Always use automatic ESC integration
- **GitHub Actions:** Secrets automatically available from organization level
- **Local Development:** Set `export PULUMI_ORG=scoobyjava-org` and secrets load automatically
- **Secret Rotation:** Update in GitHub organization → automatic sync → automatic deployment

### Testing Strategy
- Write unit tests for all business logic
- Include integration tests for external APIs
- Implement performance tests for critical paths
- Use pytest with async support

### Error Handling Pattern
```python
try:
    result = await some_operation()
    return result
except SpecificException as e:
    logger.error(f"Operation failed: {e}")
    raise BusinessLogicError(f"Failed to process: {e}")
except Exception as e:
    logger.exception("Unexpected error")
    raise SystemError("Internal system error")
```

## Business Domain Knowledge

### Pay Ready Context
- **CRITICAL SPELLING RULE: Always spell as "Pay Ready" (with space) - NEVER "PayReady"**
- Company focus: Business intelligence and automation
- Key metrics: Revenue growth, customer satisfaction, sales efficiency
- Team communication: Primarily through Slack
- CRM system: HubSpot for contact and deal management
- Call analysis: Gong.io for sales call insights

### Agent Specializations
- **Call Analysis Agent:** Process Gong.io recordings for insights
- **CRM Sync Agent:** Maintain HubSpot data quality and synchronization
- **Notification Agent:** Send intelligent Slack updates
- **Business Intelligence Agent:** Generate revenue and performance reports

### Integration Priorities
1. **HubSpot:** Primary CRM for contact/deal management
2. **Gong.io:** Critical for call analysis and sales coaching
3. **Slack:** Main communication channel for team updates
4. **Vector Databases:** For semantic search and AI capabilities

## 🚨 MONOREPO TRANSITION IN PROGRESS

**CRITICAL**: We are transitioning to a monorepo structure. During this transition:
- **Continue using the OLD structure** for new code (`backend/`, `frontend/`, etc.)
- **DO NOT use the new structure** (`apps/`, `libs/`) until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for current status

### Current File Organization (USE THIS)
```
backend/
├── agents/
│   ├── core/           # Base agent classes
│   └── specialized/    # Domain-specific agents
├── integrations/       # External service integrations
├── database/          # Data layer and migrations
├── monitoring/        # Performance and health monitoring
└── security/          # Authentication and encryption

frontend/
├── src/
│   ├── components/    # React components
│   ├── pages/         # Page components
│   └── services/      # API clients

external/              # 🆕 Strategic MCP repository collection
├── microsoft_playwright/    # Browser automation (13.4k stars)
├── glips_figma_context/    # Design-to-code (8.7k stars)
├── qdrant_memory_official/ # Official qdrant AI
└── [8 additional strategic repos]
```

### Future File Organization (DO NOT USE YET)
```
apps/                  # Monorepo applications
├── api/              # Backend API (from backend/api)
├── frontend/         # React frontend
├── mcp-servers/      # All MCP servers
└── n8n-bridge/       # N8N integration

libs/                  # Shared libraries
├── ui/               # Shared UI components
├── utils/            # Shared utilities
├── types/            # Shared TypeScript types
└── core/             # Core business logic

config/               # Centralized configurations
├── eslint/
├── prettier/
├── typescript/
└── ruff/
```

## Common Patterns

### API Client Implementation
- Use aiohttp for async HTTP requests
- Implement exponential backoff for retries
- Respect rate limits with proper throttling
- Include comprehensive error handling

### Database Operations
- Use SQLAlchemy with async support
- Implement proper connection pooling
- Use transactions for data consistency
- Include migration scripts for schema changes

### Monitoring and Logging
- Use structured logging with JSON format
- Include correlation IDs for request tracing
- Monitor performance metrics and business KPIs
- Implement health checks for all services

## AI and ML Guidelines
- Use OpenAI API for language processing
- Implement vector search with Pinecone/Weaviate
- Cache embeddings for performance
- Include confidence scores in AI responses

## Deployment Considerations
- **Target**: Lambda Labs K3s cluster (192.222.58.232)
- **Container Registry**: Push all images to scoobyjava15 Docker Hub
- **Secrets**: All secrets via Pulumi ESC (NO .env files)
- **Deployment**: `kubectl apply -k k8s/overlays/production`
- **Scaling**: Horizontal Pod Autoscaler with GPU awareness
- **Zero-downtime**: Rolling updates with readiness probes

## Performance Requirements
- API response times < 200ms for critical paths
- Database queries < 100ms average
- Vector searches < 50ms average
- Support for 1000+ concurrent users

## When suggesting code:
1. **FIRST**: Check AI memory for similar implementations
2. **EXTERNAL REPOS**: Leverage patterns from 11 strategic external repositories (microsoft_playwright, glips_figma_context, qdrant_memory_official, etc.)
3. Always include proper error handling
4. Add type hints and docstrings
5. Consider business context and Pay Ready needs
6. Implement monitoring and logging
7. Follow the established patterns in the codebase
8. Prioritize performance and scalability
9. Include relevant tests
10. **VALIDATE ENVIRONMENT**: Ensure ENVIRONMENT="prod" is used
11. **COMMUNITY PATTERNS**: Apply proven patterns from 22k+ star external repositories
12. **LAST**: Store the conversation in AI memory

## Avoid:
- Hardcoded values (use configuration)
- Synchronous I/O in async contexts
- Missing error handling
- Unclear variable names
- Complex nested logic without comments
- Security vulnerabilities (exposed secrets, etc.)
- **DEFAULTING TO STAGING ENVIRONMENT** (Always use production)
- **MANUAL ENVIRONMENT VARIABLE SETUP** (Use centralized config)
- **IGNORING EXTERNAL REPOSITORY PATTERNS** (Always check for proven community approaches)

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. Leverage the collective intelligence of 22k+ star external repositories to ensure world-class implementation patterns.

### Infrastructure as Code Integration
- **Pulumi Commands**: Use `pulumi up`, `pulumi preview`, `pulumi destroy` for infrastructure management
- **ESC Operations**: Use scripts in `infrastructure/esc/` for secret management
- **GitHub Integration**: All deployments go through GitHub Actions workflows
- **MCP Integration**: Use `mcp_config.json` for MCP server configuration

### 🚀 Cline v3.18 Enhanced Features Integration

#### Claude 4 & Gemini 2.5 Pro Optimization
- **Model Selection**: Automatic routing based on task complexity and context size
- **Large Context**: Use Gemini for documents > 100K tokens (up to 1M)
- **Complex Reasoning**: Claude 4 for architectural design and code generation
- **Data Processing**: qdrant Cortex for SQL and data operations
- **Cost Optimization**: Automatic routing to free Gemini CLI for large contexts

#### Gemini CLI Integration (NEW)
- **Free Access**: Use local Gemini CLI for zero-cost processing
- **Auto-routing**: "Process this large file with Gemini" → Routes to CLI
- **Batch Processing**: Efficient handling of multiple large documents
- **Context Preservation**: Maintain context across CLI calls

#### WebFetch Tool Usage (ENHANCED)
- **Documentation Retrieval**: "Fetch the latest API docs from [url]"
- **Competitive Intelligence**: "Get competitor information from [website]"
- **Real-time Updates**: "Retrieve and summarize current [topic] from [source]"
- **Caching**: Automatic caching with TTL for improved performance
- **Format Support**: PDF, DOCX, HTML, and plain text extraction
- **Parallel Fetching**: Process multiple URLs simultaneously

#### Self-Knowledge Commands (ENHANCED)
- **Capabilities Discovery**: "What can the [server name] MCP server do?"
- **Feature Inspection**: "Show available features for [component]"
- **Help System**: "How do I use [feature]?"
- **Server Status**: "Check capabilities of all MCP servers"
- **Performance Metrics**: "Show performance stats for [server]"
- **Usage Analytics**: "How often do we use [feature]?"

#### Improved Diff Editing (AI-POWERED)
- **Auto-fallback**: Automatically tries exact → fuzzy → context-aware → AI strategies
- **Success Rate**: 95%+ success rate for file modifications
- **Smart Updates**: "Update [file] using all available strategies"
- **Context Awareness**: AI-powered understanding of code changes
- **Multi-file Operations**: Apply changes across multiple files
- **Rollback Support**: Undo changes if needed

#### Enhanced AI Memory Integration (v3.18)
- **Auto-discovery**: Automatically detect and store architecture decisions
- **Smart Recall**: "What did we decide about [topic]?" → Context-aware retrieval
- **Pattern Matching**: Find similar implementations across the codebase
- **WebFetch Integration**: Automatically store fetched documentation

#### Enhanced Codacy Integration (v3.18)
- **Real-time Analysis**: Analyze code as you type
- **Security Scanning**: Deep vulnerability detection
- **Performance Insights**: Identify performance bottlenecks
- **AI Suggestions**: Get AI-powered improvement recommendations

### Natural Language Infrastructure Commands
When using Cursor AI for infrastructure operations, you can use natural language:

#### Examples:
- "Deploy the infrastructure" → Triggers GitHub Actions workflow
- "Get the database password" → Retrieves secret from Pulumi ESC
- "Rotate API keys" → Runs secret rotation framework
- "Sync secrets" → Synchronizes GitHub and Pulumi ESC secrets
- "Test the deployment" → Runs ESC integration tests

#### Command Patterns:
- **Secret Operations**: "get/retrieve/fetch [service] [secret_type]"
- **Deployment Operations**: "deploy/update/rollback [component]"
- **Testing Operations**: "test/validate/check [component]"
- **Configuration Operations**: "configure/setup/initialize [service]"

### MCP Server Natural Language Integration (v3.18 Enhanced)
- **Query Data**: "Get recent Gong calls" → Uses Gong MCP server with model routing
- **Deploy Apps**: "Deploy to Lambda Labs" → Uses Lambda Labs deployment with improved diff
- **Manage Data**: "Upload to Estuary" → Uses Estuary MCP server with WebFetch
- **Database Operations**: "Query qdrant" → Uses qdrant MCP server with Cortex
- **Store Memory**: "Remember this conversation" → Uses AI Memory MCP server with Claude 4
- **Recall Context**: "What did we decide about X?" → Uses AI Memory with self-knowledge
- **Fetch External Data**: "Get latest docs from [url]" → Uses WebFetch tool
- **Analyze Large Docs**: "Process this 500K token file" → Auto-routes to Gemini 2.5 Pro

### Error Handling and Debugging
- **ESC Errors**: Check Pulumi ESC logs and validate configuration
- **GitHub Actions Errors**: Review workflow logs and artifacts
- **MCP Errors**: Check Docker container logs and health endpoints
- **Secret Errors**: Validate secret names and permissions

### Best Practices for Cursor AI Integration
1. **Use Descriptive Comments**: Add context for infrastructure operations
2. **Follow Naming Conventions**: Use consistent naming for secrets and services
3. **Document Dependencies**: Clearly document service dependencies
4. **Test Before Deploy**: Always test changes in isolation first
5. **Monitor Operations**: Use logging and monitoring for all operations
6. **Remember Context**: Always use AI Memory for persistent development context
7. **🆕 Leverage External Patterns**: Always check external repositories for proven implementation approaches
8. **🆕 Community Validation**: Prefer patterns from high-star repositories with community validation
9. **🆕 Cross-Repository Intelligence**: Synthesize insights from multiple repository approaches

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. The strategic external repository collection provides access to 22k+ stars of community-validated patterns and proven implementation approaches.

## 🎯 **The Unified Dashboard is the ONLY Frontend**

**CRITICAL RULE:** All new frontend development MUST extend the one, true `UnifiedDashboard.tsx` component.

1.  **NO NEW DASHBOARDS:** Do not create new, separate dashboard components or pages. All new views, tabs, or features must be integrated into the existing `UnifiedDashboard.tsx` tabbed interface.
2.  **EXTEND, DON'T REPLACE:** Use the existing components (`