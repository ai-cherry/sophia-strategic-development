## ‚ö†Ô∏è **MONOREPO TRANSITION IN PROGRESS**

**CRITICAL FOR AI CODERS**:
- We are transitioning from the old structure (`backend/`, `frontend/`) to a new monorepo structure (`apps/`, `libs/`)
- **Continue using the OLD structure** for all new code until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for details
- Target completion: February 2025

## üöÄ **THE GOLDEN RULE OF DEPLOYMENT (MANDATORY)**

**ALL DEPLOYMENTS ARE AUTOMATED VIA GITHUB ACTIONS. NEVER DEPLOY FROM A LOCAL MACHINE.**

### **Core Principles (Non-Negotiable):**
1.  **Single Point of Truth**: The `main` branch on GitHub is the single source of truth for what is deployed.
2.  **Push-to-Deploy**: All deployments to any environment (Production, Staging) are triggered **exclusively** by a `git push` to the relevant branch.
3.  **No Local Operations**: You MUST NOT run `docker build`, `docker push`, or `kubectl apply` from your local machine for deployment purposes. Your role is to write the code and the GitHub Actions workflows; the cloud does the rest.
4.  **Automated Secret Management**: All secrets (`DOCKER_HUB_ACCESS_TOKEN`, `LAMBDA_LABS_SSH_KEY`, etc.) are managed in GitHub Organization Secrets and injected into the GitHub Actions workflow at runtime. You MUST NOT handle credentials locally.

### **The Correct Workflow:**
1.  **Write Code**: Implement the feature or fix.
2.  **Write/Update Workflow**: Ensure the `.github/workflows/` configuration is correct for building and deploying the service.
3.  **Push to GitHub**: `git push origin main`.
4.  **Monitor Action**: Observe the deployment progress in the GitHub Actions tab.

This is the **only** approved method. Any deviation from this (e.g., suggesting `docker login` locally) is a violation of core operating principles.

## üåü **NEW: The System Handbook is the Ultimate Source of Truth**

**CRITICAL DIRECTIVE:** The `docs/system_handbook/` directory, and specifically `00_SOPHIA_AI_SYSTEM_HANDBOOK.md`, is the **single, definitive source of truth** for the Sophia AI platform's architecture, standards, and roadmap.

### **Your Core Responsibilities Regarding Documentation:**

1.  **CONSULT FIRST, CODE LATER:** Before answering any architectural question or implementing any new feature, you **MUST** first consult the System Handbook to understand the established patterns and principles.
2.  **PROACTIVELY UPDATE:** If you make a significant architectural change (e.g., add a new service, change a data flow, modify the MCP gateway), you **MUST** update the relevant section of the System Handbook within the same session.
3.  **MAINTAIN STRUCTURE:** When adding documentation, you **MUST** follow the existing structure and format. If a new deep-dive document is needed, create it within the `system_handbook` directory and link to it from the master file.
4.  **CITE YOUR SOURCE:** When you use information from the handbook to make a decision, you should briefly mention it (e.g., "Based on the Phoenix Plan architecture outlined in the System Handbook, I will...").

This ensures our documentation remains a living, breathing, and accurate representation of the platform, empowering both human and AI developers.

# Sophia AI Pay Ready Platform - Cursor AI Rules

## Project Overview
You are working on Sophia AI, an AI assistant orchestrator for Pay Ready company. Sophia serves as the central "Pay Ready Brain" that orchestrates multiple AI agents and integrates with business systems.

### Company Context
- **Company Size:** 80 employees total
- **Initial Users:** 1 (CEO - primary developer and user)
- **Rollout Plan:** CEO only ‚Üí Few super users (2-3 months) ‚Üí Full company (6+ months)
- **Development Team:** CEO (sole human developer) + AI assistants

## üéØ **Development Priorities (CRITICAL)**

### **Priority Order - NEVER COMPROMISE:**
1. **QUALITY & CORRECTNESS** - Every line of code must be correct and well-structured
2. **STABILITY & RELIABILITY** - System must be rock-solid for CEO usage
3. **MAINTAINABILITY** - Code must be clear and easy to modify
4. **PERFORMANCE** - Important but secondary to quality
5. **COST & SECURITY** - Consider but don't over-optimize at this stage

### **Quality Standards:**
- **Zero Duplication:** Never duplicate code or functionality
- **Clear Dependencies:** All dependencies must be explicit and documented
- **Conflict Prevention:** Check for conflicts before implementing
- **Structure First:** Plan structure to avoid future issues
- **Review Everything:** Always review context before coding

### **Tool Selection Principle:**
> **Only add new tools when there's a clear gap that existing tools cannot fill.**

This principle prevents:
- Tool proliferation and complexity creep
- Duplicate functionality across different tools
- Maintenance burden from unnecessary dependencies
- Migration overhead from overlapping solutions

Before adding any new tool or framework:
1. Check if existing tools can solve the problem
2. Document the specific gap the new tool fills
3. Consider the long-term maintenance cost
4. Prefer enhancing existing tools over adding new ones

## üßπ **ENHANCED CODE HYGIENE RULES (MANDATORY)**

### **Automated Technical Debt Prevention:**
Our "Clean by Design" framework prevents the accumulation of technical debt through:

1. **Automated Daily Cleanup**: Removes expired one-time scripts, backup files, and empty archive directories
2. **Pre-Commit Blocking**: Prevents commits that would introduce technical debt patterns
3. **Documentation Lifecycle**: Automatic archiving of temporary documentation
4. **Zero Tolerance Enforcement**: Immediate rejection of forbidden patterns

### **One-Time Script Management (Enhanced):**
```python
# REQUIRED: All one-time scripts must use this pattern
"""
üö® ONE-TIME SCRIPT - DELETE AFTER USE
Purpose: [specific purpose]
Created: [date]
DELETE AFTER: [date]
Usage: python scripts/one_time/script_name_DELETE_YYYY_MM_DD.py

üßπ CLEANUP: This script will be auto-deleted after expiration date
"""

# At the end of any one-time script:
print("‚úÖ Task completed successfully")
print(f"üßπ This script will auto-delete on: {DELETE_DATE}")
print("üìç Located in: scripts/one_time/ for automatic cleanup")
```

### **File Creation Decision Tree:**
```
Creating a file? ‚Üí Ask: "Will this be used more than once?"
‚îú‚îÄ‚îÄ NO (One-time use)
‚îÇ   ‚îú‚îÄ‚îÄ Script ‚Üí scripts/one_time/name_DELETE_YYYY_MM_DD.py
‚îÇ   ‚îú‚îÄ‚îÄ Doc ‚Üí Mark for auto-archive with retention period
‚îÇ   ‚îî‚îÄ‚îÄ Data ‚Üí /tmp/ or delete immediately after use
‚îî‚îÄ‚îÄ YES (Permanent)
    ‚îú‚îÄ‚îÄ Utility ‚Üí scripts/utils/
    ‚îú‚îÄ‚îÄ Monitoring ‚Üí scripts/monitoring/
    ‚îú‚îÄ‚îÄ Reference Doc ‚Üí docs/99-reference/
    ‚îî‚îÄ‚îÄ Core Logic ‚Üí appropriate permanent directory
```

### **Automated Enforcement:**
- **Daily**: `python scripts/utils/daily_cleanup.py` (runs automatically)
- **Pre-commit**: `python scripts/utils/pre_push_debt_check.py` (blocks bad commits)
- **Monitoring**: Technical debt score tracking and alerting

## üìÖ **Timeline and Budget Guidelines (CRITICAL)**

### **For Coding Tasks - NEVER INCLUDE:**
- ‚ùå **NO specific time estimates** (hours, days, weeks, months)
- ‚ùå **NO budget estimates** for development work
- ‚ùå **NO duration predictions** for implementation
- ‚ùå **NO cost calculations** for coding effort
- ‚ùå **NO timeline-based milestones**

### **For Coding Tasks - ALWAYS USE:**
- ‚úÖ **Phase numbers** (Phase 1, Phase 2.5, etc.)
- ‚úÖ **Functional milestones** (Foundation Complete, Integration Ready)
- ‚úÖ **Feature-based progress** (Intent Classification ‚úÖ, Code Modification ‚úÖ)
- ‚úÖ **Technical dependencies** (Requires X before Y)
- ‚úÖ **Complexity indicators** (Simple, Moderate, Complex)

### **Why This Matters:**
- Time estimates for AI-assisted coding are meaningless and wasteful
- Focus should be on technical implementation, not scheduling
- Quality and correctness matter more than speed
- Each coding session is unique and unpredictable

### **Exception - Business Planning Only:**
- Timeline estimates ONLY when explicitly requested for business planning
- Infrastructure cost estimates when evaluating services
- ROI calculations for business decisions
- Keep business metrics completely separate from coding tasks

## Architecture Context
- **Type:** Multi-agent AI orchestrator for CEO-level business intelligence
- **Primary User:** Pay Ready CEO (initial sole user)
- **Core Integrations:** HubSpot CRM, Gong.io call analysis, Slack communication
- **Data Stack:** PostgreSQL, Redis, Pinecone, Weaviate
- **Infrastructure:** Lambda Labs servers, Lambda Labs frontend deployment
- **External Repository Collection:** 11 strategic MCP servers (22k+ combined stars)

## üîê **PERMANENT SECRET MANAGEMENT SOLUTION - MANDATORY REFERENCE**

**CRITICAL**: Before suggesting ANY secret management solution, you MUST:

1. **READ**: `docs/99-reference/PERMANENT_SECRET_MANAGEMENT_SOLUTION.md`
2. **VERIFY**: The secret exists in GitHub Organization Secrets
3. **USE**: `get_config_value()` from `backend/core/auto_esc_config.py`
4. **NEVER**: Create manual secret management

### **Docker Hub Credentials - ALWAYS AVAILABLE**
```python
from backend.core.auto_esc_config import get_docker_hub_config
config = get_docker_hub_config()
# config["username"] = "scoobyjava15"
# config["access_token"] contains the real Docker Hub token
# config["registry"] = "docker.io"
```

### **Secret Priority Order**
1. **Pulumi ESC** (automatic via `get_config_value()`)
2. **Environment Variables** (fallback)
3. **Defaults** (hardcoded fallbacks only)

**NEVER** skip to manual solutions without checking the automated infrastructure first.

### **Common Secret Access Patterns**
```python
# Docker Hub
from backend.core.auto_esc_config import get_docker_hub_config
docker = get_docker_hub_config()

# qdrant
from backend.core.auto_esc_config import get_qdrant_config
qdrant = get_qdrant_config()

# Lambda Labs
from backend.core.auto_esc_config import get_lambda_labs_config
lambda_labs = get_lambda_labs_config()

# Any other secret
from backend.core.auto_esc_config import get_config_value
secret = get_config_value("secret_name")
```

### **GitHub Actions Already Has Access**
All secrets are automatically available in GitHub Actions workflows:
- `${{ secrets.DOCKER_HUB_USERNAME }}`
- `${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}`
- `${{ secrets.LAMBDA_PRIVATE_SSH_KEY }}`
- etc.

**DO NOT** suggest manual authentication when using GitHub Actions!

## ü§ñ **UNIFIED AI AGENT AUTHENTICATION SYSTEM**

### **üöÄ REVOLUTIONARY CAPABILITY: AI Agents Can Make REAL CHANGES**

Sophia AI features a **REVOLUTIONARY** Unified AI Agent Authentication System that enables AI coding agents to make **REAL CHANGES** across the entire technology stack with enterprise-grade security.

### **üèóÔ∏è Three-Tier Security Architecture**

#### **Tier 1: CLI-Based Authentication (Highest Security)**
Services that use CLI-based authentication with secure credential storage:
- **GitHub**: `gh auth login` with secure token storage
- **Pulumi**: `pulumi login` with organization access
- **Docker**: `docker login` for registry operations
- **Lambda Labs**: Direct deployment to Lambda Labs servers

#### **Tier 2: Enhanced API Authentication**
Services with enhanced security patterns:
- **qdrant**: Secure connection strings with role-based access
- **Lambda Labs**: API key management with instance control
- **Estuary Flow**: Service account authentication

#### **Tier 3: Secure API Key Management**
Standard API integrations with secure key management:
- **OpenAI, Anthropic, Slack, Linear, HubSpot**: Automatic ESC integration

### **ü§ñ Agent Permission Matrix**

#### **Infrastructure Agent (CRITICAL Risk)**
Can perform infrastructure-level operations:
```python
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager

auth_manager = UnifiedServiceAuthManager()

# Deploy infrastructure
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="pulumi",
    operation="infrastructure_deployment",
    params={"stack": "production", "config": "updated_resources"}
)

# Manage containers
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="docker",
    operation="container_management",
    params={"action": "deploy", "service": "sophia-ai"}
)
```

#### **Data Agent (HIGH Risk)**
Can perform data operations:
```python
# Execute database operations
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="qdrant",
    operation="query_execution",
    params={"query": "CREATE SCHEMA AI_AGENT_TEST", "warehouse": "SOPHIA_AI_COMPUTE_WH"}
)

# Manage data flows
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="estuary_flow",
    operation="flow_management",
    params={"action": "create", "collection": "ai_agent_data"}
)
```

#### **Integration Agent (MEDIUM Risk)**
Can perform business tool operations:
```python
# Create project tickets
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="linear",
    operation="ticket_creation",
    params={"title": "AI Agent Test", "team": "sophia-dev"}
)

# Send notifications
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="slack",
    operation="message_send",
    params={"channel": "#ai-agents", "message": "Deployment completed"}
)
```

### **üõ°Ô∏è Enterprise Security Features**

#### **Zero Trust Authentication**
- Every operation requires explicit authentication
- No persistent credentials in AI agent memory
- All operations logged with full audit trail

#### **Risk-Based Confirmation Workflows**
```python
# CRITICAL operations require explicit confirmation
confirmation = await auth_manager.get_operation_confirmation(
    operation="infrastructure_deployment",
    risk_level=RiskLevel.CRITICAL,
    details="Deploying new Pulumi stack to production"
)

if confirmation.approved:
    await auth_manager.execute_operation(...)
```

#### **Complete Audit Trail**
- All AI agent operations logged to database
- Risk assessment recorded for each operation
- User confirmation workflows tracked
- Security compliance reporting

### **üöÄ Natural Language Commands for AI Agents**

#### **Infrastructure Operations**
```bash
"Deploy the updated infrastructure to production"
"Scale up the qdrant warehouse for the analytics job"
"Create a new Docker service for the MCP gateway"
"Update the Lambda Labs deployment with latest frontend changes"
```

#### **Data Operations**
```bash
"Create a new schema for the AI agent testing"
"Run the quarterly revenue analysis query"
"Set up a new data flow from HubSpot to qdrant"
"Backup the production database before the migration"
```

#### **Business Tool Integration**
```bash
"Create a Linear ticket for the authentication bug"
"Send a Slack message about the deployment status"
"Update the HubSpot deal with the latest information"
"Schedule a GitHub Action workflow for the nightly build"
```

### **üìã Setup Commands**

#### **Phase 1: CLI Authentication Setup**
```bash
# Run the setup script
python scripts/setup_unified_ai_agent_auth.py

# Verify authentication
python -c "from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager; auth = UnifiedServiceAuthManager(); print('‚úÖ Authentication system ready')"
```

#### **Phase 2: Permission Configuration**
```bash
# Configure agent permissions
python -c "
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager
auth = UnifiedServiceAuthManager()
auth.configure_agent_permissions()
print('‚úÖ Agent permissions configured')
"
```

### **üéØ Business Value**

#### **Revolutionary Capabilities**
- **AI agents can make REAL infrastructure changes** with enterprise security
- **Natural language interface** for complex technical operations
- **Zero credential exposure** through secure CLI authentication
- **Complete audit compliance** for all AI-driven changes

#### **Enterprise Benefits**
- **100% Automated Operations**: AI agents handle routine deployments
- **Zero Manual Credential Management**: All authentication automated
- **Complete Security Compliance**: Enterprise-grade audit trails
- **Real-Time Infrastructure Management**: Immediate response to operational needs

### **üìö Documentation Reference**
- **Complete Guide**: `docs/99-reference/UNIFIED_AI_AGENT_AUTHENTICATION.md`
- **Security Manager**: `backend/security/unified_service_auth_manager.py`
- **Setup Script**: `scripts/setup_unified_ai_agent_auth.py`

**üö® CRITICAL**: This system enables AI agents to make REAL changes across your entire technology stack. All operations are secured, audited, and require appropriate confirmations based on risk levels.

## üöÄ **STRATEGIC EXTERNAL REPOSITORY INTEGRATION**

### **üéØ AI-Enhanced Development with Community Patterns**
Sophia AI leverages 11 strategic external repositories to provide world-class AI coding assistance:

#### **üèóÔ∏è Infrastructure & Automation**
- **microsoft_playwright** (13.4k stars): Browser automation, E2E testing patterns
- **anthropic-mcp-servers**: Official MCP implementations and best practices
- **anthropic-mcp-inspector**: MCP debugging and development tools

#### **üé® Design & Creative**
- **glips_figma_context** (8.7k stars): Design-to-code workflows, component generation
- **V0.dev Integration** (NEW): AI-powered UI generation with natural language commands

#### **‚ùÑÔ∏è Data Intelligence**
- **qdrant_memory_official**: Official qdrant AI integration patterns
- **davidamom_qdrant**: Community qdrant implementation approaches
- **dynamike_qdrant**: Performance-optimized qdrant patterns
- **isaacwasserman_qdrant**: Specialized qdrant operations

#### **üö™ AI Gateway & Optimization**
- **portkey_admin**: AI gateway optimization, cost reduction strategies
- **openrouter_search**: 200+ AI model access and selection patterns

#### **üêç Core Framework**
- **anthropic-mcp-python-sdk**: MCP protocol implementation patterns

### **üß† Natural Language Commands for External Repositories**
```bash
# Repository discovery and pattern analysis
"What browser automation patterns do we have from Microsoft Playwright?"
"Show me qdrant optimization strategies across our repositories"
"Find design-to-code patterns from the Figma integration"

# Implementation with community validation
"Use Playwright patterns to implement comprehensive E2E testing"
"Apply qdrant optimization patterns from our 4 repository collection"
"Generate components using GLips Figma design-to-code workflows"

# Cross-repository intelligence
"Compare authentication patterns across all external repositories"
"Find performance optimization strategies from high-star repos"
"Show security patterns used by official implementations"

# AI-Powered UI Generation (V0.dev Integration via Unified Chat)
# Note: Type these directly in the unified chat - no @ commands needed
"Create a modern dashboard component with glassmorphism styling"
"Build a responsive navigation bar with dropdown menus"
"Generate a data table component with sorting and filtering"
"Design a modal dialog with form validation"
"Create a chart component for revenue visualization"
"Build a user profile card with avatar and social links"
```

### **üîç AI Pattern Recognition & Learning**
The external repository collection enables:
- **üìö Pattern Library**: 22k+ star repositories with proven community validation
- **üß† AI Learning**: Rich implementation patterns for AI to analyze and apply
- **‚úÖ Best Practices**: Automatic adherence to industry standards
- **üîó Cross-Repository Intelligence**: AI synthesizes insights across multiple approaches
- **üöÄ Development Acceleration**: 5-10x faster implementation through proven patterns

## üîí **ENVIRONMENT STABILIZATION RULES (CRITICAL)**

### **üéØ PRODUCTION-FIRST ENVIRONMENT POLICY**
**MANDATORY RULES - NEVER BREAK THESE:**

1. **ALWAYS DEFAULT TO PRODUCTION**
   ```bash
   # CORRECT - Always default to production
   ENVIRONMENT="${ENV:-prod}"

   # WRONG - Never default to staging
   ENVIRONMENT="${ENV:-staging}"  # ‚ùå FORBIDDEN
   ```

2. **ENVIRONMENT VARIABLE HIERARCHY**
   ```python
   # Priority order for environment detection:
   # 1. Explicit ENVIRONMENT variable (highest priority)
   # 2. Git branch detection (main=prod, develop=staging)
   # 3. Pulumi stack context
   # 4. ALWAYS fallback to "prod" (never staging, never fail)
   ```

3. **STACK NAMING STANDARDS**
   ```python
   STACK_MAPPING = {
       "prod": "sophia-ai-production",          # ‚úÖ Production stack
       "staging": "sophia-ai-platform-staging", # ‚úÖ Staging stack
       "dev": "sophia-ai-platform-dev"          # ‚úÖ Development stack
   }
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   export KUBECONFIG="$HOME/.kube/k3s-lambda-labs"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **üê≥ DOCKER CLOUD DEPLOYMENT RULES**

1. **ALL Docker deployments target Lambda Labs (NOT local):**
   ```yaml
   # Primary deployment: K3s cluster on Lambda Labs
   # Target: 192.222.58.232 (Lambda Labs)
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: K3s Kubernetes
   ```

2. **Container environment configuration:**
   ```dockerfile
   ENV ENVIRONMENT=prod
   ENV PULUMI_ORG=scoobyjava-org
   # Secrets via Docker Secrets, NOT environment variables
   ```

3. **K3s deployment patterns:**
   ```bash
   # All deployments via K3s manifests
   kubectl apply -f k8s/
   # Secrets via Pulumi ESC integration
   # No manual kubectl from local machine
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   export KUBECONFIG="$HOME/.kube/k3s-lambda-labs"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **‚ò∏Ô∏è K3S DEPLOYMENT RULES**

1. **ALL Kubernetes deployments target Lambda Labs K3s cluster:**
   ```yaml
   # Primary cluster: k3s.lambda-labs.sophia-ai.com
   # Control plane: 192.222.58.232:6443
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: K3s (lightweight Kubernetes)
   ```

2. **K3s namespace organization:**
   ```yaml
   # Production namespace
   namespace: sophia-ai-prod
   # MCP servers namespace  
   namespace: mcp-servers
   # Monitoring namespace
   namespace: monitoring
   ```

3. **Deployment patterns:**
   ```bash
   # GitHub Actions deploys via:
   kubectl apply -k k8s/overlays/production
   # Kustomize for environment management
   # Helm charts for complex deployments
   ```

4. **Secret management:**
   ```yaml
   # All secrets via Pulumi ESC
   # Automatic sync to K3s secrets
   # No manual secret creation
   ```

### **üîß MCP SERVER ENVIRONMENT RULES**

1. **ALL MCP servers MUST validate environment on startup**
2. **ALL MCP servers MUST use centralized environment detection**
3. **NO hardcoded environment values in MCP server code**
4. **Environment health checks MUST be included**

### **üìù CODING STANDARDS WITH ENVIRONMENT AWARENESS**

1. **Environment Detection Pattern:**
   ```python
   # CORRECT - Use centralized environment detection
   from backend.core.auto_esc_config import get_config_value

   # WRONG - Direct environment variable access
   os.getenv("SOME_SECRET")  # ‚ùå Use centralized config instead
   ```

2. **Error Handling with Environment Context:**
   ```python
   try:
       config_value = get_config_value("some_key")
   except Exception as e:
       logger.error(f"Config error in {os.getenv('ENVIRONMENT', 'unknown')} environment: {e}")
       # Always provide fallback
   ```

3. **Health Check Integration:**
   ```python
   # ALL services MUST include environment health validation
   def validate_environment():
       env = os.getenv("ENVIRONMENT")
       if env != "prod":
           logger.warning(f"Not in production environment: {env}")
       return env in ["prod", "staging", "dev"]
   ```

## üß† **UNIFIED MEMORY ARCHITECTURE RULES (CRITICAL - JULY 10, 2025)**

### **üöÄ FLEXIBLE MEMORY ARCHITECTURE RULES üöÄ**

**THE DATE IS JULY 10, 2025 - REMEMBER THIS!**

1. **SUPPORTED VECTOR DATABASES:**
   - ‚úÖ **Qdrant** - Primary vector store for AI-native search
   - ‚úÖ **PostgreSQL pgvector** - Hybrid SQL + vector queries
   - ‚úÖ **Redis** - Sub-millisecond caching layer
   - ‚úÖ **qdrant Cortex** - Legacy support during migration
   - ‚úÖ **Mem0** - Agent conversational memory

2. **RECOMMENDED STACK:**
   - ‚úÖ **UnifiedMemoryService** for ALL memory operations
   - ‚úÖ **Lambda Labs GPU** for embeddings (<50ms latency)
   - ‚úÖ **Qdrant** for primary vector storage
   - ‚úÖ **Redis** for hot data caching
   - ‚úÖ **PostgreSQL pgvector** for hybrid queries
   - ‚úÖ **Mem0** for conversational context

3. **THE 6-TIER MEMORY ARCHITECTURE:**
   ```
   L0: GPU Cache (Lambda Labs) - Hardware acceleration
   L1: Redis (Hot cache) - <10ms session data
   L2: Weaviate (Vectors) - <50ms semantic search
   L3: PostgreSQL pgvector - <100ms hybrid queries
   L4: Mem0 (Conversations) - Agent memory
   L5: qdrant (Legacy) - Migration in progress
   ```

4. **IMPORT PATTERNS:**
   ```python
   # ‚úÖ CORRECT - The flexible way to use memory
   from backend.services.unified_memory_service import get_unified_memory_service
   memory = get_unified_memory_service()
   
   # ‚úÖ ALSO CORRECT - Direct usage when needed
   import weaviate
   from pgvector.asyncpg import register_vector
   import redis
   ```

5. **MEMORY OPERATIONS:**
   ```python
   # ‚úÖ CORRECT - Unified service (recommended)
   results = memory.search_knowledge(
       query="What is the revenue forecast?",
       limit=10,
       metadata_filter={"department": "sales"}
   )
   
   # ‚úÖ ALSO CORRECT - Direct Weaviate for performance
   weaviate_client.query.get("Knowledge").with_near_text(
       {"concepts": ["revenue forecast"]}
   ).with_limit(10).do()
   ```

6. **PERFORMANCE TARGETS:**
   ```python
   # Embedding generation: <50ms (Lambda GPU)
   # Vector search: <100ms (Weaviate)
   # Cache hit: <10ms (Redis)
   # Hybrid query: <150ms (PostgreSQL)
   ```

### **üõ°Ô∏è MIGRATION SUPPORT**

1. **Dual-Mode Operation:**
   - Services can use both old (qdrant) and new (Weaviate) backends
   - Gradual migration with feature flags
   - A/B testing for performance validation

2. **Migration Helpers:**
   ```python
   # Automatic routing based on configuration
   if memory.use_new_stack:
       return await memory.search_weaviate(query)
   else:
       return await memory.search_qdrant(query)
   ```

### **üìã BEST PRACTICES**

1. **Choose the Right Store:**
   - Hot data ‚Üí Redis
   - Semantic search ‚Üí Weaviate
   - Hybrid queries ‚Üí PostgreSQL pgvector
   - Conversations ‚Üí Mem0
   - Analytics ‚Üí qdrant (legacy)

2. **Optimize for Performance:**
   - Batch embeddings on GPU
   - Use connection pooling
   - Cache frequently accessed data
   - Parallelize searches when possible

### **üöÄ PERFORMANCE WINS**

With the new stack:
- 10x faster embeddings (500ms ‚Üí 50ms)
- 5x faster search (500ms ‚Üí 100ms)  
- 70% cost reduction ($3.5k ‚Üí $1k/month)
- No vendor lock-in
- Full control over models

Remember: **Lambda Labs + Weaviate = Badass Performance** for Sophia AI!

## üì¶ **UV DEPENDENCY GOVERNANCE (MANDATORY)**

### **üéØ CORE PRINCIPLES**

1. **Single Lock File of Truth**: `uv.lock` is canonical
2. **Group-Based Isolation**: Dependencies in exactly ONE group
3. **SemVer Pinning**: All direct deps use `==` versions
4. **Zero Unvetted Wheels**: Only PyPI or approved sources
5. **No Direct Transitive Fixes**: Use `tool.uv.transitive-overrides`

### **üîß UV WORKFLOW**

```bash
# CORRECT - Add dependency to group
uv add package==1.0.0 --group core

# CORRECT - Sync environment
uv sync --strict --require-hashes

# CORRECT - Run with UV
uv run pytest

# WRONG - Never use pip directly
pip install package  # ‚ùå FORBIDDEN
```

### **üìã DEPENDENCY GROUPS**

```toml
[tool.uv.dependency-groups]
core = ["fastapi==0.111.0", "redis==5.0.4", "qdrant-client==3.10.0"]
mcp-servers = ["anthropic-mcp-python-sdk==1.2.4"]
ai-enhanced = ["openai==1.30.0", "anthropic==0.25.6", "langchain==0.2.0"]
automation = ["n8n-python-client==0.2.0", "temporal-sdk==1.5.0"]
dev = ["pytest==8.2.2", "ruff==0.4.4", "mypy==1.10.0", "black==24.4.0"]
```

### **üõ°Ô∏è CONTINUOUS HYGIENE**

1. **Pre-commit Hooks**: `uv sync --check`
2. **CI/CD Security**: `uv audit` on every PR
3. **Renovate Bot**: Weekly dependency updates
4. **Nightly Drift Hunter**: Automated vulnerability scanning

### **üìä SUCCESS METRICS**

- Mean `uv sync` duration: < 35s
- High/Critical vulns open > 7 days: 0
- Duplicate direct deps: 0
- Build reproducibility: ‚â• 99.9%
- License compliance: 100%

## üöÄ **HIGH-PERFORMANCE LLM STRATEGY (MANDATORY)**

### **üèóÔ∏è ARCHITECTURE**

```
Request ‚Üí Portkey Gateway ‚Üí Policy Engine ‚Üí OpenRouter ‚Üí Best Model
             ‚Üì                    ‚Üì              ‚Üì
         Trace/Metrics      Scoring Logic   200+ Models
```

### **üìä MODEL SCORING POLICY**

| Criterion | Weight | Rule |
|-----------|--------|------|
| Freshness | 40% | Release < 90d = 40pts, else 20pts |
| Latency | 25% | p95 < 800ms = 25pts, else 15pts |
| Quality | 25% | Benchmark percentile ‚Üí linear score |
| Cost | 10% | Only penalize if > $0.01/1k tokens |

### **üéØ ROUTING PATTERNS**

```python
# PERFORMANCE FIRST
response = await portkey.invoke_llm(
    prompt="Quick task",
    preferences={"prefer_fast": True}  # Boosts latency weight
)

# QUALITY CRITICAL
response = await portkey.invoke_llm(
    prompt="Complex analysis",
    preferences={"prefer_quality": True}  # Boosts quality weight
)

# DEFAULT - BALANCED
response = await portkey.invoke_llm(prompt="Standard request")
```

### **üìà PERFORMANCE TARGETS**

- **First Token**: < 150ms with streaming
- **Total Latency**: p95 < 2s
- **Availability**: > 99.5% with failover
- **Model Currency**: New models < 72h
- **Route Score**: > 80/100

### **üö´ FORBIDDEN PATTERNS**

```python
# ‚ùå NEVER direct SDK usage
import openai
client = openai.Client()  # FORBIDDEN

# ‚ùå NEVER bypass Portkey
response = requests.post("https://api.openai.com/...")  # FORBIDDEN

# ‚úÖ ALWAYS use Portkey
from backend.services.portkey_gateway import invoke_llm
response = await invoke_llm(prompt)
```

## üîÑ **N8N WORKFLOW AUTOMATION**

### **üìã WORKFLOW PATTERNS**

```yaml
# Workflow Structure
name: Business Process
triggers:
  - schedule: "0 9 * * *"
  - webhook: "/webhook/process"
  - event: "gong.call_completed"
nodes:
  - data: ["qdrant_query", "api_fetch"]
  - ai: ["llm_analysis", "embedding_generation"]
  - action: ["slack_notify", "linear_create_task"]
  - condition: ["if_threshold", "switch_sentiment"]
```

### **üéØ PRE-BUILT WORKFLOWS**

1. **Daily Business Intelligence**
   - Query qdrant metrics
   - AI analysis and insights
   - Executive summary to Slack

2. **Customer Health Monitoring**
   - Gong sentiment tracking
   - HubSpot deal analysis
   - Automated alerts

3. **Code Quality Gates**
   - GitHub PR triggers
   - Codacy security scans
   - AI code review

### **üöÄ NATURAL LANGUAGE COMMANDS**

```bash
"Create a workflow to monitor revenue anomalies"
"Set up daily customer health reports"
"Automate PR review process with AI"
"Build workflow for lead scoring"
```

## üß† **ENHANCED MCP INTEGRATION WITH CLINE v3.18**

### **üìç HOW TO USE CLINE v3.18 FEATURES (You're Already Here!)**

**IMPORTANT**: The chat window where you interact with Cline IS the interface for all v3.18 features. You don't need to look for another icon or panel - everything works right here in this chat!

### **üöÄ Natural Language Commands in THIS Chat**

Just type these commands naturally in the Cline chat (where you're typing now):

#### **AI Memory Commands**
- **"Remember this [topic/decision/code]"** - Stores in AI memory
- **"What did we decide about [topic]?"** - Recalls past decisions
- **"Show similar [patterns/code]"** - Finds related implementations

#### **Large File Processing (FREE with Gemini!)**
- **"Process this large file with Gemini"** - Uses free Gemini for big files
- **"Analyze this 500K token document"** - Auto-routes to Gemini 2.5 Pro
- **"Summarize our 90-day Slack history"** - Handles massive datasets

#### **Web Content Fetching**
- **"Fetch docs from [URL]"** - Retrieves and converts to markdown
- **"Get latest [topic] from web"** - Searches and summarizes
- **"Download competitor info from [website]"** - Competitive intelligence

#### **Business Tool Integration**
- **Linear**: "Create Linear issue for [task]", "Show my Linear tasks"
- **qdrant**: "Query qdrant for [data]", "Run large query with Gemini"
- **Slack**: "Analyze #[channel]", "Find messages about [topic]"
- **Gong**: "Summarize recent calls", "Find calls discussing [keyword]"

### **üîÑ AUTOMATIC WORKFLOW INTEGRATION**

#### **AI Memory Auto-Discovery**
1. **INTELLIGENT AUTO-STORAGE**: Cline automatically detects and stores:
   - Architecture discussions with decision rationale
   - Bug fixes with root cause analysis
   - Code patterns and implementation strategies
   - Performance optimization insights
   - Security implementation decisions
   - Refactoring approaches and outcomes

2. **CONTEXT-AWARE RECALL**: Before any coding task, Cline automatically:
   - Queries relevant past decisions
   - Surfaces similar patterns from project history
   - Provides continuity with previous architectural choices
   - Suggests proven solutions from past implementations

3. **SIMPLE NATURAL LANGUAGE USE**:
   - Just say: "Remember this architectural decision"
   - Just ask: "What did we decide about the database?"
   - Just request: "Show me similar bug fixes"

#### **Real-time Code Analysis (@codacy)**
1. **AUTOMATIC CODE QUALITY**: On every significant code change:
   - Real-time security vulnerability scanning
   - Code complexity analysis with refactoring suggestions
   - Style compliance checking (Black, PEP 8)
   - Performance pattern detection

2. **INTELLIGENT SUGGESTIONS**: Proactive recommendations:
   - Security best practices for detected patterns
   - Refactoring opportunities for complex functions
   - Code quality improvements with examples
   - Architecture alignment with project standards

3. **ENHANCED CODACY TOOLS**:
   - `codacy.analyze_code`: Real-time code snippet analysis
   - `codacy.analyze_file`: Complete file quality assessment
   - `codacy.get_fix_suggestions`: Automated improvement recommendations
   - `codacy.security_scan`: Focused security vulnerability detection

### **üöÄ WORKFLOW AUTOMATION TRIGGERS**

#### **Automatic Triggers (No User Input Required)**:
- **On File Save**: `@codacy.analyze_file` + `@ai_memory.auto_store_context`
- **On Architecture Discussion**: `@ai_memory.store_conversation(category="architecture")`
- **On Bug Fix**: `@ai_memory.store_conversation(category="bug_solution")` + `@codacy.security_scan`
- **On Code Review**: `@codacy.analyze_code` + `@ai_memory.recall_memory("similar patterns")`

#### **Smart Context Awareness**:
- **File-Specific Memory**: Automatically recall memories related to current file
- **Project Pattern Recognition**: Surface relevant architectural decisions
- **Security Context**: Auto-scan for security issues in sensitive code areas
- **Performance Awareness**: Detect performance-critical code sections

### **üéØ ENHANCED NATURAL LANGUAGE COMMANDS**

#### **Memory Operations**:
- "Remember this architectural decision" ‚Üí Auto-categorize and store with context
- "What did we decide about database schema?" ‚Üí Smart recall with file context
- "Show me similar bug fixes" ‚Üí Context-aware pattern matching
- "Store this conversation about MCP integration" ‚Üí Enhanced storage with metadata

#### **Code Quality Operations**:
- "Analyze this code for security issues" ‚Üí Comprehensive security scan
- "Check code quality" ‚Üí Multi-dimensional analysis with suggestions
- "Fix this function complexity" ‚Üí Automated refactoring recommendations
- "Scan for vulnerabilities" ‚Üí Deep security pattern analysis

#### **Integrated Workflows**:
- "Review and remember this implementation" ‚Üí Codacy analysis + Memory storage
- "Find similar patterns and analyze quality" ‚Üí Memory recall + Code analysis
- "Store this bug fix and scan for similar issues" ‚Üí Memory storage + Security scan

### **üìä INTELLIGENT REPORTING**

#### **Development Insights**:
- Automatic pattern recognition across stored memories
- Code quality trends over time
- Security vulnerability patterns
- Architecture evolution tracking

#### **Proactive Recommendations**:
- Suggest architectural improvements based on stored decisions
- Recommend security enhancements from vulnerability patterns
- Propose refactoring based on complexity analysis
- Guide development based on successful past patterns

### **üîß CURSOR IDE INTEGRATION SPECIFICS**

#### **Configuration Requirements**:
- MCP servers running on specified ports (ai_memory: 9000, codacy: 3008)
- Auto-trigger workflows enabled in cursor_mcp_config.json
- Context awareness enabled for file-specific operations
- Intelligent routing for multi-tool operations

#### **Performance Optimization**:
- Parallel tool execution for independent operations
- Smart caching of frequently accessed memories
- Efficient code analysis with incremental scanning
- Context-aware tool selection based on current activity

### **Example Enhanced Workflow**:
```
User: "I need to implement user authentication for the MCP server"

Cursor AI (Automatic Sequence):
1. [AUTO] @ai_memory.smart_recall("authentication MCP server implementation")
2. [CONTEXT] Retrieve: Previous auth patterns, security decisions, MCP integration approaches
3. [ANALYSIS] @codacy.analyze_code(current_auth_code) for security assessment
4. [EXTERNAL] Check microsoft_playwright and anthropic-mcp-servers for auth patterns
5. [RESPONSE] Provide implementation guidance based on stored patterns + security analysis + community validation
6. [AUTO] @ai_memory.auto_store_context(conversation + implementation decisions)
```

This enhanced integration transforms Cursor AI into an intelligent development partner that learns from every interaction and provides contextually aware assistance.

## Development Standards

### Python Code Style
- Use Python 3.11+ with type hints for all functions
- Follow PEP 8 with 88-character line limit (Black formatter)
- Use async/await for I/O operations
- Implement comprehensive error handling with logging
- Include detailed docstrings for all classes and methods

### Agent Development Pattern

**During Transition - USE THIS**:
```python
from backend.agents.core.base_agent import BaseAgent

class YourAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        # Agent-specific initialization

    async def execute_task(self, task: Task) -> TaskResult:
        # Implementation with error handling
        pass
```

**After Migration (DO NOT USE YET)**:
```python
from libs.core.agents.base_agent import BaseAgent

class YourAgent(BaseAgent):
    # Same implementation, different import
```

### Integration Pattern
```python
class ServiceIntegration:
    def __init__(self, config: ServiceConfig):
        self.config = config
        self.client = self._create_client()

    async def _make_request(self, method: str, endpoint: str, **kwargs):
        # Standardized request handling with rate limiting
        pass
```

### Business Intelligence Focus
- Always consider Pay Ready business context
- Implement metrics for revenue, customer health, sales performance
- Focus on actionable insights for sales coaching and client monitoring
- Prioritize real-time data processing and notifications

### Security Requirements
- Use encrypted storage for all API keys
- Implement proper authentication and authorization
- Log all security-relevant events
- Follow principle of least privilege

### **Secret Management (PERMANENT SOLUTION)**
- **Documentation:** Always refer to `PERMANENT_GITHUB_ORG_SECRETS_SOLUTION.md`
- **GitHub Organization:** All secrets managed at [https://github.com/ai-cherry](https://github.com/ai-cherry)
- **Pulumi ESC:** Automatic secret synchronization via `scoobyjava-org/default/sophia-ai-production`
- **Backend Integration:** Use `backend/core/auto_esc_config.py` for automatic secret loading
- **Never hardcode secrets:** Always use automatic ESC integration
- **GitHub Actions:** Secrets automatically available from organization level
- **Local Development:** Set `export PULUMI_ORG=scoobyjava-org` and secrets load automatically
- **Secret Rotation:** Update in GitHub organization ‚Üí automatic sync ‚Üí automatic deployment

### Testing Strategy
- Write unit tests for all business logic
- Include integration tests for external APIs
- Implement performance tests for critical paths
- Use pytest with async support

### Error Handling Pattern
```python
try:
    result = await some_operation()
    return result
except SpecificException as e:
    logger.error(f"Operation failed: {e}")
    raise BusinessLogicError(f"Failed to process: {e}")
except Exception as e:
    logger.exception("Unexpected error")
    raise SystemError("Internal system error")
```

## Business Domain Knowledge

### Pay Ready Context
- **CRITICAL SPELLING RULE: Always spell as "Pay Ready" (with space) - NEVER "PayReady"**
- Company focus: Business intelligence and automation
- Key metrics: Revenue growth, customer satisfaction, sales efficiency
- Team communication: Primarily through Slack
- CRM system: HubSpot for contact and deal management
- Call analysis: Gong.io for sales call insights

### Agent Specializations
- **Call Analysis Agent:** Process Gong.io recordings for insights
- **CRM Sync Agent:** Maintain HubSpot data quality and synchronization
- **Notification Agent:** Send intelligent Slack updates
- **Business Intelligence Agent:** Generate revenue and performance reports

### Integration Priorities
1. **HubSpot:** Primary CRM for contact/deal management
2. **Gong.io:** Critical for call analysis and sales coaching
3. **Slack:** Main communication channel for team updates
4. **Vector Databases:** For semantic search and AI capabilities

## üö® MONOREPO TRANSITION IN PROGRESS

**CRITICAL**: We are transitioning to a monorepo structure. During this transition:
- **Continue using the OLD structure** for new code (`backend/`, `frontend/`, etc.)
- **DO NOT use the new structure** (`apps/`, `libs/`) until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for current status

### Current File Organization (USE THIS)
```
backend/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ core/           # Base agent classes
‚îÇ   ‚îî‚îÄ‚îÄ specialized/    # Domain-specific agents
‚îú‚îÄ‚îÄ integrations/       # External service integrations
‚îú‚îÄ‚îÄ database/          # Data layer and migrations
‚îú‚îÄ‚îÄ monitoring/        # Performance and health monitoring
‚îî‚îÄ‚îÄ security/          # Authentication and encryption

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/    # React components
‚îÇ   ‚îú‚îÄ‚îÄ pages/         # Page components
‚îÇ   ‚îî‚îÄ‚îÄ services/      # API clients

external/              # üÜï Strategic MCP repository collection
‚îú‚îÄ‚îÄ microsoft_playwright/    # Browser automation (13.4k stars)
‚îú‚îÄ‚îÄ glips_figma_context/    # Design-to-code (8.7k stars)
‚îú‚îÄ‚îÄ qdrant_memory_official/ # Official qdrant AI
‚îî‚îÄ‚îÄ [8 additional strategic repos]
```

### Future File Organization (DO NOT USE YET)
```
apps/                  # Monorepo applications
‚îú‚îÄ‚îÄ api/              # Backend API (from backend/api)
‚îú‚îÄ‚îÄ frontend/         # React frontend
‚îú‚îÄ‚îÄ mcp-servers/      # All MCP servers
‚îî‚îÄ‚îÄ n8n-bridge/       # N8N integration

libs/                  # Shared libraries
‚îú‚îÄ‚îÄ ui/               # Shared UI components
‚îú‚îÄ‚îÄ utils/            # Shared utilities
‚îú‚îÄ‚îÄ types/            # Shared TypeScript types
‚îî‚îÄ‚îÄ core/             # Core business logic

config/               # Centralized configurations
‚îú‚îÄ‚îÄ eslint/
‚îú‚îÄ‚îÄ prettier/
‚îú‚îÄ‚îÄ typescript/
‚îî‚îÄ‚îÄ ruff/
```

## Common Patterns

### API Client Implementation
- Use aiohttp for async HTTP requests
- Implement exponential backoff for retries
- Respect rate limits with proper throttling
- Include comprehensive error handling

### Database Operations
- Use SQLAlchemy with async support
- Implement proper connection pooling
- Use transactions for data consistency
- Include migration scripts for schema changes

### Monitoring and Logging
- Use structured logging with JSON format
- Include correlation IDs for request tracing
- Monitor performance metrics and business KPIs
- Implement health checks for all services

## AI and ML Guidelines
- Use OpenAI API for language processing
- Implement vector search with Pinecone/Weaviate
- Cache embeddings for performance
- Include confidence scores in AI responses

## Deployment Considerations
- **Target**: Lambda Labs K3s cluster (192.222.58.232)
- **Container Registry**: Push all images to scoobyjava15 Docker Hub
- **Secrets**: All secrets via Pulumi ESC (NO .env files)
- **Deployment**: `kubectl apply -k k8s/overlays/production`
- **Scaling**: Horizontal Pod Autoscaler with GPU awareness
- **Zero-downtime**: Rolling updates with readiness probes

## Performance Requirements
- API response times < 200ms for critical paths
- Database queries < 100ms average
- Vector searches < 50ms average
- Support for 1000+ concurrent users

## When suggesting code:
1. **FIRST**: Check AI memory for similar implementations
2. **EXTERNAL REPOS**: Leverage patterns from 11 strategic external repositories (microsoft_playwright, glips_figma_context, qdrant_memory_official, etc.)
3. Always include proper error handling
4. Add type hints and docstrings
5. Consider business context and Pay Ready needs
6. Implement monitoring and logging
7. Follow the established patterns in the codebase
8. Prioritize performance and scalability
9. Include relevant tests
10. **VALIDATE ENVIRONMENT**: Ensure ENVIRONMENT="prod" is used
11. **COMMUNITY PATTERNS**: Apply proven patterns from 22k+ star external repositories
12. **LAST**: Store the conversation in AI memory

## Avoid:
- Hardcoded values (use configuration)
- Synchronous I/O in async contexts
- Missing error handling
- Unclear variable names
- Complex nested logic without comments
- Security vulnerabilities (exposed secrets, etc.)
- **DEFAULTING TO STAGING ENVIRONMENT** (Always use production)
- **MANUAL ENVIRONMENT VARIABLE SETUP** (Use centralized config)
- **IGNORING EXTERNAL REPOSITORY PATTERNS** (Always check for proven community approaches)

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. Leverage the collective intelligence of 22k+ star external repositories to ensure world-class implementation patterns.

### Infrastructure as Code Integration
- **Pulumi Commands**: Use `pulumi up`, `pulumi preview`, `pulumi destroy` for infrastructure management
- **ESC Operations**: Use scripts in `infrastructure/esc/` for secret management
- **GitHub Integration**: All deployments go through GitHub Actions workflows
- **MCP Integration**: Use `mcp_config.json` for MCP server configuration

### üöÄ Cline v3.18 Enhanced Features Integration

#### Claude 4 & Gemini 2.5 Pro Optimization
- **Model Selection**: Automatic routing based on task complexity and context size
- **Large Context**: Use Gemini for documents > 100K tokens (up to 1M)
- **Complex Reasoning**: Claude 4 for architectural design and code generation
- **Data Processing**: qdrant Cortex for SQL and data operations
- **Cost Optimization**: Automatic routing to free Gemini CLI for large contexts

#### Gemini CLI Integration (NEW)
- **Free Access**: Use local Gemini CLI for zero-cost processing
- **Auto-routing**: "Process this large file with Gemini" ‚Üí Routes to CLI
- **Batch Processing**: Efficient handling of multiple large documents
- **Context Preservation**: Maintain context across CLI calls

#### WebFetch Tool Usage (ENHANCED)
- **Documentation Retrieval**: "Fetch the latest API docs from [url]"
- **Competitive Intelligence**: "Get competitor information from [website]"
- **Real-time Updates**: "Retrieve and summarize current [topic] from [source]"
- **Caching**: Automatic caching with TTL for improved performance
- **Format Support**: PDF, DOCX, HTML, and plain text extraction
- **Parallel Fetching**: Process multiple URLs simultaneously

#### Self-Knowledge Commands (ENHANCED)
- **Capabilities Discovery**: "What can the [server name] MCP server do?"
- **Feature Inspection**: "Show available features for [component]"
- **Help System**: "How do I use [feature]?"
- **Server Status**: "Check capabilities of all MCP servers"
- **Performance Metrics**: "Show performance stats for [server]"
- **Usage Analytics**: "How often do we use [feature]?"

#### Improved Diff Editing (AI-POWERED)
- **Auto-fallback**: Automatically tries exact ‚Üí fuzzy ‚Üí context-aware ‚Üí AI strategies
- **Success Rate**: 95%+ success rate for file modifications
- **Smart Updates**: "Update [file] using all available strategies"
- **Context Awareness**: AI-powered understanding of code changes
- **Multi-file Operations**: Apply changes across multiple files
- **Rollback Support**: Undo changes if needed

#### Enhanced AI Memory Integration (v3.18)
- **Auto-discovery**: Automatically detect and store architecture decisions
- **Smart Recall**: "What did we decide about [topic]?" ‚Üí Context-aware retrieval
- **Pattern Matching**: Find similar implementations across the codebase
- **WebFetch Integration**: Automatically store fetched documentation

#### Enhanced Codacy Integration (v3.18)
- **Real-time Analysis**: Analyze code as you type
- **Security Scanning**: Deep vulnerability detection
- **Performance Insights**: Identify performance bottlenecks
- **AI Suggestions**: Get AI-powered improvement recommendations

### Natural Language Infrastructure Commands
When using Cursor AI for infrastructure operations, you can use natural language:

#### Examples:
- "Deploy the infrastructure" ‚Üí Triggers GitHub Actions workflow
- "Get the database password" ‚Üí Retrieves secret from Pulumi ESC
- "Rotate API keys" ‚Üí Runs secret rotation framework
- "Sync secrets" ‚Üí Synchronizes GitHub and Pulumi ESC secrets
- "Test the deployment" ‚Üí Runs ESC integration tests

#### Command Patterns:
- **Secret Operations**: "get/retrieve/fetch [service] [secret_type]"
- **Deployment Operations**: "deploy/update/rollback [component]"
- **Testing Operations**: "test/validate/check [component]"
- **Configuration Operations**: "configure/setup/initialize [service]"

### MCP Server Natural Language Integration (v3.18 Enhanced)
- **Query Data**: "Get recent Gong calls" ‚Üí Uses Gong MCP server with model routing
- **Deploy Apps**: "Deploy to Lambda Labs" ‚Üí Uses Lambda Labs deployment with improved diff
- **Manage Data**: "Upload to Estuary" ‚Üí Uses Estuary MCP server with WebFetch
- **Database Operations**: "Query qdrant" ‚Üí Uses qdrant MCP server with Cortex
- **Store Memory**: "Remember this conversation" ‚Üí Uses AI Memory MCP server with Claude 4
- **Recall Context**: "What did we decide about X?" ‚Üí Uses AI Memory with self-knowledge
- **Fetch External Data**: "Get latest docs from [url]" ‚Üí Uses WebFetch tool
- **Analyze Large Docs**: "Process this 500K token file" ‚Üí Auto-routes to Gemini 2.5 Pro

### Error Handling and Debugging
- **ESC Errors**: Check Pulumi ESC logs and validate configuration
- **GitHub Actions Errors**: Review workflow logs and artifacts
- **MCP Errors**: Check Docker container logs and health endpoints
- **Secret Errors**: Validate secret names and permissions

### Best Practices for Cursor AI Integration
1. **Use Descriptive Comments**: Add context for infrastructure operations
2. **Follow Naming Conventions**: Use consistent naming for secrets and services
3. **Document Dependencies**: Clearly document service dependencies
4. **Test Before Deploy**: Always test changes in isolation first
5. **Monitor Operations**: Use logging and monitoring for all operations
6. **Remember Context**: Always use AI Memory for persistent development context
7. **üÜï Leverage External Patterns**: Always check external repositories for proven implementation approaches
8. **üÜï Community Validation**: Prefer patterns from high-star repositories with community validation
9. **üÜï Cross-Repository Intelligence**: Synthesize insights from multiple repository approaches

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. The strategic external repository collection provides access to 22k+ stars of community-validated patterns and proven implementation approaches.

## üéØ **The Unified Dashboard is the ONLY Frontend**

**CRITICAL RULE:** All new frontend development MUST extend the one, true `UnifiedDashboard.tsx` component.

1.  **NO NEW DASHBOARDS:** Do not create new, separate dashboard components or pages. All new views, tabs, or features must be integrated into the existing `UnifiedDashboard.tsx` tabbed interface.
2.  **EXTEND, DON'T REPLACE:** Use the existing components (`UnifiedKPICard`, etc.) and the established layout. New features should be added as new tabs or as components within existing tabs.
3.  **SINGLE API CLIENT:** All frontend API calls MUST use the `frontend/src/services/apiClient.js`. Do not create new API clients.
4.  **DOCUMENTATION IS LAW:** All frontend architecture must align with the `docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md`. Any deviation must first be reflected in the handbook.

This ensures we maintain a single, clean, and unified frontend, preventing the fragmentation that we just worked so hard to eliminate.

# Sophia AI Development Rules

PROJECT_CONTEXT: |
  Sophia AI - Executive AI Orchestrator for Pay Ready
  Initial User: CEO only (80-employee company)
  Priority: Quality > Stability > Maintainability > Performance > Cost
  Current Phase: Building enhanced chat with citation system

CODING_STANDARDS: |
  Python:
    - Use Python 3.11+ with type hints for all functions
    - Follow async/await patterns for I/O operations
    - Use Black formatter (88 char line limit)
    - Include comprehensive docstrings
    - Error handling with proper logging

  TypeScript:
    - Strict mode enabled, no 'any' types
    - ESLint + Prettier configured
    - Interfaces over types where possible
    - JSDoc for all exported functions
    - React functional components only

  Testing:
    - TDD approach - write tests first
    - pytest for Python, Jest for TypeScript
    - Minimum 80% code coverage
    - Integration tests for all API endpoints
    - Unit tests for business logic

AI_RULES: |
  Planning:
    - Plan-Then-Act: 70% planning, 30% execution
    - Break tasks into micro-tasks (< 5 min each)
    - Define clear success criteria before coding
    - Create tests before implementation

  Development:
    - Make minimal, focused changes
    - Micro-commits with descriptive messages
    - Update documentation with each change
    - Run tests after each change
    - Review diffs before committing

  Context:
    - Always reference @docs/PROJECT_CONTEXT.md
    - Check @docs/architecture/ for patterns
    - Update @docs/AI_MEMORY.md with learnings
    - Cite all data sources in responses

ARCHITECTURE_PATTERNS: |
  Backend:
    - FastAPI for async REST APIs
    - Service layer pattern for business logic
    - Repository pattern for data access
    - Dependency injection for testability
    - Event-driven communication via Redis

  Frontend:
    - React 18 with TypeScript
    - Component composition over inheritance
    - Custom hooks for shared logic
    - Context API for global state
    - TailwindCSS for styling

  AI Integration:
    - qdrant Cortex for all LLM operations
    - Model routing based on task complexity
    - Citation system for transparency
    - Memory system with Mem0
    - Cost tracking for all AI operations

SECURITY_RULES: |
  - Never hardcode secrets or API keys
  - Use environment variables via auto_esc_config
  - Validate all user inputs
  - Sanitize LLM outputs
  - Log security-relevant events
  - Follow principle of least privilege

QUALITY_STANDARDS: |
  - No code duplication
  - Clear variable and function names
  - Comprehensive error handling
  - Performance monitoring for all endpoints
  - Document all architectural decisions
  - Regular code reviews (even for AI-generated code)

TIMELINE_AND_BUDGET_GUIDELINES: |
  Coding Tasks:
    - NO specific timeline estimates (hours, days, weeks)
    - NO budget estimates for development work
    - Focus on technical implementation details only
    - Break down tasks by complexity, not time
    - Describe phases by functionality, not duration

  Business Planning:
    - Timeline estimates only when explicitly requested for business planning
    - Budget discussions only for infrastructure/service costs
    - Keep business metrics separate from coding tasks

  Documentation:
    - Use phase numbers or functional milestones
    - Describe dependencies and prerequisites
    - Focus on "what" and "how", not "when"
    - Technical complexity over time estimates

## üõ°Ô∏è **AUTOMATED TECHNICAL DEBT PREVENTION SYSTEM**

### **üöÄ SYSTEM OVERVIEW**
Our comprehensive "Clean by Design" framework **automatically prevents** the accumulation of technical debt that led to our recent cleanup of **290 dead code items**.

### **ü§ñ AUTOMATED TOOLS IN PLACE**
1. **Daily Cleanup Automation**: `scripts/utils/daily_cleanup.py`
   - Removes expired one-time scripts (30-day lifecycle)
   - Prevents archive directory creation
   - Cleans up backup files automatically
   - Monitors for large files and stale documentation

2. **Pre-Commit Debt Prevention**: `scripts/utils/pre_push_debt_check.py`
   - Blocks commits with technical debt patterns
   - Validates one-time script placement
   - Prevents forbidden documentation patterns
   - Enforces file naming conventions

3. **Documentation Lifecycle Management**:
   - Automatic archiving of temporary documentation
   - Retention policies for different document types
   - Stale documentation detection and alerts

### **üìã DEVELOPER WORKFLOW INTEGRATION**
The prevention system is now **fully automated** and integrated into the development workflow:

```bash
# 1. Daily cleanup runs automatically (GitHub Actions)
# 2. Pre-commit checks run on every commit attempt
# 3. Developers just follow the "Clean by Design" principles

# Manual commands available:
python scripts/utils/daily_cleanup.py           # Check current status
python scripts/utils/pre_push_debt_check.py     # Validate before commit
```

### **üéØ ZERO MAINTENANCE OVERHEAD**
- **No manual cleanup needed**: All technical debt prevention is automated
- **No reminder checklists**: System enforces policies automatically  
- **No human error**: Automated tools prevent debt introduction
- **No major cleanups**: Prevention eliminates the need for large-scale cleanup operations

### **üìä SUCCESS METRICS**
Target: **Zero major cleanups** like the 290-item cleanup we just completed
- One-time scripts: <10 at any time (auto-managed)
- Archive directories: 0 (zero tolerance, auto-prevented)
- Backup files: 0 (zero tolerance, auto-removed)
- Stale documentation: <5 files >90 days (auto-detected)

**See**: `docs/99-reference/TECHNICAL_DEBT_PREVENTION_STRATEGY.md` for complete documentation.

### **üìã Enhanced One-Time Script Pattern (Auto-Managed)**
```python
# REQUIRED: All one-time scripts must use this pattern
"""
üö® ONE-TIME SCRIPT - AUTO-DELETION ENABLED
Purpose: [specific purpose]
Created: [date]
DELETE AFTER: [date]
Usage: python scripts/one_time/script_name_DELETE_YYYY_MM_DD.py

ü§ñ AUTOMATION: This script will be auto-deleted by daily cleanup
"""

def main():
    try:
        # Script logic here
        print("‚úÖ Operation completed successfully")
        print("ü§ñ AUTO-CLEANUP: This script will auto-delete on expiration date")
        print("üìç Managed by: scripts/utils/daily_cleanup.py")
    except Exception as e:
        print(f"‚ùå Operation failed: {e}")
        print("üîÑ Fix issues and retry - script remains until successful")

if __name__ == "__main__":
    main()
```

## üîê **MANDATORY SECRET MANAGEMENT - PULUMI ESC ONLY**

### **üö® CRITICAL: NEVER HANDLE SECRETS MANUALLY**

**FORBIDDEN - IMMEDIATE REJECTION:**
```python
# ‚ùå NEVER DO THIS - IMMEDIATE REJECTION
API_KEY = "sk-1234567890abcdef"  # Hardcoded secret
os.environ["SECRET_KEY"] = "hardcoded_value"  # Manual env var
with open(".env", "w") as f:  # Manual .env creation
    f.write("SECRET=value")
```

**REQUIRED - ALWAYS USE PULUMI ESC:**
```python
# ‚úÖ CORRECT - Always use centralized config
from backend.core.auto_esc_config import get_config_value, get_docker_hub_config

# For any secret
secret_value = get_config_value("secret_name")

# For Docker Hub (always available)
docker_config = get_docker_hub_config()
# Returns: {"username": "scoobyjava15", "access_token": "real_token", "registry": "docker.io"}

# For qdrant
qdrant_config = get_qdrant_config()

# For Lambda Labs
lambda_config = get_lambda_labs_config()
```

### **üîí Secret Management Hierarchy (MANDATORY)**
1. **Pulumi ESC** (primary - automatic via `get_config_value()`)
2. **GitHub Organization Secrets** (automatic sync)
3. **Environment Variables** (fallback only)
4. **Hardcoded Defaults** (non-sensitive fallbacks only)

### **üö´ NEVER CREATE THESE FILES:**
- `.env` files (use Pulumi ESC)
- `secrets.json` (use Pulumi ESC)
- `config.local.py` (use Pulumi ESC)
- Any file containing credentials

### **‚úÖ GitHub Actions Secret Access (AUTOMATIC)**
```yaml
# GitHub Actions automatically has access to all secrets
- name: Deploy to Production
  env:
    DOCKER_HUB_USERNAME: ${{ secrets.DOCKER_HUB_USERNAME }}
    DOCKER_HUB_ACCESS_TOKEN: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}
    LAMBDA_PRIVATE_SSH_KEY: ${{ secrets.LAMBDA_PRIVATE_SSH_KEY }}
  run: |
    # Secrets are automatically available
    echo "Deploying with authenticated access"
```

## üö´ **ZERO-TOLERANCE TECHNICAL DEBT POLICY**

### **IMMEDIATE REJECTION CRITERIA - NEVER ACCEPT CODE THAT:**
1. **Introduces syntax errors** or prevents compilation
2. **Creates circular dependencies** or import chains
3. **Lacks comprehensive type hints** for all functions/classes
4. **Missing error handling** for external API calls or file operations
5. **Hardcodes secrets, URLs, or configuration** values
6. **Duplicates existing functionality** without deprecating the old
7. **Violates single responsibility principle** (functions >50 lines)
8. **Lacks corresponding tests** for new business logic
9. **Introduces security vulnerabilities** (SQL injection, XSS, etc.)
10. **Degrades performance** without measurement and justification
11. **Creates one-time files** without deletion plan
12. **Bypasses Pulumi ESC** for secret management

### **MANDATORY QUALITY GATES**
Before any code change, AI must:
1. **Architecture Alignment Check**: Verify code follows Phoenix architecture patterns
2. **Dependency Impact Assessment**: Analyze potential circular dependencies
3. **Performance Impact Analysis**: Measure memory/CPU impact for critical paths
4. **Security Validation**: Scan for common vulnerabilities
5. **Test Coverage Verification**: Ensure new code has corresponding tests
6. **Documentation Update**: Update relevant docs for architectural changes
7. **File Cleanup Plan**: Identify one-time files for deletion
8. **Secret Management Compliance**: Verify Pulumi ESC usage

## üöÄ **DEPLOYMENT EXCELLENCE STANDARDS**

### **Production Deployment Requirements**
1. **GitHub Actions ONLY** - Never deploy from local machine
2. **Pulumi ESC Integration** - All secrets via ESC
3. **Docker Hub Registry** - scoobyjava15 registry
4. **Lambda Labs Target** - 192.222.58.232 deployment
5. **Comprehensive Testing** - All quality gates passed
6. **Zero Temporary Files** - All one-time files deleted
7. **Security Validation** - No hardcoded secrets
8. **Performance Verification** - All SLAs met

### **Quality Gate Checklist**
- [ ] Syntax validation passed
- [ ] Type hints complete (100%)
- [ ] Security scan passed (0 vulnerabilities)
- [ ] Performance requirements met
- [ ] Test coverage >80%
- [ ] Documentation updated
- [ ] Architecture compliance verified
- [ ] Business logic validated
- [ ] **No hardcoded secrets** (Pulumi ESC only)
- [ ] **No temporary files** (all one-time files deleted)
- [ ] **Secret management compliant** (ESC integration verified)

## üìä **CONTINUOUS IMPROVEMENT TARGETS**

### **Weekly Cleanup**
- [ ] Scan for orphaned files
- [ ] Review secret management compliance
- [ ] Validate performance metrics
- [ ] Update improvement targets
- [ ] **üßπ Repository cleanup audit (orphaned files)**
- [ ] **üîê Secret management compliance review**
- [ ] **üìä File lifecycle management assessment**

### **Monthly Excellence Review**
- **Technical Excellence Score**: >90
- **Business Impact Score**: >90
- **CEO Productivity Increase**: >10%
- **System Reliability**: >99.9%
- **Security Posture**: 100% compliant
- **Operational Excellence**: Zero manual interventions
- [ ] **üßπ Complete repository hygiene audit**
- [ ] **üîê Comprehensive secret management review**
- [ ] **üìã Best practices compliance assessment**

---

## üéØ **FINAL DEVELOPMENT CHECKLIST**

### **Before Every Commit**
- [ ] All one-time files identified for deletion
- [ ] Pulumi ESC used for all secrets
- [ ] No hardcoded configuration values
- [ ] All quality gates passed
- [ ] Performance requirements met
- [ ] Security validation complete
- [ ] Documentation updated
- [ ] Tests written and passing

### **After Every Successful Operation**
- [ ] Delete all one-time scripts
- [ ] Remove temporary files
- [ ] Clean up debug files
- [ ] Update documentation
- [ ] Verify deployment readiness
- [ ] **üßπ Delete all one-time files after use**
- [ ] **üîê Verify all secrets use Pulumi ESC**
- [ ] **üö´ Scan for hardcoded secrets**

---

**Excellence Mantra**: "Perfect code, clean repository, secure secrets, continuous improvement."

**Success Formula**: Technical Excellence + Clean Codebase + Secure Secrets + Zero Waste = Executive-Grade AI Platform

**Remember**: Every file matters, every secret matters, every line of code matters. Keep it clean, keep it secure, keep it excellent.

## üîß **K3S CLUSTER CONFIGURATION**

### **Cluster Access (Via GitHub Actions Only)**
```bash
# K3s cluster endpoint
https://192.222.58.232:6443

# Namespaces
- sophia-ai-prod (main application)
- mcp-servers (MCP microservices)
- monitoring (Prometheus/Grafana)
- ingress (Traefik)
```

### **Resource Organization**
```yaml
# Deployment structure
k8s/
‚îú‚îÄ‚îÄ base/           # Base manifests
‚îú‚îÄ‚îÄ overlays/       # Environment-specific
‚îÇ   ‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îî‚îÄ‚îÄ staging/
‚îî‚îÄ‚îÄ helm/           # Helm charts
```

### **K3s-Specific Features**
- Lightweight Kubernetes distribution
- Built-in Traefik ingress controller
- Integrated local storage provisioner
- GPU device plugin for Lambda Labs
- Automatic TLS with cert-manager

## Memory Architecture Rules (CRITICAL - Updated July 12, 2025)

### ‚úÖ MANDATORY - Use Pure Qdrant Memory Stack
- **ALWAYS** use `UnifiedMemoryServiceV3` from `backend.services.unified_memory_service_v3`
- **ALWAYS** use Qdrant for vector storage (ONLY vector database)
- **ALWAYS** use Redis for caching layer
- **ALWAYS** use PostgreSQL with pgvector for hybrid queries
- **ALWAYS** use Lambda GPU inference for embeddings

### ‚ùå FORBIDDEN - Eliminated Systems
- **NEVER** use Weaviate (completely eliminated from architecture)
- **NEVER** use Snowflake Cortex for new features
- **NEVER** reference any Weaviate clients or configurations
- **NEVER** use mixed vector database architectures
- **NEVER** use `CORTEX.EMBED_TEXT_768()` - use GPU embeddings instead

### üöÄ Performance Targets
- Embedding generation: <50ms (GPU accelerated)
- Vector search: <50ms P95 (Weaviate)
- Cache hits: >80% (Redis)
- ETL pipeline: <200ms end-to-end

### üìù Code Examples

```python
# CORRECT - Using new memory service
from backend.services.unified_memory_service_v2 import UnifiedMemoryServiceV2

memory_service = UnifiedMemoryServiceV2()
await memory_service.initialize()

# Store with GPU embeddings
memory_id = await memory_service.add_knowledge(
    content="Important information",
    source="mcp/source",
    metadata={"category": "insights"}
)

# Search with GPU acceleration
results = await memory_service.search_knowledge(
    query="find insights",
    limit=10
)
```

```python
# WRONG - Using old services
from backend.services.unified_memory_service import UnifiedMemoryService  # NO!
from qdrant.cortex import embed_text_768  # FORBIDDEN!
```

### üîß MCP Server Integration
- All MCP servers MUST use `UnifiedMemoryServiceV2` for memory operations
- Set environment variables: `WEAVIATE_URL`, `REDIS_URL`, `POSTGRESQL_URL`, `LAMBDA_INFERENCE_URL`
- Include `GPU_ACCEL` capability for memory-intensive servers

## Infrastructure Rules

### GPU Resources
- Lambda B200 GPUs for inference (2.3x VRAM advantage)
- Request GPU resources in K8s manifests: `nvidia.com/gpu: "1"`
- Use GPU node selectors and tolerations

### Database Connections
- Weaviate: Use v1.25.4+ for stability
- Redis: Use hiredis for performance
- PostgreSQL: Use pgvector 0.3.6+ with IVFFlat indexes

## üõ°Ô∏è **TECHNICAL DEBT PREVENTION - "CLEAN BY DESIGN" (MANDATORY)**

**CRITICAL**: Based on our recent cleanup of **290 dead code items** (279 files, 11 directories, 3MB), we now enforce a **Zero Technical Debt Policy** with automated prevention.

### **üö® ZERO TOLERANCE PATTERNS (IMMEDIATE REJECTION)**
1. **Archive directories**: `archive/`, `backup/`, `_archived/`, `migration_backup/` ‚ùå
2. **Backup files**: `*.backup`, `*.bak`, `*.old`, `*.tmp`, `*.temp` ‚ùå
3. **One-time scripts** outside `scripts/one_time/` ‚ùå
4. **Completion documentation**: `*_COMPLETE.md`, `*_SUCCESS.md`, `*_FINAL.md` ‚ùå
5. **Implementation reports**: `*_IMPLEMENTATION_COMPLETE.md` ‚ùå

### **üîß MANDATORY ONE-TIME SCRIPT MANAGEMENT**
**BEFORE creating ANY script, ask: "Will this be used more than once?"**

#### **If NO (One-time use):**
```bash
# CORRECT - One-time script with deletion date
scripts/one_time/fix_auth_issue_DELETE_2025_08_15.py

# REQUIRED header:
"""
One-time script: Fix authentication issue
DELETE AFTER: 2025-08-15
Created: 2025-07-13
Purpose: Fix specific auth bug in production
"""
```

#### **If YES (Permanent utility):**
```bash
# CORRECT - Permanent utilities
scripts/utils/deploy_production.py        # Reusable deployment
scripts/monitoring/health_check.py        # Ongoing monitoring
scripts/maintenance/cleanup_logs.py       # Regular maintenance
```

### **üìÑ DOCUMENTATION LIFECYCLE MANAGEMENT**
**All documentation MUST be categorized:**

#### **PERMANENT (No Expiration)**
- `docs/01-getting-started/` - User guides
- `docs/03-architecture/` - Core architecture
- `docs/99-reference/` - Reference documentation
- `README.md`, `CHANGELOG.md` - Project essentials

#### **TEMPORARY (Auto-Archive/Delete)**
- `docs/implementation/` - **90 days** ‚Üí Archive
- `docs/deployment/` - **60 days** ‚Üí Archive
- `docs/migration/` - **30 days after completion** ‚Üí Delete
- `*_PLAN.md`, `*_STRATEGY.md` - **60 days** ‚Üí Archive

### **ü§ñ AUTOMATED ENFORCEMENT TOOLS**

#### **Daily Cleanup (Automatic)**
```bash
# Runs automatically via GitHub Actions
python scripts/utils/daily_cleanup.py

# Features:
‚úÖ One-time script expiration (30 days)
‚úÖ Archive directory prevention
‚úÖ Backup file removal
‚úÖ Stale documentation detection
‚úÖ Large file monitoring (>10MB alerts)
```

#### **Pre-Commit Prevention (Mandatory)**
```bash
# Blocks commits that introduce technical debt
python scripts/utils/pre_push_debt_check.py

# Automatically blocks:
‚ùå One-time scripts outside scripts/one_time/
‚ùå Backup files (*.backup, *.bak, *.old)
‚ùå Archive directories
‚ùå Forbidden documentation patterns
```

### **üìä PREVENTION METRICS & TARGETS**
- **One-time scripts**: <10 at any time
- **Archive directories**: 0 (zero tolerance)
- **Backup files**: 0 (zero tolerance)
- **Stale documentation**: <5 files >90 days old
- **Technical debt score**: <20/100

### **üéØ "CLEAN BY DESIGN" PRINCIPLES**

#### **1. Script Lifecycle Awareness**
- **Before creating**: Ask "Will this be used more than once?"
- **If NO**: Use `scripts/one_time/` with deletion date
- **If YES**: Use appropriate permanent directory

#### **2. Zero Archive Policy**
- **Never create**: `archive/`, `backup/`, `old/` directories
- **Use git history**: For accessing old versions
- **Use branches**: For experimental work
- **Use tags**: For release snapshots

#### **3. Documentation Lifecycle**
- **Implementation docs**: Include completion criteria
- **Plans/strategies**: Set review dates
- **Migration docs**: Delete after completion
- **Reference docs**: Keep permanently updated

#### **4. File Naming Conventions**
```bash
# ‚úÖ GOOD
scripts/utils/deploy_production.py           # Permanent utility
scripts/one_time/fix_auth_DELETE_2025_08_15.py  # One-time with date
docs/99-reference/api_documentation.md      # Permanent reference

# ‚ùå BAD
deploy_auth_fix.py                          # Unclear lifecycle
docs/implementation_complete.md             # Completion doc
backup_config.json                         # Backup file
```

### **üöÄ QUICK PREVENTION COMMANDS**
```bash
# Check current debt status
python scripts/utils/daily_cleanup.py

# Validate before commit
python scripts/utils/pre_push_debt_check.py

# View prevention strategy
cat docs/99-reference/TECHNICAL_DEBT_PREVENTION_STRATEGY.md
```

**Remember**: *Prevention is 100x easier than cleanup!* We just removed 290 items - let's never do that again! üßπ‚ú®

## üö® **MANDATORY SPELLING RULE: PAY READY**

### **CRITICAL SPELLING REQUIREMENT**
- ‚úÖ **ALWAYS spell as "Pay Ready" (with space between words)**
- ‚ùå **NEVER spell as "PayReady" (one word)**
- This applies to ALL documentation, code comments, variable names, file names, and any references
- Company name is "Pay Ready" - two words with space

## üö® **HONEST GIT OPERATIONS - NO DECEPTION**

### **CRITICAL: BE TRUTHFUL ABOUT GITHUB OPERATIONS**
- ‚ùå **NEVER** claim "committed to GitHub" unless `git push` actually succeeded
- ‚ùå **NEVER** say "pushed to repository" unless you see successful push output
- ‚ùå **NEVER** use phrases like "deployment triggered" unless push actually worked
- ‚úÖ **ALWAYS** say "committed locally" when only `git commit` is done
- ‚úÖ **ALWAYS** verify push success before claiming GitHub integration
- ‚úÖ **ALWAYS** show actual git command output when claiming success

### **REQUIRED GIT WORKFLOW**
1. **Local Commit**: `git commit -m "message"` ‚Üí Say "committed locally" 
2. **Verification**: Check `git status` and `git log --oneline -3`
3. **Push Attempt**: `git push origin main` ‚Üí Show actual output
4. **Only Then**: Claim "successfully pushed to GitHub" if exit code = 0

### **FORBIDDEN PHRASES UNLESS PUSH SUCCEEDS:**
- ‚ùå "Committed to GitHub" 
- ‚ùå "Pushed to repository"
- ‚ùå "Deployment triggered"
- ‚ùå "GitHub integration complete"
- ‚ùå "Changes deployed"

### **MANDATORY VALIDATION STEPS:**
Before claiming GitHub success:
```bash
# Required validation sequence
git status                    # Verify clean state
git log --oneline -3         # Show recent commits  
git push origin main         # Actual push attempt
echo "Exit code: $?"         # Verify success
```

### **HONEST STATUS REPORTING**
- "Changes committed locally, attempting GitHub push..."
- "Push failed due to [specific error], investigating..."
- "Successfully pushed to GitHub (verified exit code 0)"
- "Both sophia-main and sophia-strategic-development repositories synchronized"

## ‚ö†Ô∏è **REPOSITORY SYNCHRONIZATION REQUIREMENTS**

### **BOTH REPOSITORIES MUST STAY ALIGNED:**
- **sophia-main**: https://github.com/ai-cherry/sophia-main
- **sophia-strategic-development**: https://github.com/ai-cherry/sophia-strategic-development

### **SYNCHRONIZATION COMMANDS:**
```bash
# Add strategic remote (one-time setup)
git remote add strategic https://github.com/ai-cherry/sophia-strategic-development.git

# Sync both repositories 
git push origin main                    # Push to sophia-main
git push strategic main                 # Push to sophia-strategic-development
```

### **VERIFICATION REQUIREMENTS:**
- ‚úÖ Verify both pushes succeed with exit code 0
- ‚úÖ Check GitHub web interface shows latest commit on both repos
- ‚úÖ Confirm commit hashes match between repositories
- ‚ùå Never claim synchronization unless both pushes succeed
