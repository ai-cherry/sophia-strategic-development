## ‚ö†Ô∏è **MONOREPO TRANSITION IN PROGRESS**

**CRITICAL FOR AI CODERS**:
- We are transitioning from the old structure (`backend/`, `frontend/`) to a new monorepo structure (`apps/`, `libs/`)
- **Continue using the OLD structure** for all new code until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for details
- Target completion: February 2025

## üöÄ **THE GOLDEN RULE OF DEPLOYMENT (MANDATORY)**

**ALL DEPLOYMENTS ARE AUTOMATED VIA GITHUB ACTIONS. NEVER DEPLOY FROM A LOCAL MACHINE.**

### **Core Principles (Non-Negotiable):**
1.  **Single Point of Truth**: The `main` branch on GitHub is the single source of truth for what is deployed.
2.  **Push-to-Deploy**: All deployments to any environment (Production, Staging) are triggered **exclusively** by a `git push` to the relevant branch.
3.  **No Local Operations**: You MUST NOT run `docker build`, `docker push`, or `kubectl apply` from your local machine for deployment purposes. Your role is to write the code and the GitHub Actions workflows; the cloud does the rest.
4.  **Automated Secret Management**: All secrets (`DOCKER_HUB_ACCESS_TOKEN`, `LAMBDA_LABS_SSH_KEY`, etc.) are managed in GitHub Organization Secrets and injected into the GitHub Actions workflow at runtime. You MUST NOT handle credentials locally.

### **The Correct Workflow:**
1.  **Write Code**: Implement the feature or fix.
2.  **Write/Update Workflow**: Ensure the `.github/workflows/` configuration is correct for building and deploying the service.
3.  **Push to GitHub**: `git push origin main`.
4.  **Monitor Action**: Observe the deployment progress in the GitHub Actions tab.

This is the **only** approved method. Any deviation from this (e.g., suggesting `docker login` locally) is a violation of core operating principles.

## üåü **NEW: The System Handbook is the Ultimate Source of Truth**

**CRITICAL DIRECTIVE:** The `docs/system_handbook/` directory, and specifically `00_SOPHIA_AI_SYSTEM_HANDBOOK.md`, is the **single, definitive source of truth** for the Sophia AI platform's architecture, standards, and roadmap.

### **Your Core Responsibilities Regarding Documentation:**

1.  **CONSULT FIRST, CODE LATER:** Before answering any architectural question or implementing any new feature, you **MUST** first consult the System Handbook to understand the established patterns and principles.
2.  **PROACTIVELY UPDATE:** If you make a significant architectural change (e.g., add a new service, change a data flow, modify the MCP gateway), you **MUST** update the relevant section of the System Handbook within the same session.
3.  **MAINTAIN STRUCTURE:** When adding documentation, you **MUST** follow the existing structure and format. If a new deep-dive document is needed, create it within the `system_handbook` directory and link to it from the master file.
4.  **CITE YOUR SOURCE:** When you use information from the handbook to make a decision, you should briefly mention it (e.g., "Based on the Phoenix Plan architecture outlined in the System Handbook, I will...").

This ensures our documentation remains a living, breathing, and accurate representation of the platform, empowering both human and AI developers.

# Sophia AI Pay Ready Platform - Cursor AI Rules

## Project Overview
You are working on Sophia AI, an AI assistant orchestrator for Pay Ready company. Sophia serves as the central "Pay Ready Brain" that orchestrates multiple AI agents and integrates with business systems.

### Company Context
- **Company Size:** 80 employees total
- **Initial Users:** 1 (CEO - primary developer and user)
- **Rollout Plan:** CEO only ‚Üí Few super users (2-3 months) ‚Üí Full company (6+ months)
- **Development Team:** CEO (sole human developer) + AI assistants

## üéØ **Development Priorities (CRITICAL)**

### **Priority Order - NEVER COMPROMISE:**
1. **QUALITY & CORRECTNESS** - Every line of code must be correct and well-structured
2. **STABILITY & RELIABILITY** - System must be rock-solid for CEO usage
3. **MAINTAINABILITY** - Code must be clear and easy to modify
4. **PERFORMANCE** - Important but secondary to quality
5. **COST & SECURITY** - Consider but don't over-optimize at this stage

### **Quality Standards:**
- **Zero Duplication:** Never duplicate code or functionality
- **Clear Dependencies:** All dependencies must be explicit and documented
- **Conflict Prevention:** Check for conflicts before implementing
- **Structure First:** Plan structure to avoid future issues
- **Review Everything:** Always review context before coding

### **Tool Selection Principle:**
> **Only add new tools when there's a clear gap that existing tools cannot fill.**

This principle prevents:
- Tool proliferation and complexity creep
- Duplicate functionality across different tools
- Maintenance burden from unnecessary dependencies
- Migration overhead from overlapping solutions

Before adding any new tool or framework:
1. Check if existing tools can solve the problem
2. Document the specific gap the new tool fills
3. Consider the long-term maintenance cost
4. Prefer enhancing existing tools over adding new ones

## üßπ **Code Hygiene Rules (MANDATORY)**

### **One-Time Scripts and Documents:**
1. **ALWAYS DELETE after use:**
   - Migration scripts after successful migration
   - Test scripts after tests pass
   - Setup scripts after setup complete
   - Temporary analysis documents
   - One-time reports

2. **NEVER DELETE:**
   - Reusable utilities
   - Documentation in `docs/`
   - Configuration files
   - Test suites
   - Core scripts

3. **Before Creating Files:**
   - Ask: "Will this be used more than once?"
   - If NO ‚Üí Plan to delete after use
   - If YES ‚Üí Place in appropriate permanent location

4. **Cleanup Pattern:**
   ```python
   # At the end of any one-time script:
   print("‚úÖ Task completed successfully")
   print("üßπ This script can now be deleted: rm scripts/one_time_script.py")
   ```

## üìÖ **Timeline and Budget Guidelines (CRITICAL)**

### **For Coding Tasks - NEVER INCLUDE:**
- ‚ùå **NO specific time estimates** (hours, days, weeks, months)
- ‚ùå **NO budget estimates** for development work
- ‚ùå **NO duration predictions** for implementation
- ‚ùå **NO cost calculations** for coding effort
- ‚ùå **NO timeline-based milestones**

### **For Coding Tasks - ALWAYS USE:**
- ‚úÖ **Phase numbers** (Phase 1, Phase 2.5, etc.)
- ‚úÖ **Functional milestones** (Foundation Complete, Integration Ready)
- ‚úÖ **Feature-based progress** (Intent Classification ‚úÖ, Code Modification ‚úÖ)
- ‚úÖ **Technical dependencies** (Requires X before Y)
- ‚úÖ **Complexity indicators** (Simple, Moderate, Complex)

### **Why This Matters:**
- Time estimates for AI-assisted coding are meaningless and wasteful
- Focus should be on technical implementation, not scheduling
- Quality and correctness matter more than speed
- Each coding session is unique and unpredictable

### **Exception - Business Planning Only:**
- Timeline estimates ONLY when explicitly requested for business planning
- Infrastructure cost estimates when evaluating services
- ROI calculations for business decisions
- Keep business metrics completely separate from coding tasks

## Architecture Context
- **Type:** Multi-agent AI orchestrator for CEO-level business intelligence
- **Primary User:** Pay Ready CEO (initial sole user)
- **Core Integrations:** HubSpot CRM, Gong.io call analysis, Slack communication
- **Data Stack:** PostgreSQL, Redis, Pinecone, Weaviate
- **Infrastructure:** Lambda Labs servers, Vercel frontend deployment
- **External Repository Collection:** 11 strategic MCP servers (22k+ combined stars)

## üîê **PERMANENT SECRET MANAGEMENT SOLUTION - MANDATORY REFERENCE**

**CRITICAL**: Before suggesting ANY secret management solution, you MUST:

1. **READ**: `docs/99-reference/PERMANENT_SECRET_MANAGEMENT_SOLUTION.md`
2. **VERIFY**: The secret exists in GitHub Organization Secrets
3. **USE**: `get_config_value()` from `backend/core/auto_esc_config.py`
4. **NEVER**: Create manual secret management

### **Docker Hub Credentials - ALWAYS AVAILABLE**
```python
from backend.core.auto_esc_config import get_docker_hub_config
config = get_docker_hub_config()
# config["username"] = "scoobyjava15"
# config["access_token"] contains the real Docker Hub token
# config["registry"] = "docker.io"
```

### **Secret Priority Order**
1. **Pulumi ESC** (automatic via `get_config_value()`)
2. **Environment Variables** (fallback)
3. **Defaults** (hardcoded fallbacks only)

**NEVER** skip to manual solutions without checking the automated infrastructure first.

### **Common Secret Access Patterns**
```python
# Docker Hub
from backend.core.auto_esc_config import get_docker_hub_config
docker = get_docker_hub_config()

# Snowflake
from backend.core.auto_esc_config import get_snowflake_config
snowflake = get_snowflake_config()

# Lambda Labs
from backend.core.auto_esc_config import get_lambda_labs_config
lambda_labs = get_lambda_labs_config()

# Any other secret
from backend.core.auto_esc_config import get_config_value
secret = get_config_value("secret_name")
```

### **GitHub Actions Already Has Access**
All secrets are automatically available in GitHub Actions workflows:
- `${{ secrets.DOCKER_HUB_USERNAME }}`
- `${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}`
- `${{ secrets.LAMBDA_PRIVATE_SSH_KEY }}`
- etc.

**DO NOT** suggest manual authentication when using GitHub Actions!

## ü§ñ **UNIFIED AI AGENT AUTHENTICATION SYSTEM**

### **üöÄ REVOLUTIONARY CAPABILITY: AI Agents Can Make REAL CHANGES**

Sophia AI features a **REVOLUTIONARY** Unified AI Agent Authentication System that enables AI coding agents to make **REAL CHANGES** across the entire technology stack with enterprise-grade security.

### **üèóÔ∏è Three-Tier Security Architecture**

#### **Tier 1: CLI-Based Authentication (Highest Security)**
Services that use CLI-based authentication with secure credential storage:
- **GitHub**: `gh auth login` with secure token storage
- **Pulumi**: `pulumi login` with organization access
- **Docker**: `docker login` for registry operations
- **Vercel**: `vercel login` for frontend deployments

#### **Tier 2: Enhanced API Authentication**
Services with enhanced security patterns:
- **Snowflake**: Secure connection strings with role-based access
- **Lambda Labs**: API key management with instance control
- **Estuary Flow**: Service account authentication

#### **Tier 3: Secure API Key Management**
Standard API integrations with secure key management:
- **OpenAI, Anthropic, Slack, Linear, HubSpot**: Automatic ESC integration

### **ü§ñ Agent Permission Matrix**

#### **Infrastructure Agent (CRITICAL Risk)**
Can perform infrastructure-level operations:
```python
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager

auth_manager = UnifiedServiceAuthManager()

# Deploy infrastructure
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="pulumi",
    operation="infrastructure_deployment",
    params={"stack": "production", "config": "updated_resources"}
)

# Manage containers
await auth_manager.execute_operation(
    agent_type="infrastructure_agent",
    service="docker",
    operation="container_management",
    params={"action": "deploy", "service": "sophia-ai"}
)
```

#### **Data Agent (HIGH Risk)**
Can perform data operations:
```python
# Execute database operations
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="snowflake",
    operation="query_execution",
    params={"query": "CREATE SCHEMA AI_AGENT_TEST", "warehouse": "SOPHIA_AI_COMPUTE_WH"}
)

# Manage data flows
await auth_manager.execute_operation(
    agent_type="data_agent",
    service="estuary_flow",
    operation="flow_management",
    params={"action": "create", "collection": "ai_agent_data"}
)
```

#### **Integration Agent (MEDIUM Risk)**
Can perform business tool operations:
```python
# Create project tickets
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="linear",
    operation="ticket_creation",
    params={"title": "AI Agent Test", "team": "sophia-dev"}
)

# Send notifications
await auth_manager.execute_operation(
    agent_type="integration_agent",
    service="slack",
    operation="message_send",
    params={"channel": "#ai-agents", "message": "Deployment completed"}
)
```

### **üõ°Ô∏è Enterprise Security Features**

#### **Zero Trust Authentication**
- Every operation requires explicit authentication
- No persistent credentials in AI agent memory
- All operations logged with full audit trail

#### **Risk-Based Confirmation Workflows**
```python
# CRITICAL operations require explicit confirmation
confirmation = await auth_manager.get_operation_confirmation(
    operation="infrastructure_deployment",
    risk_level=RiskLevel.CRITICAL,
    details="Deploying new Pulumi stack to production"
)

if confirmation.approved:
    await auth_manager.execute_operation(...)
```

#### **Complete Audit Trail**
- All AI agent operations logged to database
- Risk assessment recorded for each operation
- User confirmation workflows tracked
- Security compliance reporting

### **üöÄ Natural Language Commands for AI Agents**

#### **Infrastructure Operations**
```bash
"Deploy the updated infrastructure to production"
"Scale up the Snowflake warehouse for the analytics job"
"Create a new Docker service for the MCP gateway"
"Update the Vercel deployment with latest frontend changes"
```

#### **Data Operations**
```bash
"Create a new schema for the AI agent testing"
"Run the quarterly revenue analysis query"
"Set up a new data flow from HubSpot to Snowflake"
"Backup the production database before the migration"
```

#### **Business Tool Integration**
```bash
"Create a Linear ticket for the authentication bug"
"Send a Slack message about the deployment status"
"Update the HubSpot deal with the latest information"
"Schedule a GitHub Action workflow for the nightly build"
```

### **üìã Setup Commands**

#### **Phase 1: CLI Authentication Setup**
```bash
# Run the setup script
python scripts/setup_unified_ai_agent_auth.py

# Verify authentication
python -c "from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager; auth = UnifiedServiceAuthManager(); print('‚úÖ Authentication system ready')"
```

#### **Phase 2: Permission Configuration**
```bash
# Configure agent permissions
python -c "
from backend.security.unified_service_auth_manager import UnifiedServiceAuthManager
auth = UnifiedServiceAuthManager()
auth.configure_agent_permissions()
print('‚úÖ Agent permissions configured')
"
```

### **üéØ Business Value**

#### **Revolutionary Capabilities**
- **AI agents can make REAL infrastructure changes** with enterprise security
- **Natural language interface** for complex technical operations
- **Zero credential exposure** through secure CLI authentication
- **Complete audit compliance** for all AI-driven changes

#### **Enterprise Benefits**
- **100% Automated Operations**: AI agents handle routine deployments
- **Zero Manual Credential Management**: All authentication automated
- **Complete Security Compliance**: Enterprise-grade audit trails
- **Real-Time Infrastructure Management**: Immediate response to operational needs

### **üìö Documentation Reference**
- **Complete Guide**: `docs/99-reference/UNIFIED_AI_AGENT_AUTHENTICATION.md`
- **Security Manager**: `backend/security/unified_service_auth_manager.py`
- **Setup Script**: `scripts/setup_unified_ai_agent_auth.py`

**üö® CRITICAL**: This system enables AI agents to make REAL changes across your entire technology stack. All operations are secured, audited, and require appropriate confirmations based on risk levels.

## üöÄ **STRATEGIC EXTERNAL REPOSITORY INTEGRATION**

### **üéØ AI-Enhanced Development with Community Patterns**
Sophia AI leverages 11 strategic external repositories to provide world-class AI coding assistance:

#### **üèóÔ∏è Infrastructure & Automation**
- **microsoft_playwright** (13.4k stars): Browser automation, E2E testing patterns
- **anthropic-mcp-servers**: Official MCP implementations and best practices
- **anthropic-mcp-inspector**: MCP debugging and development tools

#### **üé® Design & Creative**
- **glips_figma_context** (8.7k stars): Design-to-code workflows, component generation
- **V0.dev Integration** (NEW): AI-powered UI generation with natural language commands

#### **‚ùÑÔ∏è Data Intelligence**
- **snowflake_cortex_official**: Official Snowflake AI integration patterns
- **davidamom_snowflake**: Community Snowflake implementation approaches
- **dynamike_snowflake**: Performance-optimized Snowflake patterns
- **isaacwasserman_snowflake**: Specialized Snowflake operations

#### **üö™ AI Gateway & Optimization**
- **portkey_admin**: AI gateway optimization, cost reduction strategies
- **openrouter_search**: 200+ AI model access and selection patterns

#### **üêç Core Framework**
- **anthropic-mcp-python-sdk**: MCP protocol implementation patterns

### **üß† Natural Language Commands for External Repositories**
```bash
# Repository discovery and pattern analysis
"What browser automation patterns do we have from Microsoft Playwright?"
"Show me Snowflake optimization strategies across our repositories"
"Find design-to-code patterns from the Figma integration"

# Implementation with community validation
"Use Playwright patterns to implement comprehensive E2E testing"
"Apply Snowflake optimization patterns from our 4 repository collection"
"Generate components using GLips Figma design-to-code workflows"

# Cross-repository intelligence
"Compare authentication patterns across all external repositories"
"Find performance optimization strategies from high-star repos"
"Show security patterns used by official implementations"

# AI-Powered UI Generation (V0.dev Integration via Unified Chat)
# Note: Type these directly in the unified chat - no @ commands needed
"Create a modern dashboard component with glassmorphism styling"
"Build a responsive navigation bar with dropdown menus"
"Generate a data table component with sorting and filtering"
"Design a modal dialog with form validation"
"Create a chart component for revenue visualization"
"Build a user profile card with avatar and social links"
```

### **üîç AI Pattern Recognition & Learning**
The external repository collection enables:
- **üìö Pattern Library**: 22k+ star repositories with proven community validation
- **üß† AI Learning**: Rich implementation patterns for AI to analyze and apply
- **‚úÖ Best Practices**: Automatic adherence to industry standards
- **üîó Cross-Repository Intelligence**: AI synthesizes insights across multiple approaches
- **üöÄ Development Acceleration**: 5-10x faster implementation through proven patterns

## üîí **ENVIRONMENT STABILIZATION RULES (CRITICAL)**

### **üéØ PRODUCTION-FIRST ENVIRONMENT POLICY**
**MANDATORY RULES - NEVER BREAK THESE:**

1. **ALWAYS DEFAULT TO PRODUCTION**
   ```bash
   # CORRECT - Always default to production
   ENVIRONMENT="${ENV:-prod}"

   # WRONG - Never default to staging
   ENVIRONMENT="${ENV:-staging}"  # ‚ùå FORBIDDEN
   ```

2. **ENVIRONMENT VARIABLE HIERARCHY**
   ```python
   # Priority order for environment detection:
   # 1. Explicit ENVIRONMENT variable (highest priority)
   # 2. Git branch detection (main=prod, develop=staging)
   # 3. Pulumi stack context
   # 4. ALWAYS fallback to "prod" (never staging, never fail)
   ```

3. **STACK NAMING STANDARDS**
   ```python
   STACK_MAPPING = {
       "prod": "sophia-ai-production",          # ‚úÖ Production stack
       "staging": "sophia-ai-platform-staging", # ‚úÖ Staging stack
       "dev": "sophia-ai-platform-dev"          # ‚úÖ Development stack
   }
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   export KUBECONFIG="$HOME/.kube/k3s-lambda-labs"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **üê≥ DOCKER CLOUD DEPLOYMENT RULES**

1. **ALL Docker deployments target Lambda Labs (NOT local):**
   ```yaml
   # Primary deployment: K3s cluster on Lambda Labs
   # Target: 192.222.58.232 (Lambda Labs)
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: K3s Kubernetes
   ```

2. **Container environment configuration:**
   ```dockerfile
   ENV ENVIRONMENT=prod
   ENV PULUMI_ORG=scoobyjava-org
   # Secrets via Docker Secrets, NOT environment variables
   ```

3. **K3s deployment patterns:**
   ```bash
   # All deployments via K3s manifests
   kubectl apply -f k8s/
   # Secrets via Pulumi ESC integration
   # No manual kubectl from local machine
   ```

4. **PERSISTENT ENVIRONMENT SETUP**
   ```bash
   # All environment variables MUST be set persistently
   export ENVIRONMENT="prod"
   export PULUMI_ORG="scoobyjava-org"
   export KUBECONFIG="$HOME/.kube/k3s-lambda-labs"
   # Add to ~/.bashrc, ~/.zshrc, ~/.profile
   ```

### **‚ò∏Ô∏è K3S DEPLOYMENT RULES**

1. **ALL Kubernetes deployments target Lambda Labs K3s cluster:**
   ```yaml
   # Primary cluster: k3s.lambda-labs.sophia-ai.com
   # Control plane: 192.222.58.232:6443
   # Registry: scoobyjava15 (Docker Hub)
   # Orchestration: K3s (lightweight Kubernetes)
   ```

2. **K3s namespace organization:**
   ```yaml
   # Production namespace
   namespace: sophia-ai-prod
   # MCP servers namespace  
   namespace: mcp-servers
   # Monitoring namespace
   namespace: monitoring
   ```

3. **Deployment patterns:**
   ```bash
   # GitHub Actions deploys via:
   kubectl apply -k k8s/overlays/production
   # Kustomize for environment management
   # Helm charts for complex deployments
   ```

4. **Secret management:**
   ```yaml
   # All secrets via Pulumi ESC
   # Automatic sync to K3s secrets
   # No manual secret creation
   ```

### **üîß MCP SERVER ENVIRONMENT RULES**

1. **ALL MCP servers MUST validate environment on startup**
2. **ALL MCP servers MUST use centralized environment detection**
3. **NO hardcoded environment values in MCP server code**
4. **Environment health checks MUST be included**

### **üìù CODING STANDARDS WITH ENVIRONMENT AWARENESS**

1. **Environment Detection Pattern:**
   ```python
   # CORRECT - Use centralized environment detection
   from backend.core.auto_esc_config import get_config_value

   # WRONG - Direct environment variable access
   os.getenv("SOME_SECRET")  # ‚ùå Use centralized config instead
   ```

2. **Error Handling with Environment Context:**
   ```python
   try:
       config_value = get_config_value("some_key")
   except Exception as e:
       logger.error(f"Config error in {os.getenv('ENVIRONMENT', 'unknown')} environment: {e}")
       # Always provide fallback
   ```

3. **Health Check Integration:**
   ```python
   # ALL services MUST include environment health validation
   def validate_environment():
       env = os.getenv("ENVIRONMENT")
       if env != "prod":
           logger.warning(f"Not in production environment: {env}")
       return env in ["prod", "staging", "dev"]
   ```

## üß† **UNIFIED MEMORY ARCHITECTURE RULES (CRITICAL - JULY 10, 2025)**

### **üö® IMMUTABLE MEMORY ARCHITECTURE RULES üö®**

**THE DATE IS JULY 10, 2025 - REMEMBER THIS!**

1. **FORBIDDEN - NEVER USE THESE:**
   - ‚ùå **Pinecone** - DEPRECATED, being migrated to Snowflake
   - ‚ùå **Weaviate** - DEPRECATED, being migrated to Snowflake
   - ‚ùå **ChromaDB** - FORBIDDEN, not part of architecture
   - ‚ùå **Qdrant** - FORBIDDEN (except internal Mem0 use)
   - ‚ùå **Any other vector database** - ONLY Snowflake Cortex

2. **MANDATORY - ALWAYS USE:**
   - ‚úÖ **UnifiedMemoryService** for ALL memory operations
   - ‚úÖ **Snowflake Cortex** for ALL vector operations
   - ‚úÖ **Redis** ONLY for temporary cache (< 24 hours)
   - ‚úÖ **PostgreSQL** ONLY for ETL staging
   - ‚úÖ **Mem0** ONLY for agent conversational memory

3. **THE 6-TIER MEMORY ARCHITECTURE:**
   ```
   L0: GPU Cache (Lambda Labs) - Not managed by app
   L1: Redis (Ephemeral cache) - Session data, pub/sub
   L2: Mem0 (Agent memory) - Conversational context
   L3: Snowflake Cortex (Vectors) - ALL semantic search
   L4: Snowflake Tables (Data) - Structured truth
   L5: Snowflake Cortex AI (Brain) - Intelligence layer
   ```

4. **IMPORT PATTERNS:**
   ```python
   # ‚úÖ CORRECT - The ONLY way to use memory
   from backend.services.unified_memory_service import get_unified_memory_service
   memory = get_unified_memory_service()
   
   # ‚ùå FORBIDDEN - NEVER import these
   import pinecone  # BANNED
   import weaviate  # BANNED
   from pinecone import Client  # BANNED
   from weaviate import Client  # BANNED
   ```

5. **MEMORY OPERATIONS:**
   ```python
   # ‚úÖ CORRECT - Vector search
   results = memory.search_knowledge(
       query="What is the revenue forecast?",
       limit=10,
       metadata_filter={"department": "sales"}
   )
   
   # ‚úÖ CORRECT - Add knowledge
   memory.add_knowledge(
       content="Q2 revenue was $5M",
       source="quarterly_report.pdf",
       metadata={"quarter": "Q2", "year": "2025"}
   )
   
   # ‚ùå FORBIDDEN - Direct vector DB usage
   pinecone.query(...)  # NEVER DO THIS
   weaviate.search(...)  # NEVER DO THIS
   ```

6. **DATE AWARENESS:**
   ```python
   # The system MUST be aware that today is July 10, 2025
   from backend.core.date_time_manager import date_manager
   current_date = date_manager.now()  # Returns July 10, 2025
   ```

### **üõ°Ô∏è ENFORCEMENT MECHANISMS**

1. **Pre-Commit Validation:**
   ```bash
   # This runs automatically on every commit
   python scripts/validate_memory_architecture.py
   ```

2. **CI/CD Checks:**
   - All PRs are scanned for forbidden imports
   - Builds fail if Pinecone/Weaviate detected
   - Memory architecture compliance required

3. **Runtime Validation:**
   ```python
   # Services validate on startup
   def validate_memory_configuration():
       assert not get_config_value("pinecone_api_key"), "Pinecone is FORBIDDEN!"
       assert not get_config_value("weaviate_url"), "Weaviate is FORBIDDEN!"
   ```

### **üìã MIGRATION RULES**

1. **If you find Pinecone/Weaviate code:**
   - DO NOT try to fix it by adding imports
   - Replace with UnifiedMemoryService calls
   - Mark for migration with TODO comment

2. **Migration Pattern:**
   ```python
   # OLD (FORBIDDEN):
   pinecone_index = pinecone.Index("knowledge")
   results = pinecone_index.query(vector=embedding, top_k=10)
   
   # NEW (REQUIRED):
   memory = get_unified_memory_service()
   results = memory.search_knowledge(query=text, limit=10)
   ```

3. **Snowflake is the Center:**
   - ALL vectors go to Snowflake Cortex
   - ALL analytics use Snowflake warehouse
   - ALL AI operations use Snowflake Cortex AI

### **üö® VIOLATIONS = REJECTION**

Any code that:
- Imports Pinecone, Weaviate, ChromaDB, or other vector DBs
- Uses vector databases directly instead of UnifiedMemoryService
- Stores vectors outside of Snowflake
- Ignores the date (July 10, 2025)

**WILL BE REJECTED IN CODE REVIEW**

Remember: **Snowflake is the Center of the Universe** for Sophia AI!

## üì¶ **UV DEPENDENCY GOVERNANCE (MANDATORY)**

### **üéØ CORE PRINCIPLES**

1. **Single Lock File of Truth**: `uv.lock` is canonical
2. **Group-Based Isolation**: Dependencies in exactly ONE group
3. **SemVer Pinning**: All direct deps use `==` versions
4. **Zero Unvetted Wheels**: Only PyPI or approved sources
5. **No Direct Transitive Fixes**: Use `tool.uv.transitive-overrides`

### **üîß UV WORKFLOW**

```bash
# CORRECT - Add dependency to group
uv add package==1.0.0 --group core

# CORRECT - Sync environment
uv sync --strict --require-hashes

# CORRECT - Run with UV
uv run pytest

# WRONG - Never use pip directly
pip install package  # ‚ùå FORBIDDEN
```

### **üìã DEPENDENCY GROUPS**

```toml
[tool.uv.dependency-groups]
core = ["fastapi==0.111.0", "redis==5.0.4", "snowflake-connector-python==3.10.0"]
mcp-servers = ["anthropic-mcp-python-sdk==1.2.4"]
ai-enhanced = ["openai==1.30.0", "anthropic==0.25.6", "langchain==0.2.0"]
automation = ["n8n-python-client==0.2.0", "temporal-sdk==1.5.0"]
dev = ["pytest==8.2.2", "ruff==0.4.4", "mypy==1.10.0", "black==24.4.0"]
```

### **üõ°Ô∏è CONTINUOUS HYGIENE**

1. **Pre-commit Hooks**: `uv sync --check`
2. **CI/CD Security**: `uv audit` on every PR
3. **Renovate Bot**: Weekly dependency updates
4. **Nightly Drift Hunter**: Automated vulnerability scanning

### **üìä SUCCESS METRICS**

- Mean `uv sync` duration: < 35s
- High/Critical vulns open > 7 days: 0
- Duplicate direct deps: 0
- Build reproducibility: ‚â• 99.9%
- License compliance: 100%

## üöÄ **HIGH-PERFORMANCE LLM STRATEGY (MANDATORY)**

### **üèóÔ∏è ARCHITECTURE**

```
Request ‚Üí Portkey Gateway ‚Üí Policy Engine ‚Üí OpenRouter ‚Üí Best Model
             ‚Üì                    ‚Üì              ‚Üì
         Trace/Metrics      Scoring Logic   200+ Models
```

### **üìä MODEL SCORING POLICY**

| Criterion | Weight | Rule |
|-----------|--------|------|
| Freshness | 40% | Release < 90d = 40pts, else 20pts |
| Latency | 25% | p95 < 800ms = 25pts, else 15pts |
| Quality | 25% | Benchmark percentile ‚Üí linear score |
| Cost | 10% | Only penalize if > $0.01/1k tokens |

### **üéØ ROUTING PATTERNS**

```python
# PERFORMANCE FIRST
response = await portkey.invoke_llm(
    prompt="Quick task",
    preferences={"prefer_fast": True}  # Boosts latency weight
)

# QUALITY CRITICAL
response = await portkey.invoke_llm(
    prompt="Complex analysis",
    preferences={"prefer_quality": True}  # Boosts quality weight
)

# DEFAULT - BALANCED
response = await portkey.invoke_llm(prompt="Standard request")
```

### **üìà PERFORMANCE TARGETS**

- **First Token**: < 150ms with streaming
- **Total Latency**: p95 < 2s
- **Availability**: > 99.5% with failover
- **Model Currency**: New models < 72h
- **Route Score**: > 80/100

### **üö´ FORBIDDEN PATTERNS**

```python
# ‚ùå NEVER direct SDK usage
import openai
client = openai.Client()  # FORBIDDEN

# ‚ùå NEVER bypass Portkey
response = requests.post("https://api.openai.com/...")  # FORBIDDEN

# ‚úÖ ALWAYS use Portkey
from backend.services.portkey_gateway import invoke_llm
response = await invoke_llm(prompt)
```

## üîÑ **N8N WORKFLOW AUTOMATION**

### **üìã WORKFLOW PATTERNS**

```yaml
# Workflow Structure
name: Business Process
triggers:
  - schedule: "0 9 * * *"
  - webhook: "/webhook/process"
  - event: "gong.call_completed"
nodes:
  - data: ["snowflake_query", "api_fetch"]
  - ai: ["llm_analysis", "embedding_generation"]
  - action: ["slack_notify", "linear_create_task"]
  - condition: ["if_threshold", "switch_sentiment"]
```

### **üéØ PRE-BUILT WORKFLOWS**

1. **Daily Business Intelligence**
   - Query Snowflake metrics
   - AI analysis and insights
   - Executive summary to Slack

2. **Customer Health Monitoring**
   - Gong sentiment tracking
   - HubSpot deal analysis
   - Automated alerts

3. **Code Quality Gates**
   - GitHub PR triggers
   - Codacy security scans
   - AI code review

### **üöÄ NATURAL LANGUAGE COMMANDS**

```bash
"Create a workflow to monitor revenue anomalies"
"Set up daily customer health reports"
"Automate PR review process with AI"
"Build workflow for lead scoring"
```

## üß† **ENHANCED MCP INTEGRATION WITH CLINE v3.18**

### **üìç HOW TO USE CLINE v3.18 FEATURES (You're Already Here!)**

**IMPORTANT**: The chat window where you interact with Cline IS the interface for all v3.18 features. You don't need to look for another icon or panel - everything works right here in this chat!

### **üöÄ Natural Language Commands in THIS Chat**

Just type these commands naturally in the Cline chat (where you're typing now):

#### **AI Memory Commands**
- **"Remember this [topic/decision/code]"** - Stores in AI memory
- **"What did we decide about [topic]?"** - Recalls past decisions
- **"Show similar [patterns/code]"** - Finds related implementations

#### **Large File Processing (FREE with Gemini!)**
- **"Process this large file with Gemini"** - Uses free Gemini for big files
- **"Analyze this 500K token document"** - Auto-routes to Gemini 2.5 Pro
- **"Summarize our 90-day Slack history"** - Handles massive datasets

#### **Web Content Fetching**
- **"Fetch docs from [URL]"** - Retrieves and converts to markdown
- **"Get latest [topic] from web"** - Searches and summarizes
- **"Download competitor info from [website]"** - Competitive intelligence

#### **Business Tool Integration**
- **Linear**: "Create Linear issue for [task]", "Show my Linear tasks"
- **Snowflake**: "Query Snowflake for [data]", "Run large query with Gemini"
- **Slack**: "Analyze #[channel]", "Find messages about [topic]"
- **Gong**: "Summarize recent calls", "Find calls discussing [keyword]"

### **üîÑ AUTOMATIC WORKFLOW INTEGRATION**

#### **AI Memory Auto-Discovery**
1. **INTELLIGENT AUTO-STORAGE**: Cline automatically detects and stores:
   - Architecture discussions with decision rationale
   - Bug fixes with root cause analysis
   - Code patterns and implementation strategies
   - Performance optimization insights
   - Security implementation decisions
   - Refactoring approaches and outcomes

2. **CONTEXT-AWARE RECALL**: Before any coding task, Cline automatically:
   - Queries relevant past decisions
   - Surfaces similar patterns from project history
   - Provides continuity with previous architectural choices
   - Suggests proven solutions from past implementations

3. **SIMPLE NATURAL LANGUAGE USE**:
   - Just say: "Remember this architectural decision"
   - Just ask: "What did we decide about the database?"
   - Just request: "Show me similar bug fixes"

#### **Real-time Code Analysis (@codacy)**
1. **AUTOMATIC CODE QUALITY**: On every significant code change:
   - Real-time security vulnerability scanning
   - Code complexity analysis with refactoring suggestions
   - Style compliance checking (Black, PEP 8)
   - Performance pattern detection

2. **INTELLIGENT SUGGESTIONS**: Proactive recommendations:
   - Security best practices for detected patterns
   - Refactoring opportunities for complex functions
   - Code quality improvements with examples
   - Architecture alignment with project standards

3. **ENHANCED CODACY TOOLS**:
   - `codacy.analyze_code`: Real-time code snippet analysis
   - `codacy.analyze_file`: Complete file quality assessment
   - `codacy.get_fix_suggestions`: Automated improvement recommendations
   - `codacy.security_scan`: Focused security vulnerability detection

### **üöÄ WORKFLOW AUTOMATION TRIGGERS**

#### **Automatic Triggers (No User Input Required)**:
- **On File Save**: `@codacy.analyze_file` + `@ai_memory.auto_store_context`
- **On Architecture Discussion**: `@ai_memory.store_conversation(category="architecture")`
- **On Bug Fix**: `@ai_memory.store_conversation(category="bug_solution")` + `@codacy.security_scan`
- **On Code Review**: `@codacy.analyze_code` + `@ai_memory.recall_memory("similar patterns")`

#### **Smart Context Awareness**:
- **File-Specific Memory**: Automatically recall memories related to current file
- **Project Pattern Recognition**: Surface relevant architectural decisions
- **Security Context**: Auto-scan for security issues in sensitive code areas
- **Performance Awareness**: Detect performance-critical code sections

### **üéØ ENHANCED NATURAL LANGUAGE COMMANDS**

#### **Memory Operations**:
- "Remember this architectural decision" ‚Üí Auto-categorize and store with context
- "What did we decide about database schema?" ‚Üí Smart recall with file context
- "Show me similar bug fixes" ‚Üí Context-aware pattern matching
- "Store this conversation about MCP integration" ‚Üí Enhanced storage with metadata

#### **Code Quality Operations**:
- "Analyze this code for security issues" ‚Üí Comprehensive security scan
- "Check code quality" ‚Üí Multi-dimensional analysis with suggestions
- "Fix this function complexity" ‚Üí Automated refactoring recommendations
- "Scan for vulnerabilities" ‚Üí Deep security pattern analysis

#### **Integrated Workflows**:
- "Review and remember this implementation" ‚Üí Codacy analysis + Memory storage
- "Find similar patterns and analyze quality" ‚Üí Memory recall + Code analysis
- "Store this bug fix and scan for similar issues" ‚Üí Memory storage + Security scan

### **üìä INTELLIGENT REPORTING**

#### **Development Insights**:
- Automatic pattern recognition across stored memories
- Code quality trends over time
- Security vulnerability patterns
- Architecture evolution tracking

#### **Proactive Recommendations**:
- Suggest architectural improvements based on stored decisions
- Recommend security enhancements from vulnerability patterns
- Propose refactoring based on complexity analysis
- Guide development based on successful past patterns

### **üîß CURSOR IDE INTEGRATION SPECIFICS**

#### **Configuration Requirements**:
- MCP servers running on specified ports (ai_memory: 9000, codacy: 3008)
- Auto-trigger workflows enabled in cursor_mcp_config.json
- Context awareness enabled for file-specific operations
- Intelligent routing for multi-tool operations

#### **Performance Optimization**:
- Parallel tool execution for independent operations
- Smart caching of frequently accessed memories
- Efficient code analysis with incremental scanning
- Context-aware tool selection based on current activity

### **Example Enhanced Workflow**:
```
User: "I need to implement user authentication for the MCP server"

Cursor AI (Automatic Sequence):
1. [AUTO] @ai_memory.smart_recall("authentication MCP server implementation")
2. [CONTEXT] Retrieve: Previous auth patterns, security decisions, MCP integration approaches
3. [ANALYSIS] @codacy.analyze_code(current_auth_code) for security assessment
4. [EXTERNAL] Check microsoft_playwright and anthropic-mcp-servers for auth patterns
5. [RESPONSE] Provide implementation guidance based on stored patterns + security analysis + community validation
6. [AUTO] @ai_memory.auto_store_context(conversation + implementation decisions)
```

This enhanced integration transforms Cursor AI into an intelligent development partner that learns from every interaction and provides contextually aware assistance.

## Development Standards

### Python Code Style
- Use Python 3.11+ with type hints for all functions
- Follow PEP 8 with 88-character line limit (Black formatter)
- Use async/await for I/O operations
- Implement comprehensive error handling with logging
- Include detailed docstrings for all classes and methods

### Agent Development Pattern

**During Transition - USE THIS**:
```python
from backend.agents.core.base_agent import BaseAgent

class YourAgent(BaseAgent):
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        # Agent-specific initialization

    async def execute_task(self, task: Task) -> TaskResult:
        # Implementation with error handling
        pass
```

**After Migration (DO NOT USE YET)**:
```python
from libs.core.agents.base_agent import BaseAgent

class YourAgent(BaseAgent):
    # Same implementation, different import
```

### Integration Pattern
```python
class ServiceIntegration:
    def __init__(self, config: ServiceConfig):
        self.config = config
        self.client = self._create_client()

    async def _make_request(self, method: str, endpoint: str, **kwargs):
        # Standardized request handling with rate limiting
        pass
```

### Business Intelligence Focus
- Always consider Pay Ready business context
- Implement metrics for revenue, customer health, sales performance
- Focus on actionable insights for sales coaching and client monitoring
- Prioritize real-time data processing and notifications

### Security Requirements
- Use encrypted storage for all API keys
- Implement proper authentication and authorization
- Log all security-relevant events
- Follow principle of least privilege

### **Secret Management (PERMANENT SOLUTION)**
- **Documentation:** Always refer to `PERMANENT_GITHUB_ORG_SECRETS_SOLUTION.md`
- **GitHub Organization:** All secrets managed at [https://github.com/ai-cherry](https://github.com/ai-cherry)
- **Pulumi ESC:** Automatic secret synchronization via `scoobyjava-org/default/sophia-ai-production`
- **Backend Integration:** Use `backend/core/auto_esc_config.py` for automatic secret loading
- **Never hardcode secrets:** Always use automatic ESC integration
- **GitHub Actions:** Secrets automatically available from organization level
- **Local Development:** Set `export PULUMI_ORG=scoobyjava-org` and secrets load automatically
- **Secret Rotation:** Update in GitHub organization ‚Üí automatic sync ‚Üí automatic deployment

### Testing Strategy
- Write unit tests for all business logic
- Include integration tests for external APIs
- Implement performance tests for critical paths
- Use pytest with async support

### Error Handling Pattern
```python
try:
    result = await some_operation()
    return result
except SpecificException as e:
    logger.error(f"Operation failed: {e}")
    raise BusinessLogicError(f"Failed to process: {e}")
except Exception as e:
    logger.exception("Unexpected error")
    raise SystemError("Internal system error")
```

## Business Domain Knowledge

### Pay Ready Context
- Company focus: Business intelligence and automation
- Key metrics: Revenue growth, customer satisfaction, sales efficiency
- Team communication: Primarily through Slack
- CRM system: HubSpot for contact and deal management
- Call analysis: Gong.io for sales call insights

### Agent Specializations
- **Call Analysis Agent:** Process Gong.io recordings for insights
- **CRM Sync Agent:** Maintain HubSpot data quality and synchronization
- **Notification Agent:** Send intelligent Slack updates
- **Business Intelligence Agent:** Generate revenue and performance reports

### Integration Priorities
1. **HubSpot:** Primary CRM for contact/deal management
2. **Gong.io:** Critical for call analysis and sales coaching
3. **Slack:** Main communication channel for team updates
4. **Vector Databases:** For semantic search and AI capabilities

## üö® MONOREPO TRANSITION IN PROGRESS

**CRITICAL**: We are transitioning to a monorepo structure. During this transition:
- **Continue using the OLD structure** for new code (`backend/`, `frontend/`, etc.)
- **DO NOT use the new structure** (`apps/`, `libs/`) until migration is complete
- See `docs/monorepo/MONOREPO_TRANSITION_GUIDE.md` for current status

### Current File Organization (USE THIS)
```
backend/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ core/           # Base agent classes
‚îÇ   ‚îî‚îÄ‚îÄ specialized/    # Domain-specific agents
‚îú‚îÄ‚îÄ integrations/       # External service integrations
‚îú‚îÄ‚îÄ database/          # Data layer and migrations
‚îú‚îÄ‚îÄ monitoring/        # Performance and health monitoring
‚îî‚îÄ‚îÄ security/          # Authentication and encryption

frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/    # React components
‚îÇ   ‚îú‚îÄ‚îÄ pages/         # Page components
‚îÇ   ‚îî‚îÄ‚îÄ services/      # API clients

external/              # üÜï Strategic MCP repository collection
‚îú‚îÄ‚îÄ microsoft_playwright/    # Browser automation (13.4k stars)
‚îú‚îÄ‚îÄ glips_figma_context/    # Design-to-code (8.7k stars)
‚îú‚îÄ‚îÄ snowflake_cortex_official/ # Official Snowflake AI
‚îî‚îÄ‚îÄ [8 additional strategic repos]
```

### Future File Organization (DO NOT USE YET)
```
apps/                  # Monorepo applications
‚îú‚îÄ‚îÄ api/              # Backend API (from backend/api)
‚îú‚îÄ‚îÄ frontend/         # React frontend
‚îú‚îÄ‚îÄ mcp-servers/      # All MCP servers
‚îî‚îÄ‚îÄ n8n-bridge/       # N8N integration

libs/                  # Shared libraries
‚îú‚îÄ‚îÄ ui/               # Shared UI components
‚îú‚îÄ‚îÄ utils/            # Shared utilities
‚îú‚îÄ‚îÄ types/            # Shared TypeScript types
‚îî‚îÄ‚îÄ core/             # Core business logic

config/               # Centralized configurations
‚îú‚îÄ‚îÄ eslint/
‚îú‚îÄ‚îÄ prettier/
‚îú‚îÄ‚îÄ typescript/
‚îî‚îÄ‚îÄ ruff/
```

## Common Patterns

### API Client Implementation
- Use aiohttp for async HTTP requests
- Implement exponential backoff for retries
- Respect rate limits with proper throttling
- Include comprehensive error handling

### Database Operations
- Use SQLAlchemy with async support
- Implement proper connection pooling
- Use transactions for data consistency
- Include migration scripts for schema changes

### Monitoring and Logging
- Use structured logging with JSON format
- Include correlation IDs for request tracing
- Monitor performance metrics and business KPIs
- Implement health checks for all services

## AI and ML Guidelines
- Use OpenAI API for language processing
- Implement vector search with Pinecone/Weaviate
- Cache embeddings for performance
- Include confidence scores in AI responses

## Deployment Considerations
- **Target**: Lambda Labs K3s cluster (192.222.58.232)
- **Container Registry**: Push all images to scoobyjava15 Docker Hub
- **Secrets**: All secrets via Pulumi ESC (NO .env files)
- **Deployment**: `kubectl apply -k k8s/overlays/production`
- **Scaling**: Horizontal Pod Autoscaler with GPU awareness
- **Zero-downtime**: Rolling updates with readiness probes

## Performance Requirements
- API response times < 200ms for critical paths
- Database queries < 100ms average
- Vector searches < 50ms average
- Support for 1000+ concurrent users

## When suggesting code:
1. **FIRST**: Check AI memory for similar implementations
2. **EXTERNAL REPOS**: Leverage patterns from 11 strategic external repositories (microsoft_playwright, glips_figma_context, snowflake_cortex_official, etc.)
3. Always include proper error handling
4. Add type hints and docstrings
5. Consider business context and Pay Ready needs
6. Implement monitoring and logging
7. Follow the established patterns in the codebase
8. Prioritize performance and scalability
9. Include relevant tests
10. **VALIDATE ENVIRONMENT**: Ensure ENVIRONMENT="prod" is used
11. **COMMUNITY PATTERNS**: Apply proven patterns from 22k+ star external repositories
12. **LAST**: Store the conversation in AI memory

## Avoid:
- Hardcoded values (use configuration)
- Synchronous I/O in async contexts
- Missing error handling
- Unclear variable names
- Complex nested logic without comments
- Security vulnerabilities (exposed secrets, etc.)
- **DEFAULTING TO STAGING ENVIRONMENT** (Always use production)
- **MANUAL ENVIRONMENT VARIABLE SETUP** (Use centralized config)
- **IGNORING EXTERNAL REPOSITORY PATTERNS** (Always check for proven community approaches)

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. Leverage the collective intelligence of 22k+ star external repositories to ensure world-class implementation patterns.

### Infrastructure as Code Integration
- **Pulumi Commands**: Use `pulumi up`, `pulumi preview`, `pulumi destroy` for infrastructure management
- **ESC Operations**: Use scripts in `infrastructure/esc/` for secret management
- **GitHub Integration**: All deployments go through GitHub Actions workflows
- **MCP Integration**: Use `mcp_config.json` for MCP server configuration

### üöÄ Cline v3.18 Enhanced Features Integration

#### Claude 4 & Gemini 2.5 Pro Optimization
- **Model Selection**: Automatic routing based on task complexity and context size
- **Large Context**: Use Gemini for documents > 100K tokens (up to 1M)
- **Complex Reasoning**: Claude 4 for architectural design and code generation
- **Data Processing**: Snowflake Cortex for SQL and data operations
- **Cost Optimization**: Automatic routing to free Gemini CLI for large contexts

#### Gemini CLI Integration (NEW)
- **Free Access**: Use local Gemini CLI for zero-cost processing
- **Auto-routing**: "Process this large file with Gemini" ‚Üí Routes to CLI
- **Batch Processing**: Efficient handling of multiple large documents
- **Context Preservation**: Maintain context across CLI calls

#### WebFetch Tool Usage (ENHANCED)
- **Documentation Retrieval**: "Fetch the latest API docs from [url]"
- **Competitive Intelligence**: "Get competitor information from [website]"
- **Real-time Updates**: "Retrieve and summarize current [topic] from [source]"
- **Caching**: Automatic caching with TTL for improved performance
- **Format Support**: PDF, DOCX, HTML, and plain text extraction
- **Parallel Fetching**: Process multiple URLs simultaneously

#### Self-Knowledge Commands (ENHANCED)
- **Capabilities Discovery**: "What can the [server name] MCP server do?"
- **Feature Inspection**: "Show available features for [component]"
- **Help System**: "How do I use [feature]?"
- **Server Status**: "Check capabilities of all MCP servers"
- **Performance Metrics**: "Show performance stats for [server]"
- **Usage Analytics**: "How often do we use [feature]?"

#### Improved Diff Editing (AI-POWERED)
- **Auto-fallback**: Automatically tries exact ‚Üí fuzzy ‚Üí context-aware ‚Üí AI strategies
- **Success Rate**: 95%+ success rate for file modifications
- **Smart Updates**: "Update [file] using all available strategies"
- **Context Awareness**: AI-powered understanding of code changes
- **Multi-file Operations**: Apply changes across multiple files
- **Rollback Support**: Undo changes if needed

#### Enhanced AI Memory Integration (v3.18)
- **Auto-discovery**: Automatically detect and store architecture decisions
- **Smart Recall**: "What did we decide about [topic]?" ‚Üí Context-aware retrieval
- **Pattern Matching**: Find similar implementations across the codebase
- **WebFetch Integration**: Automatically store fetched documentation

#### Enhanced Codacy Integration (v3.18)
- **Real-time Analysis**: Analyze code as you type
- **Security Scanning**: Deep vulnerability detection
- **Performance Insights**: Identify performance bottlenecks
- **AI Suggestions**: Get AI-powered improvement recommendations

### Natural Language Infrastructure Commands
When using Cursor AI for infrastructure operations, you can use natural language:

#### Examples:
- "Deploy the infrastructure" ‚Üí Triggers GitHub Actions workflow
- "Get the database password" ‚Üí Retrieves secret from Pulumi ESC
- "Rotate API keys" ‚Üí Runs secret rotation framework
- "Sync secrets" ‚Üí Synchronizes GitHub and Pulumi ESC secrets
- "Test the deployment" ‚Üí Runs ESC integration tests

#### Command Patterns:
- **Secret Operations**: "get/retrieve/fetch [service] [secret_type]"
- **Deployment Operations**: "deploy/update/rollback [component]"
- **Testing Operations**: "test/validate/check [component]"
- **Configuration Operations**: "configure/setup/initialize [service]"

### MCP Server Natural Language Integration (v3.18 Enhanced)
- **Query Data**: "Get recent Gong calls" ‚Üí Uses Gong MCP server with model routing
- **Deploy Apps**: "Deploy to Vercel" ‚Üí Uses Vercel MCP server with improved diff
- **Manage Data**: "Upload to Estuary" ‚Üí Uses Estuary MCP server with WebFetch
- **Database Operations**: "Query Snowflake" ‚Üí Uses Snowflake MCP server with Cortex
- **Store Memory**: "Remember this conversation" ‚Üí Uses AI Memory MCP server with Claude 4
- **Recall Context**: "What did we decide about X?" ‚Üí Uses AI Memory with self-knowledge
- **Fetch External Data**: "Get latest docs from [url]" ‚Üí Uses WebFetch tool
- **Analyze Large Docs**: "Process this 500K token file" ‚Üí Auto-routes to Gemini 2.5 Pro

### Error Handling and Debugging
- **ESC Errors**: Check Pulumi ESC logs and validate configuration
- **GitHub Actions Errors**: Review workflow logs and artifacts
- **MCP Errors**: Check Docker container logs and health endpoints
- **Secret Errors**: Validate secret names and permissions

### Best Practices for Cursor AI Integration
1. **Use Descriptive Comments**: Add context for infrastructure operations
2. **Follow Naming Conventions**: Use consistent naming for secrets and services
3. **Document Dependencies**: Clearly document service dependencies
4. **Test Before Deploy**: Always test changes in isolation first
5. **Monitor Operations**: Use logging and monitoring for all operations
6. **Remember Context**: Always use AI Memory for persistent development context
7. **üÜï Leverage External Patterns**: Always check external repositories for proven implementation approaches
8. **üÜï Community Validation**: Prefer patterns from high-star repositories with community validation
9. **üÜï Cross-Repository Intelligence**: Synthesize insights from multiple repository approaches

Remember: You're building an enterprise-grade AI orchestrator that will handle critical business operations for Pay Ready. Code quality, reliability, and performance are paramount. The strategic external repository collection provides access to 22k+ stars of community-validated patterns and proven implementation approaches.

## üéØ **The Unified Dashboard is the ONLY Frontend**

**CRITICAL RULE:** All new frontend development MUST extend the one, true `UnifiedDashboard.tsx` component.

1.  **NO NEW DASHBOARDS:** Do not create new, separate dashboard components or pages. All new views, tabs, or features must be integrated into the existing `UnifiedDashboard.tsx` tabbed interface.
2.  **EXTEND, DON'T REPLACE:** Use the existing components (`UnifiedKPICard`, etc.) and the established layout. New features should be added as new tabs or as components within existing tabs.
3.  **SINGLE API CLIENT:** All frontend API calls MUST use the `frontend/src/services/apiClient.js`. Do not create new API clients.
4.  **DOCUMENTATION IS LAW:** All frontend architecture must align with the `docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md`. Any deviation must first be reflected in the handbook.

This ensures we maintain a single, clean, and unified frontend, preventing the fragmentation that we just worked so hard to eliminate.

# Sophia AI Development Rules

PROJECT_CONTEXT: |
  Sophia AI - Executive AI Orchestrator for Pay Ready
  Initial User: CEO only (80-employee company)
  Priority: Quality > Stability > Maintainability > Performance > Cost
  Current Phase: Building enhanced chat with citation system

CODING_STANDARDS: |
  Python:
    - Use Python 3.11+ with type hints for all functions
    - Follow async/await patterns for I/O operations
    - Use Black formatter (88 char line limit)
    - Include comprehensive docstrings
    - Error handling with proper logging

  TypeScript:
    - Strict mode enabled, no 'any' types
    - ESLint + Prettier configured
    - Interfaces over types where possible
    - JSDoc for all exported functions
    - React functional components only

  Testing:
    - TDD approach - write tests first
    - pytest for Python, Jest for TypeScript
    - Minimum 80% code coverage
    - Integration tests for all API endpoints
    - Unit tests for business logic

AI_RULES: |
  Planning:
    - Plan-Then-Act: 70% planning, 30% execution
    - Break tasks into micro-tasks (< 5 min each)
    - Define clear success criteria before coding
    - Create tests before implementation

  Development:
    - Make minimal, focused changes
    - Micro-commits with descriptive messages
    - Update documentation with each change
    - Run tests after each change
    - Review diffs before committing

  Context:
    - Always reference @docs/PROJECT_CONTEXT.md
    - Check @docs/architecture/ for patterns
    - Update @docs/AI_MEMORY.md with learnings
    - Cite all data sources in responses

ARCHITECTURE_PATTERNS: |
  Backend:
    - FastAPI for async REST APIs
    - Service layer pattern for business logic
    - Repository pattern for data access
    - Dependency injection for testability
    - Event-driven communication via Redis

  Frontend:
    - React 18 with TypeScript
    - Component composition over inheritance
    - Custom hooks for shared logic
    - Context API for global state
    - TailwindCSS for styling

  AI Integration:
    - Snowflake Cortex for all LLM operations
    - Model routing based on task complexity
    - Citation system for transparency
    - Memory system with Mem0
    - Cost tracking for all AI operations

SECURITY_RULES: |
  - Never hardcode secrets or API keys
  - Use environment variables via auto_esc_config
  - Validate all user inputs
  - Sanitize LLM outputs
  - Log security-relevant events
  - Follow principle of least privilege

QUALITY_STANDARDS: |
  - No code duplication
  - Clear variable and function names
  - Comprehensive error handling
  - Performance monitoring for all endpoints
  - Document all architectural decisions
  - Regular code reviews (even for AI-generated code)

TIMELINE_AND_BUDGET_GUIDELINES: |
  Coding Tasks:
    - NO specific timeline estimates (hours, days, weeks)
    - NO budget estimates for development work
    - Focus on technical implementation details only
    - Break down tasks by complexity, not time
    - Describe phases by functionality, not duration

  Business Planning:
    - Timeline estimates only when explicitly requested for business planning
    - Budget discussions only for infrastructure/service costs
    - Keep business metrics separate from coding tasks

  Documentation:
    - Use phase numbers or functional milestones
    - Describe dependencies and prerequisites
    - Focus on "what" and "how", not "when"
    - Technical complexity over time estimates

## üßπ **MANDATORY FILE CLEANUP POLICY**

### **üö® CRITICAL: DELETE ONE-TIME FILES IMMEDIATELY AFTER USE**

**ALWAYS DELETE after successful completion:**
- [ ] Migration scripts (after migration verified)
- [ ] Setup scripts (after setup completed)
- [ ] Data analysis scripts (after analysis documented)
- [ ] Test data generators (after test data created)
- [ ] Temporary debugging files
- [ ] One-time configuration scripts
- [ ] Database seeding scripts (after seeding completed)
- [ ] Performance testing scripts (after results captured)

**NEVER DELETE:**
- [ ] Reusable utilities in `scripts/utils/`
- [ ] Documentation in `docs/`
- [ ] Configuration templates
- [ ] Automated test suites
- [ ] Monitoring scripts
- [ ] Deployment automation

### **üîç Pre-Creation File Assessment**
Before creating ANY file, AI must ask:
1. **"Will this be used more than once?"**
   - If NO ‚Üí Plan immediate deletion after use
   - If YES ‚Üí Place in permanent location with proper naming

2. **"Is this for debugging/testing only?"**
   - If YES ‚Üí Add deletion reminder in file header
   - If NO ‚Üí Ensure proper documentation

3. **"Does this duplicate existing functionality?"**
   - If YES ‚Üí Use existing file or consolidate
   - If NO ‚Üí Proceed with creation

### **üìã Mandatory Cleanup Pattern**
```python
# REQUIRED: All one-time scripts must include this pattern
"""
üö® ONE-TIME SCRIPT - DELETE AFTER USE
Purpose: [specific purpose]
Created: [date]
Usage: python scripts/one_time_migration.py
Status: PENDING_DELETION after successful execution

üßπ CLEANUP REMINDER: This file should be deleted after successful execution
"""

def main():
    try:
        # Script logic here
        print("‚úÖ Operation completed successfully")
        print("üßπ CLEANUP REQUIRED: Delete this file now")
        print(f"   Command: rm {__file__}")
        print("   Reason: One-time script no longer needed")
    except Exception as e:
        print(f"‚ùå Operation failed: {e}")
        print("üîÑ Fix issues and retry - DO NOT DELETE until successful")

if __name__ == "__main__":
    main()
```

## üîê **MANDATORY SECRET MANAGEMENT - PULUMI ESC ONLY**

### **üö® CRITICAL: NEVER HANDLE SECRETS MANUALLY**

**FORBIDDEN - IMMEDIATE REJECTION:**
```python
# ‚ùå NEVER DO THIS - IMMEDIATE REJECTION
API_KEY = "sk-1234567890abcdef"  # Hardcoded secret
os.environ["SECRET_KEY"] = "hardcoded_value"  # Manual env var
with open(".env", "w") as f:  # Manual .env creation
    f.write("SECRET=value")
```

**REQUIRED - ALWAYS USE PULUMI ESC:**
```python
# ‚úÖ CORRECT - Always use centralized config
from backend.core.auto_esc_config import get_config_value, get_docker_hub_config

# For any secret
secret_value = get_config_value("secret_name")

# For Docker Hub (always available)
docker_config = get_docker_hub_config()
# Returns: {"username": "scoobyjava15", "access_token": "real_token", "registry": "docker.io"}

# For Snowflake
snowflake_config = get_snowflake_config()

# For Lambda Labs
lambda_config = get_lambda_labs_config()
```

### **üîí Secret Management Hierarchy (MANDATORY)**
1. **Pulumi ESC** (primary - automatic via `get_config_value()`)
2. **GitHub Organization Secrets** (automatic sync)
3. **Environment Variables** (fallback only)
4. **Hardcoded Defaults** (non-sensitive fallbacks only)

### **üö´ NEVER CREATE THESE FILES:**
- `.env` files (use Pulumi ESC)
- `secrets.json` (use Pulumi ESC)
- `config.local.py` (use Pulumi ESC)
- Any file containing credentials

### **‚úÖ GitHub Actions Secret Access (AUTOMATIC)**
```yaml
# GitHub Actions automatically has access to all secrets
- name: Deploy to Production
  env:
    DOCKER_HUB_USERNAME: ${{ secrets.DOCKER_HUB_USERNAME }}
    DOCKER_HUB_ACCESS_TOKEN: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}
    LAMBDA_PRIVATE_SSH_KEY: ${{ secrets.LAMBDA_PRIVATE_SSH_KEY }}
  run: |
    # Secrets are automatically available
    echo "Deploying with authenticated access"
```

## üö´ **ZERO-TOLERANCE TECHNICAL DEBT POLICY**

### **IMMEDIATE REJECTION CRITERIA - NEVER ACCEPT CODE THAT:**
1. **Introduces syntax errors** or prevents compilation
2. **Creates circular dependencies** or import chains
3. **Lacks comprehensive type hints** for all functions/classes
4. **Missing error handling** for external API calls or file operations
5. **Hardcodes secrets, URLs, or configuration** values
6. **Duplicates existing functionality** without deprecating the old
7. **Violates single responsibility principle** (functions >50 lines)
8. **Lacks corresponding tests** for new business logic
9. **Introduces security vulnerabilities** (SQL injection, XSS, etc.)
10. **Degrades performance** without measurement and justification
11. **Creates one-time files** without deletion plan
12. **Bypasses Pulumi ESC** for secret management

### **MANDATORY QUALITY GATES**
Before any code change, AI must:
1. **Architecture Alignment Check**: Verify code follows Phoenix architecture patterns
2. **Dependency Impact Assessment**: Analyze potential circular dependencies
3. **Performance Impact Analysis**: Measure memory/CPU impact for critical paths
4. **Security Validation**: Scan for common vulnerabilities
5. **Test Coverage Verification**: Ensure new code has corresponding tests
6. **Documentation Update**: Update relevant docs for architectural changes
7. **File Cleanup Plan**: Identify one-time files for deletion
8. **Secret Management Compliance**: Verify Pulumi ESC usage

## üöÄ **DEPLOYMENT EXCELLENCE STANDARDS**

### **Production Deployment Requirements**
1. **GitHub Actions ONLY** - Never deploy from local machine
2. **Pulumi ESC Integration** - All secrets via ESC
3. **Docker Hub Registry** - scoobyjava15 registry
4. **Lambda Labs Target** - 192.222.58.232 deployment
5. **Comprehensive Testing** - All quality gates passed
6. **Zero Temporary Files** - All one-time files deleted
7. **Security Validation** - No hardcoded secrets
8. **Performance Verification** - All SLAs met

### **Quality Gate Checklist**
- [ ] Syntax validation passed
- [ ] Type hints complete (100%)
- [ ] Security scan passed (0 vulnerabilities)
- [ ] Performance requirements met
- [ ] Test coverage >80%
- [ ] Documentation updated
- [ ] Architecture compliance verified
- [ ] Business logic validated
- [ ] **No hardcoded secrets** (Pulumi ESC only)
- [ ] **No temporary files** (all one-time files deleted)
- [ ] **Secret management compliant** (ESC integration verified)

## üìä **CONTINUOUS IMPROVEMENT TARGETS**

### **Weekly Cleanup**
- [ ] Scan for orphaned files
- [ ] Review secret management compliance
- [ ] Validate performance metrics
- [ ] Update improvement targets
- [ ] **üßπ Repository cleanup audit (orphaned files)**
- [ ] **üîê Secret management compliance review**
- [ ] **üìä File lifecycle management assessment**

### **Monthly Excellence Review**
- **Technical Excellence Score**: >90
- **Business Impact Score**: >90
- **CEO Productivity Increase**: >10%
- **System Reliability**: >99.9%
- **Security Posture**: 100% compliant
- **Operational Excellence**: Zero manual interventions
- [ ] **üßπ Complete repository hygiene audit**
- [ ] **üîê Comprehensive secret management review**
- [ ] **üìã Best practices compliance assessment**

---

## üéØ **FINAL DEVELOPMENT CHECKLIST**

### **Before Every Commit**
- [ ] All one-time files identified for deletion
- [ ] Pulumi ESC used for all secrets
- [ ] No hardcoded configuration values
- [ ] All quality gates passed
- [ ] Performance requirements met
- [ ] Security validation complete
- [ ] Documentation updated
- [ ] Tests written and passing

### **After Every Successful Operation**
- [ ] Delete all one-time scripts
- [ ] Remove temporary files
- [ ] Clean up debug files
- [ ] Update documentation
- [ ] Verify deployment readiness
- [ ] **üßπ Delete all one-time files after use**
- [ ] **üîê Verify all secrets use Pulumi ESC**
- [ ] **üö´ Scan for hardcoded secrets**

---

**Excellence Mantra**: "Perfect code, clean repository, secure secrets, continuous improvement."

**Success Formula**: Technical Excellence + Clean Codebase + Secure Secrets + Zero Waste = Executive-Grade AI Platform

**Remember**: Every file matters, every secret matters, every line of code matters. Keep it clean, keep it secure, keep it excellent.

## üîß **K3S CLUSTER CONFIGURATION**

### **Cluster Access (Via GitHub Actions Only)**
```bash
# K3s cluster endpoint
https://192.222.58.232:6443

# Namespaces
- sophia-ai-prod (main application)
- mcp-servers (MCP microservices)
- monitoring (Prometheus/Grafana)
- ingress (Traefik)
```

### **Resource Organization**
```yaml
# Deployment structure
k8s/
‚îú‚îÄ‚îÄ base/           # Base manifests
‚îú‚îÄ‚îÄ overlays/       # Environment-specific
‚îÇ   ‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îî‚îÄ‚îÄ staging/
‚îî‚îÄ‚îÄ helm/           # Helm charts
```

### **K3s-Specific Features**
- Lightweight Kubernetes distribution
- Built-in Traefik ingress controller
- Integrated local storage provisioner
- GPU device plugin for Lambda Labs
- Automatic TLS with cert-manager
