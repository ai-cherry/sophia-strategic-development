# Sophia AI MCP Servers - Lambda Labs GPU Optimized Configuration

global:
  namespace: sophia-mcp
  imageRegistry: ghcr.io/ai-cherry
  imageTag: "latest"
  environment: production
  
  # Lambda Labs GPU Configuration
  lambdaLabs:
    enabled: true
    gpuType: "rtx-4090"
    instanceType: "gpu_1x_a10"

# Individual MCP Servers Configuration with GPU Support
mcpServers:
  # AI Memory MCP Server - Critical (GPU Enhanced)
  aiMemory:
    enabled: true
    name: sophia-mcp-ai-memory
    replicas: 2
    port: 9000
    
    resources:
      requests:
        nvidia.com/gpu: 0.25
        memory: "512Mi"
        cpu: "200m"
      limits:
        nvidia.com/gpu: 0.25
        memory: "2Gi"
        cpu: "1000m"
    
    env:
      - name: MCP_SERVER_TYPE
        value: "ai_memory"
      - name: CUDA_VISIBLE_DEVICES
        value: "all"
      - name: NVIDIA_VISIBLE_DEVICES
        value: "all"
      - name: PYTORCH_CUDA_ALLOC_CONF
        value: "max_split_size_mb:512"

  # Snowflake Admin MCP Server - Critical (GPU Enhanced)
  snowflakeAdmin:
    enabled: true
    name: sophia-mcp-snowflake-admin
    replicas: 2
    port: 8080
    
    resources:
      requests:
        nvidia.com/gpu: 0.25
        memory: "512Mi"
        cpu: "200m"
      limits:
        nvidia.com/gpu: 0.25
        memory: "2Gi"
        cpu: "1000m"
    
    env:
      - name: MCP_SERVER_TYPE
        value: "snowflake_admin"
      - name: CUDA_VISIBLE_DEVICES
        value: "all"
      - name: NVIDIA_VISIBLE_DEVICES
        value: "all"

# Lambda Labs GPU Node Configuration
nodeSelector:
  lambdalabs.com/gpu-type: "rtx-4090"
  lambdalabs.com/instance-type: "gpu_1x_a10"
  kubernetes.io/arch: amd64

tolerations:
  - key: "lambdalabs.com/gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

# Lambda Labs Storage Configuration
storage:
  storageClass: lambda-labs-ssd
  volumes:
    aiMemoryData:
      size: 10Gi
      storageClass: lambda-labs-ssd
    logs:
      size: 5Gi
      storageClass: lambda-labs-ssd

# GPU Monitoring
monitoring:
  enabled: true
  gpu:
    enabled: true
    dcgmExporter:
      enabled: true
      port: 9400

# GPU-aware Auto-scaling
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: nvidia.com/gpu
        target:
          type: Utilization
          averageUtilization: 85
