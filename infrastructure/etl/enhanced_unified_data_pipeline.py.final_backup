#!/usr/bin/env python3
"""
Pure Estuary Flow Data Pipeline for Sophia AI
Comprehensive data pipeline orchestrator using only Estuary Flow
Implements robust ELT pattern: Sources ‚Üí PostgreSQL ‚Üí Redis ‚Üí modern_stack ‚Üí Vector DBs
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 921 lines

Recommended decomposition:
- enhanced_unified_data_pipeline_core.py - Core functionality
- enhanced_unified_data_pipeline_utils.py - Utility functions
- enhanced_unified_data_pipeline_models.py - Data models
- enhanced_unified_data_pipeline_handlers.py - Request handlers

TODO: Implement file decomposition (Plan created: 2025-07-13)
"""

from backend.services.unified_memory_service_v3 import UnifiedMemoryServiceV3
import asyncio
import logging
from dataclasses import dataclass, field
from datetime import UTC, datetime
from enum import Enum
from typing import Any

import asyncpg
import redis.asyncio as redis

# REMOVED: modern_stack dependency
from core.config_manager import get_config_value
from infrastructure.etl.estuary_flow_orchestrator import EstuaryFlowOrchestrator
from backend.services.unified_memory_service_v2 import UnifiedMemoryServiceV2

logger = logging.getLogger(__name__)


class DataSource(Enum):
    """Supported data sources"""

    HUBSPOT = "hubspot"
    GONG = "gong"
    SLACK = "slack"
    SALESFORCE = "salesforce"
    ZENDESK = "zendesk"


class FlowStatus(Enum):
    """Estuary Flow status"""

    ACTIVE = "active"
    PAUSED = "paused"
    FAILED = "failed"
    INITIALIZING = "initializing"


@dataclass
class EstuaryPipelineConfig:
    """Configuration for the pure Estuary Flow pipeline"""

    sources: list[DataSource] = field(
        default_factory=lambda: [DataSource.HUBSPOT, DataSource.GONG]
    )
    postgresql_config: dict[str, Any] = field(default_factory=dict)
    redis_config: dict[str, Any] = field(default_factory=dict)
    # REMOVED: modern_stack dependencydict)
    estuary_config: dict[str, Any] = field(default_factory=dict)
    monitoring_enabled: bool = True
    auto_retry: bool = True
    max_retries: int = 3
    flow_prefix: str = "sophia_ai"


@dataclass
class PipelineStatus:
    """Status of the Estuary Flow pipeline"""

    sources_active: dict[str, FlowStatus] = field(default_factory=dict)
    destinations_active: dict[str, FlowStatus] = field(default_factory=dict)
    flows_active: dict[str, FlowStatus] = field(default_factory=dict)
    last_sync: datetime | None = None
    errors: list[str] = field(default_factory=list)
    metrics: dict[str, Any] = field(default_factory=dict)
    total_records_processed: int = 0
    data_freshness_minutes: int = 0


class PureEstuaryDataPipeline:
    """
    Pure Estuary Flow data pipeline orchestrator
    Manages data flow from multiple sources to multiple destinations using only Estuary Flow
    Provides enterprise-grade reliability, monitoring, and performance
    """

    def __init__(self, config: EstuaryPipelineConfig | None = None):
        self.config = config or EstuaryPipelineConfig()
        self.estuary_orchestrator: EstuaryFlowOrchestrator | None = None
        self.postgresql_pool: asyncpg.Pool | None = None
        self.redis_client: redis.Redis | None = None
        self.memory_service_v3: modern_stackCortexService | None = None
        self.status = PipelineStatus()

        # Initialize configurations
        self._initialize_configs()

    def _initialize_configs(self):
        """Initialize configuration from environment variables"""
        # PostgreSQL configuration
        self.config.postgresql_config = {
            "host": get_config_value("database_host", "localhost"),
            "port": get_config_value("database_port", 5432),
            "database": get_config_value("database_name", "sophia_ai"),
            "user": get_config_value("database_user", "sophia_user"),
            "password": get_config_value("database_password"),
            "min_size": 5,
            "max_size": 20,
        }

        # Redis configuration
        self.config.redis_config = {
            "host": get_config_value("redis_host", "localhost"),
            "port": get_config_value("redis_port", 6379),
            "password": get_config_value("redis_password"),
            "db": get_config_value("redis_db", 0),
        }

        # Estuary Flow configuration
        self.config.estuary_config = {
            "api_url": get_config_value(
                "estuary_flow_api_url", "https://api.estuary.dev"
            ),
            "access_token": get_config_value("estuary_access_token"),
            "tenant": get_config_value("estuary_flow_tenant", "sophia-ai"),
            "namespace": get_config_value("estuary_namespace", "sophia/ai"),
        }

# REMOVED: modern_stack dependencyuration (aligned with actual setup)
# REMOVED: modern_stack dependency.get_connection_params()

    async def __aenter__(self):
        """Async context manager entry"""
        await self._initialize_connections()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self._cleanup_connections()

    async def _initialize_connections(self):
        """Initialize all database and service connections"""
        logger.info("üîå Initializing pure Estuary Flow pipeline connections...")

        try:
            # Initialize PostgreSQL connection pool
            self.postgresql_pool = await asyncpg.create_pool(
                **self.config.postgresql_config
            )
            logger.info("‚úÖ PostgreSQL connection pool initialized")

            # Initialize Redis connection
            self.redis_client = redis.Redis(**self.config.redis_config)
            await self.redis_client.ping()
            logger.info("‚úÖ Redis connection initialized")

            # Initialize modern_stack service
            self.memory_service_v3 = UnifiedMemoryServiceV2()
            logger.info("‚úÖ modern_stack service initialized")

            # Initialize Estuary Flow orchestrator
            self.estuary_orchestrator = EstuaryFlowOrchestrator()
            await self.estuary_orchestrator.__aenter__()
            logger.info("‚úÖ Estuary Flow orchestrator initialized")

        except Exception as e:
            logger.exception(f"‚ùå Connection initialization failed: {e}")
            await self._cleanup_connections()
            raise

    async def _cleanup_connections(self):
        """Clean up all connections"""
        logger.info("üßπ Cleaning up pipeline connections...")

        if self.estuary_orchestrator:
            try:
                await self.estuary_orchestrator.__aexit__(None, None, None)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Estuary Flow cleanup error: {e}")

        if self.postgresql_pool:
            try:
                await self.postgresql_pool.close()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è PostgreSQL cleanup error: {e}")

        if self.redis_client:
            try:
                await self.redis_client.close()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Redis cleanup error: {e}")

    async def setup_complete_pipeline(self) -> dict[str, Any]:
        """
        Set up the complete data pipeline using pure Estuary Flow
        """
        logger.info(
            "üöÄ Setting up complete Sophia AI data pipeline with pure Estuary Flow..."
        )

        results = {
            "engine_used": "estuary_flow",
            "sources_configured": [],
            "destinations_configured": [],
            "flows_created": [],
            "collections_created": [],
            "errors": [],
        }

        try:
            # Set up database schemas
            await self._setup_database_schemas()
            results["destinations_configured"].append("postgresql_schemas")

            # Set up Estuary Flow collections
            collections = await self._setup_estuary_collections()
            results["collections_created"] = collections

            # Configure data sources
            source_results = await self._setup_estuary_sources()
            results.update(source_results)

            # Set up data transformations
            await self._setup_data_transformations()
            results["destinations_configured"].append("data_transformations")

            # Set up modern_stack integration
            await self._setup_modern_stack_integration()
            results["destinations_configured"].append("modern_stack")

            # Set up Redis caching
            await self._setup_redis_caching()
            results["destinations_configured"].append("redis_cache")

            # Set up monitoring and alerting
            if self.config.monitoring_enabled:
                await self._setup_monitoring()
                results["destinations_configured"].append("monitoring")

            # Start all flows
            await self._start_all_flows()

            # Update status
            self.status.last_sync = datetime.now(UTC)

            logger.info("‚úÖ Complete pure Estuary Flow pipeline setup successful!")
            return results

        except Exception as e:
            logger.exception(f"‚ùå Pipeline setup failed: {e}")
            results["errors"].append(str(e))
            raise

    async def _setup_estuary_collections(self) -> list[str]:
        """Set up Estuary Flow collections for all data sources"""
        logger.info("üìä Setting up Estuary Flow collections...")

        collections = []

        # Define collection schemas for each data source
        collection_configs = {
            "hubspot_contacts": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "email": {"type": "string"},
                        "firstname": {"type": "string"},
                        "lastname": {"type": "string"},
                        "company": {"type": "string"},
                        "phone": {"type": "string"},
                        "created_at": {"type": "string", "format": "date-time"},
                        "updated_at": {"type": "string", "format": "date-time"},
                        "properties": {"type": "object"},
                    },
                    "required": ["id", "email"],
                },
                "key": ["/id"],
            },
            "hubspot_deals": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "dealname": {"type": "string"},
                        "amount": {"type": "number"},
                        "dealstage": {"type": "string"},
                        "pipeline": {"type": "string"},
                        "closedate": {"type": "string", "format": "date-time"},
                        "created_at": {"type": "string", "format": "date-time"},
                        "updated_at": {"type": "string", "format": "date-time"},
                        "properties": {"type": "object"},
                    },
                    "required": ["id", "dealname"],
                },
                "key": ["/id"],
            },
            "gong_calls": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "title": {"type": "string"},
                        "url": {"type": "string"},
                        "started": {"type": "string", "format": "date-time"},
                        "duration": {"type": "integer"},
                        "participants": {"type": "array"},
                        "transcript": {"type": "string"},
                        "summary": {"type": "string"},
                        "sentiment": {"type": "string"},
                        "topics": {"type": "array"},
                        "created_at": {"type": "string", "format": "date-time"},
                    },
                    "required": ["id", "title"],
                },
                "key": ["/id"],
            },
            "slack_messages": {
                "schema": {
                    "type": "object",
                    "properties": {
                        "ts": {"type": "string"},
                        "channel": {"type": "string"},
                        "user": {"type": "string"},
                        "text": {"type": "string"},
                        "thread_ts": {"type": "string"},
                        "reply_count": {"type": "integer"},
                        "created_at": {"type": "string", "format": "date-time"},
                    },
                    "required": ["ts", "channel", "user"],
                },
                "key": ["/ts", "/channel"],
            },
        }

        # Create collections in Estuary Flow
        for collection_name, config in collection_configs.items():
            try:
                full_collection_name = f"{self.config.flow_prefix}/{collection_name}"

                collection_spec = {
                    "schema": config["schema"],
                    "key": config["key"],
                    "projections": {
                        "id": {"location": "/id", "partition": True},
                        "created_at": {"location": "/created_at"},
                    },
                }

                await self.estuary_orchestrator.create_collection(
                    collection_name=full_collection_name, schema=collection_spec
                )

                collections.append(full_collection_name)
                logger.info(f"‚úÖ Created Estuary collection: {full_collection_name}")

            except Exception as e:
                logger.exception(
                    f"‚ùå Failed to create collection {collection_name}: {e}"
                )
                self.status.errors.append(
                    f"Collection creation failed: {collection_name} - {e}"
                )

        return collections

    async def _setup_estuary_sources(self) -> dict[str, Any]:
        """Set up Estuary Flow sources for all configured data sources"""
        logger.info("üîå Setting up Estuary Flow sources...")

        results = {"sources_configured": [], "flows_created": []}

        # HubSpot source configuration
        if DataSource.HUBSPOT in self.config.sources:
            try:
                hubspot_config = {
                    "start_date": "2024-01-01T00:00:00Z",
                    "credentials": {
                        "credentials_title": "Private App Credentials",
                        "access_token": get_config_value("hubspot_access_token"),
                    },
                }

                # Create HubSpot captures
                hubspot_captures = [
                    {
                        "name": f"{self.config.flow_prefix}/hubspot_contacts_capture",
                        "endpoint": {
                            "connector": {
                                "image": "ghcr.io/estuary/source-hubspot:dev",
                                "config": hubspot_config,
                            }
                        },
                        "bindings": [
                            {
                                "resource": {
                                    "stream": "contacts",
                                    "syncMode": "incremental",
                                },
                                "target": f"{self.config.flow_prefix}/hubspot_contacts",
                            }
                        ],
                    },
                    {
                        "name": f"{self.config.flow_prefix}/hubspot_deals_capture",
                        "endpoint": {
                            "connector": {
                                "image": "ghcr.io/estuary/source-hubspot:dev",
                                "config": hubspot_config,
                            }
                        },
                        "bindings": [
                            {
                                "resource": {
                                    "stream": "deals",
                                    "syncMode": "incremental",
                                },
                                "target": f"{self.config.flow_prefix}/hubspot_deals",
                            }
                        ],
                    },
                ]

                for capture in hubspot_captures:
                    await self.estuary_orchestrator.create_capture(capture)
                    results["flows_created"].append(capture["name"])

                results["sources_configured"].append("hubspot")
                self.status.sources_active["hubspot"] = FlowStatus.ACTIVE

            except Exception as e:
                logger.exception(f"‚ùå HubSpot source setup failed: {e}")
                self.status.errors.append(f"HubSpot setup failed: {e}")
                self.status.sources_active["hubspot"] = FlowStatus.FAILED

        # Gong source configuration
        if DataSource.GONG in self.config.sources:
            try:
                gong_config = {
                    "access_key": get_config_value("gong_access_key"),
                    "access_key_secret": get_config_value("gong_access_key_secret"),
                    "start_date": "2024-01-01T00:00:00Z",
                }

                gong_capture = {
                    "name": f"{self.config.flow_prefix}/gong_calls_capture",
                    "endpoint": {
                        "connector": {
                            "image": "ghcr.io/estuary/source-gong:dev",
                            "config": gong_config,
                        }
                    },
                    "bindings": [
                        {
                            "resource": {"stream": "calls", "syncMode": "incremental"},
                            "target": f"{self.config.flow_prefix}/gong_calls",
                        }
                    ],
                }

                await self.estuary_orchestrator.create_capture(gong_capture)
                results["flows_created"].append(gong_capture["name"])
                results["sources_configured"].append("gong")
                self.status.sources_active["gong"] = FlowStatus.ACTIVE

            except Exception as e:
                logger.exception(f"‚ùå Gong source setup failed: {e}")
                self.status.errors.append(f"Gong setup failed: {e}")
                self.status.sources_active["gong"] = FlowStatus.FAILED

        # Slack source configuration
        if DataSource.SLACK in self.config.sources:
            try:
                slack_config = {
                    "api_token": get_config_value("slack_bot_token"),
                    "start_date": "2024-01-01T00:00:00Z",
                    "lookback_window": 7,
                    "join_channels": True,
                }

                slack_capture = {
                    "name": f"{self.config.flow_prefix}/slack_messages_capture",
                    "endpoint": {
                        "connector": {
                            "image": "ghcr.io/estuary/source-slack:dev",
                            "config": slack_config,
                        }
                    },
                    "bindings": [
                        {
                            "resource": {
                                "stream": "messages",
                                "syncMode": "incremental",
                            },
                            "target": f"{self.config.flow_prefix}/slack_messages",
                        }
                    ],
                }

                await self.estuary_orchestrator.create_capture(slack_capture)
                results["flows_created"].append(slack_capture["name"])
                results["sources_configured"].append("slack")
                self.status.sources_active["slack"] = FlowStatus.ACTIVE

            except Exception as e:
                logger.exception(f"‚ùå Slack source setup failed: {e}")
                self.status.errors.append(f"Slack setup failed: {e}")
                self.status.sources_active["slack"] = FlowStatus.FAILED

        return results

    async def _setup_database_schemas(self):
        """Set up PostgreSQL database schemas for staging data"""
        logger.info("üóÑÔ∏è Setting up PostgreSQL database schemas...")

        schema_sql = """
        -- Create schemas for different data sources
        CREATE SCHEMA IF NOT EXISTS estuary_raw;
        CREATE SCHEMA IF NOT EXISTS estuary_staging;
        CREATE SCHEMA IF NOT EXISTS estuary_processed;

        -- HubSpot tables
        CREATE TABLE IF NOT EXISTS estuary_raw.hubspot_contacts (
            id VARCHAR PRIMARY KEY,
            email VARCHAR,
            firstname VARCHAR,
            lastname VARCHAR,
            company VARCHAR,
            phone VARCHAR,
            created_at TIMESTAMP,
            updated_at TIMESTAMP,
            properties JSONB,
            _estuary_flow_document JSONB,
            _estuary_flow_published_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS estuary_raw.hubspot_deals (
            id VARCHAR PRIMARY KEY,
            dealname VARCHAR,
            amount DECIMAL,
            dealstage VARCHAR,
            pipeline VARCHAR,
            closedate TIMESTAMP,
            created_at TIMESTAMP,
            updated_at TIMESTAMP,
            properties JSONB,
            _estuary_flow_document JSONB,
            _estuary_flow_published_at TIMESTAMP DEFAULT NOW()
        );

        -- Gong tables
        CREATE TABLE IF NOT EXISTS estuary_raw.gong_calls (
            id VARCHAR PRIMARY KEY,
            title VARCHAR,
            url VARCHAR,
            started TIMESTAMP,
            duration INTEGER,
            participants JSONB,
            transcript TEXT,
            summary TEXT,
            sentiment VARCHAR,
            topics JSONB,
            created_at TIMESTAMP,
            _estuary_flow_document JSONB,
            _estuary_flow_published_at TIMESTAMP DEFAULT NOW()
        );

        -- Slack tables
        CREATE TABLE IF NOT EXISTS estuary_raw.slack_messages (
            ts VARCHAR,
            channel VARCHAR,
            user_id VARCHAR,
            text TEXT,
            thread_ts VARCHAR,
            reply_count INTEGER,
            created_at TIMESTAMP,
            _estuary_flow_document JSONB,
            _estuary_flow_published_at TIMESTAMP DEFAULT NOW(),
            PRIMARY KEY (ts, channel)
        );

        -- Create indexes for performance
        CREATE INDEX IF NOT EXISTS idx_hubspot_contacts_email ON estuary_raw.hubspot_contacts(email);
        CREATE INDEX IF NOT EXISTS idx_hubspot_contacts_updated ON estuary_raw.hubspot_contacts(updated_at);
        CREATE INDEX IF NOT EXISTS idx_hubspot_deals_stage ON estuary_raw.hubspot_deals(dealstage);
        CREATE INDEX IF NOT EXISTS idx_hubspot_deals_updated ON estuary_raw.hubspot_deals(updated_at);
        CREATE INDEX IF NOT EXISTS idx_gong_calls_started ON estuary_raw.gong_calls(started);
        CREATE INDEX IF NOT EXISTS idx_slack_messages_channel ON estuary_raw.slack_messages(channel);
        CREATE INDEX IF NOT EXISTS idx_slack_messages_created ON estuary_raw.slack_messages(created_at);
        """

        async with self.postgresql_pool.acquire() as conn:
            await conn.execute(schema_sql)

        logger.info("‚úÖ PostgreSQL schemas created successfully")

    async def _setup_data_transformations(self):
        """Set up Estuary Flow derivations for data transformations"""
        logger.info("üîÑ Setting up Estuary Flow data transformations...")

        # Create unified contact derivation
        unified_contacts_derivation = {
            "name": f"{self.config.flow_prefix}/unified_contacts",
            "endpoint": {
                "connector": {
                    "image": "ghcr.io/estuary/materialize-sql:dev",
                    "config": {
                        "driver": "postgres",
                        "host": self.config.postgresql_config["host"],
                        "port": self.config.postgresql_config["port"],
                        "database": self.config.postgresql_config["database"],
                        "user": self.config.postgresql_config["user"],
                        "password": self.config.postgresql_config["password"],
                    },
                }
            },
            "bindings": [
                {
                    "source": f"{self.config.flow_prefix}/hubspot_contacts",
                    "target": "estuary_staging.unified_contacts",
                }
            ],
            "transforms": {
                "unify_contacts": {
                    "source": f"{self.config.flow_prefix}/hubspot_contacts",
                    "shuffle": "any",
                    "lambda": """
                    SELECT
                        id as contact_id,
                        'hubspot' as source_system,
                        email,
                        firstname,
                        lastname,
                        company,
                        phone,
                        created_at,
                        updated_at,
                        properties,
                        NOW() as processed_at
                    FROM source
                    """,
                }
            },
        }

        await self.estuary_orchestrator.create_derivation(unified_contacts_derivation)

        # Create deal intelligence derivation
        deal_intelligence_derivation = {
            "name": f"{self.config.flow_prefix}/deal_intelligence",
            "endpoint": {
                "connector": {
                    "image": "ghcr.io/estuary/materialize-sql:dev",
                    "config": {
                        "driver": "postgres",
                        "host": self.config.postgresql_config["host"],
                        "port": self.config.postgresql_config["port"],
                        "database": self.config.postgresql_config["database"],
                        "user": self.config.postgresql_config["user"],
                        "password": self.config.postgresql_config["password"],
                    },
                }
            },
            "bindings": [
                {
                    "source": f"{self.config.flow_prefix}/hubspot_deals",
                    "target": "estuary_staging.deal_intelligence",
                }
            ],
            "transforms": {
                "enrich_deals": {
                    "source": f"{self.config.flow_prefix}/hubspot_deals",
                    "shuffle": "any",
                    "lambda": """
                    SELECT
                        id as deal_id,
                        dealname,
                        amount,
                        dealstage,
                        pipeline,
                        closedate,
                        CASE
                            WHEN amount > 100000 THEN 'high_value'
                            WHEN amount > 50000 THEN 'medium_value'
                            ELSE 'low_value'
                        END as deal_tier,
                        CASE
                            WHEN dealstage IN ('closedwon', 'closed-won') THEN 'won'
                            WHEN dealstage IN ('closedlost', 'closed-lost') THEN 'lost'
                            ELSE 'active'
                        END as deal_status,
                        created_at,
                        updated_at,
                        properties,
                        NOW() as processed_at
                    FROM source
                    """,
                }
            },
        }

        await self.estuary_orchestrator.create_derivation(deal_intelligence_derivation)

        logger.info("‚úÖ Data transformations configured successfully")

    async def _setup_modern_stack_integration(self):
        """Set up Estuary Flow materialization to modern_stack"""
        logger.info("‚ùÑÔ∏è Setting up modern_stack integration...")

# REMOVED: modern_stack dependencyuration
        # REMOVED: modern_stack dependency (
# REMOVED: modern_stack dependency()
        )
        modern_stack_materialization[
            "name"
        ] = f"{self.config.flow_prefix}/modern_stack_materialization"

        await self.estuary_orchestrator.create_materialization(
            modern_stack_materialization
        )

        self.status.destinations_active["modern_stack"] = FlowStatus.ACTIVE
# REMOVED: modern_stack dependencyured successfully")

    async def _setup_redis_caching(self):
        """Set up Redis caching for real-time data access"""
        logger.info("üîÑ Setting up Redis caching...")

        # Set up Redis streams for real-time data
        await self.redis_client.xgroup_create(
            "contacts_stream", "sophia_ai", id="0", mkstream=True
        )
        await self.redis_client.xgroup_create(
            "deals_stream", "sophia_ai", id="0", mkstream=True
        )
        await self.redis_client.xgroup_create(
            "calls_stream", "sophia_ai", id="0", mkstream=True
        )

        self.status.destinations_active["redis"] = FlowStatus.ACTIVE
        logger.info("‚úÖ Redis caching configured successfully")

    async def _setup_monitoring(self):
        """Set up monitoring and alerting for the pipeline"""
        logger.info("üìä Setting up pipeline monitoring...")

        # Create monitoring collection
        monitoring_collection = {
            "schema": {
                "type": "object",
                "properties": {
                    "timestamp": {"type": "string", "format": "date-time"},
                    "flow_name": {"type": "string"},
                    "status": {"type": "string"},
                    "records_processed": {"type": "integer"},
                    "errors": {"type": "array"},
                    "metrics": {"type": "object"},
                },
                "required": ["timestamp", "flow_name", "status"],
            },
            "key": ["/timestamp", "/flow_name"],
        }

        await self.estuary_orchestrator.create_collection(
            collection_name=f"{self.config.flow_prefix}/monitoring",
            schema=monitoring_collection,
        )

        logger.info("‚úÖ Monitoring configured successfully")

    async def _start_all_flows(self):
        """Start all Estuary Flow captures, derivations, and materializations"""
        logger.info("‚ñ∂Ô∏è Starting all Estuary Flow flows...")

        # Enable all flows
        flows_to_enable = [
            f"{self.config.flow_prefix}/hubspot_contacts_capture",
            f"{self.config.flow_prefix}/hubspot_deals_capture",
            f"{self.config.flow_prefix}/gong_calls_capture",
            f"{self.config.flow_prefix}/slack_messages_capture",
            f"{self.config.flow_prefix}/unified_contacts",
            f"{self.config.flow_prefix}/deal_intelligence",
            f"{self.config.flow_prefix}/modern_stack_materialization",
        ]

        for flow_name in flows_to_enable:
            try:
                await self.estuary_orchestrator.enable_flow(flow_name)
                self.status.flows_active[flow_name] = FlowStatus.ACTIVE
                logger.info(f"‚úÖ Started flow: {flow_name}")
            except Exception as e:
                logger.exception(f"‚ùå Failed to start flow {flow_name}: {e}")
                self.status.flows_active[flow_name] = FlowStatus.FAILED
                self.status.errors.append(f"Flow start failed: {flow_name} - {e}")

        logger.info("‚úÖ All flows started successfully")

    async def get_pipeline_status(self) -> PipelineStatus:
        """Get current pipeline status and metrics"""
        logger.info("üìä Getting pipeline status...")

        try:
            # Update metrics from Estuary Flow
            if self.estuary_orchestrator:
                flow_stats = await self.estuary_orchestrator.get_flow_stats()

                # Calculate total records processed
                total_records = sum(
                    stats.get("records_processed", 0) for stats in flow_stats.values()
                )
                self.status.total_records_processed = total_records

                # Calculate data freshness
                latest_sync = max(
                    (
                        stats.get("last_sync")
                        for stats in flow_stats.values()
                        if stats.get("last_sync")
                    ),
                    default=None,
                )

                if latest_sync:
                    freshness = (datetime.now(UTC) - latest_sync).total_seconds() / 60
                    self.status.data_freshness_minutes = int(freshness)

                self.status.metrics = {
                    "flow_stats": flow_stats,
                    "total_flows": len(self.status.flows_active),
                    "active_flows": len(
                        [
                            s
                            for s in self.status.flows_active.values()
                            if s == FlowStatus.ACTIVE
                        ]
                    ),
                    "failed_flows": len(
                        [
                            s
                            for s in self.status.flows_active.values()
                            if s == FlowStatus.FAILED
                        ]
                    ),
                }

            return self.status

        except Exception as e:
            logger.exception(f"‚ùå Failed to get pipeline status: {e}")
            self.status.errors.append(f"Status check failed: {e}")
            return self.status

    async def pause_pipeline(self):
        """Pause all pipeline flows"""
        logger.info("‚è∏Ô∏è Pausing pipeline...")

        for flow_name in self.status.flows_active:
            try:
                await self.estuary_orchestrator.disable_flow(flow_name)
                self.status.flows_active[flow_name] = FlowStatus.PAUSED
            except Exception as e:
                logger.exception(f"‚ùå Failed to pause flow {flow_name}: {e}")

    async def resume_pipeline(self):
        """Resume all pipeline flows"""
        logger.info("‚ñ∂Ô∏è Resuming pipeline...")

        for flow_name in self.status.flows_active:
            try:
                await self.estuary_orchestrator.enable_flow(flow_name)
                self.status.flows_active[flow_name] = FlowStatus.ACTIVE
            except Exception as e:
                logger.exception(f"‚ùå Failed to resume flow {flow_name}: {e}")


# Convenience functions for easy pipeline management
async def setup_sophia_data_pipeline(
    config: EstuaryPipelineConfig | None = None,
) -> dict[str, Any]:
    """Set up the complete Sophia AI data pipeline"""
    async with PureEstuaryDataPipeline(config) as pipeline:
        return await pipeline.setup_complete_pipeline()


async def get_sophia_pipeline_status(
    config: EstuaryPipelineConfig | None = None,
) -> PipelineStatus:
    """Get the current status of the Sophia AI data pipeline"""
    async with PureEstuaryDataPipeline(config) as pipeline:
        return await pipeline.get_pipeline_status()


async def pause_sophia_pipeline(config: EstuaryPipelineConfig | None = None):
    """Pause the Sophia AI data pipeline"""
    async with PureEstuaryDataPipeline(config) as pipeline:
        await pipeline.pause_pipeline()


async def resume_sophia_pipeline(config: EstuaryPipelineConfig | None = None):
    """Resume the Sophia AI data pipeline"""
    async with PureEstuaryDataPipeline(config) as pipeline:
        await pipeline.resume_pipeline()


if __name__ == "__main__":
    # Example usage
    async def main():
        logger.info("üöÄ Starting Sophia AI Pure Estuary Flow Data Pipeline...")

        config = EstuaryPipelineConfig(
            sources=[DataSource.HUBSPOT, DataSource.GONG, DataSource.SLACK],
            monitoring_enabled=True,
        )

        try:
            result = await setup_sophia_data_pipeline(config)
            logger.info(f"‚úÖ Pipeline setup completed: {result}")

            # Get status
            status = await get_sophia_pipeline_status(config)
            logger.info(f"üìä Pipeline status: {status}")

        except Exception as e:
            logger.exception(f"‚ùå Pipeline setup failed: {e}")
            raise

    asyncio.run(main())
