# SQL injection fixes applied - using parameterized queries
"""
Gong Data Ingestion Script for Snowflake

This script fetches call data from the Gong API and loads it into Snowflake's
GONG_CALLS_RAW VARIANT table for further processing by Snowflake Cortex AI.

Features:
- Incremental data loading with state management
- Error handling with retry logic and dead letter queue
- Structured logging with correlation IDs
- Integration with Pulumi ESC for secret management
- Support for both full and incremental sync modes
"""
from core.optimized_database_manager import (
    OptimizedDatabaseManager,
    BatchOperation,
    ConnectionType
)


"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 789 lines

Recommended decomposition:
- ingest_gong_data_core.py - Core functionality
- ingest_gong_data_utils.py - Utility functions
- ingest_gong_data_models.py - Data models
- ingest_gong_data_handlers.py - Request handlers

TODO: Implement file decomposition (Plan created: 2025-07-13)
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
import uuid
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any

import aiohttp
import snowflake.connector
import structlog
from pydantic import BaseModel, Field

from core.config_manager import get_config_value as config

logger = structlog.get_logger()


class SyncMode(Enum):
    def _validate_schema_name(self, schema_name: str) -> str:
        """Validate schema name for security"""
        # Allow only alphanumeric, dots, and underscores
        if not re.match(r"^[a-zA-Z0-9_.]+$", schema_name):
            raise ValueError(f"Invalid schema name: {schema_name}")
        return schema_name

    """Sync modes for Gong data ingestion"""

    FULL = "full"
    INCREMENTAL = "incremental"
    BACKFILL = "backfill"


@dataclass
class IngestionState:
    """State tracking for incremental ingestion"""

    last_sync_timestamp: datetime
    last_call_id: str | None = None
    total_calls_processed: int = 0
    sync_mode: SyncMode = SyncMode.INCREMENTAL
    correlation_id: str = Field(default_factory=lambda: str(uuid.uuid4()))


class GongCall(BaseModel):
    """Gong call data model"""

    call_id: str
    title: str
    started: datetime
    duration: int  # seconds
    primary_user_id: str
    direction: str  # "Inbound" or "Outbound"
    system: str
    scope: str
    media: str  # "Video", "Audio", etc.
    language: str
    workspace_id: str
    call_url: str
    participants: list[dict[str, Any]] = Field(default_factory=list)
    custom_data: dict[str, Any] = Field(default_factory=dict)
    crm_data: dict[str, Any] = Field(default_factory=dict)


class GongAPIClient:
    """Async client for Gong API operations"""

    def __init__(self):
        self.base_url = "https://api.gong.io/v2"
        self.session: aiohttp.ClientSession | None = None
        self.access_key = config.get("gong_access_key")
        self.access_key_secret = config.get("gong_access_key_secret")
        self.rate_limit_delay = 1.0  # seconds between requests

    async def __aenter__(self):
        """Async context manager entry"""
        if not self.access_key or not self.access_key_secret:
            raise ValueError("Gong API credentials not found in configuration")

        auth = aiohttp.BasicAuth(self.access_key, self.access_key_secret)
        timeout = aiohttp.ClientTimeout(total=300)  # 5 minute timeout

        self.session = aiohttp.ClientSession(
            auth=auth,
            timeout=timeout,
            headers={
                "Content-Type": "application/json",
                "User-Agent": "Sophia-AI-Gong-Ingestion/1.0",
            },
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def get_calls(
        self,
        from_date: datetime,
        to_date: datetime,
        cursor: str | None = None,
        limit: int = 100,
    ) -> dict[str, Any]:
        """
        Fetch calls from Gong API with pagination support

        Args:
            from_date: Start date for call retrieval
            to_date: End date for call retrieval
            cursor: Pagination cursor for subsequent requests
            limit: Number of calls per request (max 100)

        Returns:
            API response with calls data and pagination info
        """
        endpoint = f"{self.base_url}/calls"

        params = {
            "fromDateTime": from_date.isoformat(),
            "toDateTime": to_date.isoformat(),
            "limit": min(limit, 100),  # Gong API limit
        }

        if cursor:
            params["cursor"] = cursor

        try:
            await asyncio.sleep(self.rate_limit_delay)  # Rate limiting

            async with self.session.get(endpoint, params=params) as response:
                if response.status == 429:  # Rate limited
                    retry_after = int(response.headers.get("Retry-After", 60))
                    logger.warning(f"Rate limited, waiting {retry_after} seconds")
                    await asyncio.sleep(retry_after)
                    return await self.get_calls(from_date, to_date, cursor, limit)

                response.raise_for_status()
                data = await response.json()

                logger.debug(
                    "Fetched calls from Gong API",
                    calls_count=len(data.get("calls", [])),
                    cursor=cursor,
                    has_more=data.get("records", {}).get("totalRecords", 0) > 0,
                )

                return data

        except aiohttp.ClientError as e:
            logger.error(f"Gong API request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in Gong API call: {e}")
            raise

    async def get_call_transcript(self, call_id: str) -> dict[str, Any]:
        """
        Fetch transcript for a specific call

        Args:
            call_id: Gong call ID

        Returns:
            Transcript data with speaker segments
        """
        endpoint = f"{self.base_url}/calls/{call_id}/transcript"

        try:
            await asyncio.sleep(self.rate_limit_delay)

            async with self.session.get(endpoint) as response:
                if response.status == 404:
                    logger.warning(f"Transcript not found for call {call_id}")
                    return {"call_id": call_id, "transcript": None}

                response.raise_for_status()
                data = await response.json()

                logger.debug(f"Fetched transcript for call {call_id}")
                return data

        except aiohttp.ClientError as e:
            logger.error(f"Failed to fetch transcript for call {call_id}: {e}")
            return {"call_id": call_id, "transcript": None, "error": str(e)}


class SnowflakeGongLoader:
    """Loads Gong data into Snowflake tables"""

    def __init__(self):
        self.connection = None
        self.database = config.get("snowflake_database", "SOPHIA_AI")
        self.schema = config.get("snowflake_schema", "GONG_DATA")
        self.warehouse = config.get("snowflake_warehouse", "SOPHIA_AI_WH")

    async def initialize(self) -> None:
        """Initialize Snowflake connection"""
        try:
            self.connection = snowflake.connector.connect(
                user=config.get("snowflake_user"),
                password=config.get("snowflake_password"),
                account=config.get("snowflake_account"),
                warehouse=self.warehouse,
                database=self.database,
                schema=self.schema,
                role=config.get("snowflake_role", "ACCOUNTADMIN"),
            )

            # Ensure schema exists
            await self._ensure_schema_exists()

            logger.info("✅ Snowflake Gong loader initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize Snowflake connection: {e}")
            raise

    async def _ensure_schema_exists(self):
        """Ensure the Gong data schema exists"""
        cursor = self.connection.cursor()
        try:
            cursor.execute("CREATE SCHEMA IF NOT EXISTS %s", (self._validate_schema_name(f"{self.database}.{self.schema}",)))
            )
            cursor.execute("USE SCHEMA %s", (self._validate_schema_name(f"{self.database}.{self.schema}",)))
            )
            logger.debug(f"Ensured schema {self.database}.{self.schema} exists")
        except Exception as e:
            logger.error(f"Failed to ensure schema exists: {e}")
            raise
        finally:
            cursor.close()

    async def load_raw_calls(self, calls_data: list[dict[str, Any]]) -> int:
        """
        Load raw call data into GONG_CALLS_RAW table

        Args:
            calls_data: List of raw call data from Gong API

        Returns:
            Number of records loaded
        """
        if not calls_data:
            return 0

        # Create table if not exists
        await self._ensure_raw_calls_table()

        cursor = self.connection.cursor()
        try:
            # Begin transaction for atomicity
            cursor.execute("BEGIN TRANSACTION")

            insert_query = """
            INSERT INTO GONG_CALLS_RAW (
                CALL_ID,
                RAW_DATA,
                INGESTED_AT,
                CORRELATION_ID
            ) VALUES (?, ?, ?, ?)
            """

            # Prepare data for insertion
            correlation_id = str(uuid.uuid4())
            insert_data = []

            for call in calls_data:
                insert_data.append(
                    (call.get("id"), json.dumps(call), datetime.now(), correlation_id)
                )

            # Batch insert
            cursor.executemany(insert_query, insert_data)

            # Commit transaction
            cursor.execute("COMMIT")

            logger.info(
                f"Loaded {len(insert_data)} raw calls into Snowflake",
                correlation_id=correlation_id,
            )

            return len(insert_data)

        except Exception as e:
            # Rollback transaction on error
            try:
                cursor.execute("ROLLBACK")
            except Exception as rollback_error:
                logger.error(f"Failed to rollback transaction: {rollback_error}")

            logger.error(f"Failed to load raw calls: {e}")
            raise
        finally:
            cursor.close()

    async def load_call_transcripts(
        self, transcripts_data: list[dict[str, Any]]
    ) -> int:
        """
        Load call transcripts into GONG_CALL_TRANSCRIPTS_RAW table

        Args:
            transcripts_data: List of transcript data from Gong API

        Returns:
            Number of records loaded
        """
        if not transcripts_data:
            return 0

        await self._ensure_transcripts_table()

        cursor = self.connection.cursor()
        try:
            # Begin transaction for atomicity
            cursor.execute("BEGIN TRANSACTION")

            insert_query = """
            INSERT INTO GONG_CALL_TRANSCRIPTS_RAW (
                CALL_ID,
                TRANSCRIPT_DATA,
                INGESTED_AT,
                CORRELATION_ID
            ) VALUES (?, ?, ?, ?)
            """

            correlation_id = str(uuid.uuid4())
            insert_data = []

            for transcript in transcripts_data:
                if transcript.get("transcript"):  # Only insert if transcript exists
                    insert_data.append(
                        (
                            transcript.get("call_id"),
                            json.dumps(transcript),
                            datetime.now(),
                            correlation_id,
                        )
                    )

            if insert_data:
                cursor.executemany(insert_query, insert_data)

                # Commit transaction
                cursor.execute("COMMIT")

                logger.info(
                    f"Loaded {len(insert_data)} transcripts into Snowflake",
                    correlation_id=correlation_id,
                )
            else:
                # Commit empty transaction
                cursor.execute("COMMIT")

            return len(insert_data)

        except Exception as e:
            # Rollback transaction on error
            try:
                cursor.execute("ROLLBACK")
            except Exception as rollback_error:
                logger.error(f"Failed to rollback transaction: {rollback_error}")

            logger.error(f"Failed to load transcripts: {e}")
            raise
        finally:
            cursor.close()

    async def _ensure_raw_calls_table(self):
        """Ensure GONG_CALLS_RAW table exists"""
        cursor = self.connection.cursor()
        try:
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS GONG_CALLS_RAW (
                CALL_ID VARCHAR(255) PRIMARY KEY,
                RAW_DATA VARIANT,
                INGESTED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
                CORRELATION_ID VARCHAR(255),
                PROCESSED BOOLEAN DEFAULT FALSE,
                PROCESSED_AT TIMESTAMP_LTZ
            )
            """
            cursor.execute(create_table_sql)
            logger.debug("Ensured GONG_CALLS_RAW table exists")
        except Exception as e:
            logger.error(f"Failed to create GONG_CALLS_RAW table: {e}")
            raise
        finally:
            cursor.close()

    async def _ensure_transcripts_table(self):
        """Ensure GONG_CALL_TRANSCRIPTS_RAW table exists"""
        cursor = self.connection.cursor()
        try:
            create_table_sql = """
            CREATE TABLE IF NOT EXISTS GONG_CALL_TRANSCRIPTS_RAW (
                CALL_ID VARCHAR(255) PRIMARY KEY,
                TRANSCRIPT_DATA VARIANT,
                INGESTED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP(),
                CORRELATION_ID VARCHAR(255),
                PROCESSED BOOLEAN DEFAULT FALSE,
                PROCESSED_AT TIMESTAMP_LTZ
            )
            """
            cursor.execute(create_table_sql)
            logger.debug("Ensured GONG_CALL_TRANSCRIPTS_RAW table exists")
        except Exception as e:
            logger.error(f"Failed to create GONG_CALL_TRANSCRIPTS_RAW table: {e}")
            raise
        finally:
            cursor.close()

    async def get_last_sync_state(self) -> IngestionState | None:
        """Get the last sync state from Snowflake"""
        cursor = self.connection.cursor()
        try:
            # Create state table if not exists
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS GONG_INGESTION_STATE (
                    ID NUMBER IDENTITY PRIMARY KEY,
                    LAST_SYNC_TIMESTAMP TIMESTAMP_LTZ,
                    LAST_CALL_ID VARCHAR(255),
                    TOTAL_CALLS_PROCESSED NUMBER,
                    SYNC_MODE VARCHAR(50),
                    CORRELATION_ID VARCHAR(255),
                    CREATED_AT TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()
                )
            """
            )

            # Get latest state
            cursor.execute(
                """
                SELECT
                    LAST_SYNC_TIMESTAMP,
                    LAST_CALL_ID,
                    TOTAL_CALLS_PROCESSED,
                    SYNC_MODE,
                    CORRELATION_ID
                FROM GONG_INGESTION_STATE
                ORDER BY ID DESC
                LIMIT 1
            """
            )

            result = cursor.fetchone()
            if result:
                return IngestionState(
                    last_sync_timestamp=result[0],
                    last_call_id=result[1],
                    total_calls_processed=result[2] or 0,
                    sync_mode=(
                        SyncMode(result[3]) if result[3] else SyncMode.INCREMENTAL
                    ),
                    correlation_id=result[4] or str(uuid.uuid4()),
                )

            # No previous state, return default
            return IngestionState(
                last_sync_timestamp=datetime.now()
                - timedelta(days=7),  # Start with last week
                sync_mode=SyncMode.INCREMENTAL,
                correlation_id=str(uuid.uuid4()),
            )

        except Exception as e:
            logger.error(f"Failed to get sync state: {e}")
            raise
        finally:
            cursor.close()

    async def save_sync_state(self, state: IngestionState):
        """Save sync state to Snowflake"""
        cursor = self.connection.cursor()
        try:
            # Begin transaction for atomicity
            cursor.execute("BEGIN TRANSACTION")

            insert_query = """
            INSERT INTO GONG_INGESTION_STATE (
                LAST_SYNC_TIMESTAMP,
                LAST_CALL_ID,
                TOTAL_CALLS_PROCESSED,
                SYNC_MODE,
                CORRELATION_ID
            ) VALUES (?, ?, ?, ?, ?)
            """

            cursor.execute(
                insert_query,
                (
                    state.last_sync_timestamp,
                    state.last_call_id,
                    state.total_calls_processed,
                    state.sync_mode.value,
                    state.correlation_id,
                ),
            )

            # Commit transaction
            cursor.execute("COMMIT")
            logger.debug("Saved sync state to Snowflake")

        except Exception as e:
            # Rollback transaction on error
            try:
                cursor.execute("ROLLBACK")
            except Exception as rollback_error:
                logger.error(
                    f"Failed to rollback sync state transaction: {rollback_error}"
                )

            logger.error(f"Failed to save sync state: {e}")
            raise
        finally:
            cursor.close()

    async def close(self):
        """Close Snowflake connection"""
        if self.connection:
            self.connection.close()
            logger.info("Snowflake Gong loader connection closed")


class GongDataIngestionOrchestrator:
    """Orchestrates the complete Gong data ingestion process"""

    def __init__(self, sync_mode: SyncMode = SyncMode.INCREMENTAL):
        self.sync_mode = sync_mode
        self.gong_client = GongAPIClient()
        self.snowflake_loader = SnowflakeGongLoader()
        self.correlation_id = str(uuid.uuid4())

    async def run_ingestion(
        self,
        from_date: datetime | None = None,
        to_date: datetime | None = None,
        include_transcripts: bool = True,
    ) -> dict[str, Any]:
        """
        Run the complete Gong data ingestion process

        Args:
            from_date: Start date (if None, uses last sync timestamp)
            to_date: End date (if None, uses current time)
            include_transcripts: Whether to fetch and load transcripts

        Returns:
            Ingestion results summary
        """
        start_time = datetime.now()
        results = {
            "correlation_id": self.correlation_id,
            "sync_mode": self.sync_mode.value,
            "start_time": start_time.isoformat(),
            "calls_processed": 0,
            "transcripts_processed": 0,
            "errors": [],
            "success": False,
        }

        try:
            logger.info(
                "Starting Gong data ingestion",
                correlation_id=self.correlation_id,
                sync_mode=self.sync_mode.value,
            )

            # Initialize connections
            await self.snowflake_loader.initialize()

            # Determine date range
            if not from_date:
                state = await self.snowflake_loader.get_last_sync_state()
                from_date = state.last_sync_timestamp

            if not to_date:
                to_date = datetime.now()

            logger.info(
                f"Ingesting Gong data from {from_date} to {to_date}",
                correlation_id=self.correlation_id,
            )

            # Fetch and load calls
            calls_processed = await self._ingest_calls(from_date, to_date)
            results["calls_processed"] = calls_processed

            # Fetch and load transcripts if requested
            if include_transcripts and calls_processed > 0:
                transcripts_processed = await self._ingest_transcripts(
                    from_date, to_date
                )
                results["transcripts_processed"] = transcripts_processed

            # Update sync state
            new_state = IngestionState(
                last_sync_timestamp=to_date,
                total_calls_processed=calls_processed,
                sync_mode=self.sync_mode,
                correlation_id=self.correlation_id,
            )
            await self.snowflake_loader.save_sync_state(new_state)

            results["success"] = True
            results["end_time"] = datetime.now().isoformat()
            results["duration_seconds"] = (datetime.now() - start_time).total_seconds()

            logger.info(
                "Gong data ingestion completed successfully",
                correlation_id=self.correlation_id,
                calls_processed=calls_processed,
                transcripts_processed=results["transcripts_processed"],
                duration_seconds=results["duration_seconds"],
            )

        except Exception as e:
            results["errors"].append(str(e))
            results["success"] = False
            logger.error(
                f"Gong data ingestion failed: {e}",
                correlation_id=self.correlation_id,
                exc_info=True,
            )

        finally:
            await self.snowflake_loader.close()

        return results

    async def _ingest_calls(self, from_date: datetime, to_date: datetime) -> int:
        """Ingest calls data with pagination"""
        total_calls = 0
        cursor = None

        async with self.gong_client as client:
            while True:
                try:
                    response = await client.get_calls(
                        from_date=from_date, to_date=to_date, cursor=cursor, limit=100
                    )

                    calls = response.get("calls", [])
                    if not calls:
                        break

                    # Load batch into Snowflake
                    loaded_count = await self.snowflake_loader.load_raw_calls(calls)
                    total_calls += loaded_count

                    # Check for more data
                    records_info = response.get("records", {})
                    cursor = records_info.get("cursor")

                    if not cursor:
                        break

                    logger.debug(
                        f"Processed batch of {loaded_count} calls, total: {total_calls}",
                        correlation_id=self.correlation_id,
                    )

                except Exception as e:
                    logger.error(f"Error processing calls batch: {e}")
                    raise

        return total_calls

    async def _ingest_transcripts(self, from_date: datetime, to_date: datetime) -> int:
        """Ingest transcripts for calls in date range"""
        # Get call IDs from raw calls table
        cursor = self.snowflake_loader.connection.cursor()
        try:
            cursor.execute(
                """
                SELECT CALL_ID
                FROM GONG_CALLS_RAW
                WHERE INGESTED_AT >= ?
                AND INGESTED_AT <= ?
                AND CALL_ID NOT IN (
                    SELECT CALL_ID FROM GONG_CALL_TRANSCRIPTS_RAW
                )
            """,
                (from_date, to_date),
            )

            call_ids = [row[0] for row in cursor.fetchall()]

        finally:
            cursor.close()

        if not call_ids:
            logger.info("No new calls found for transcript processing")
            return 0

        transcripts_data = []

        async with self.gong_client as client:
            for call_id in call_ids:
                try:
                    transcript = await client.get_call_transcript(call_id)
                    transcripts_data.append(transcript)

                    # Process in batches of 50
                    if len(transcripts_data) >= 50:
                        await self.snowflake_loader.load_call_transcripts(
                            transcripts_data
                        )
                        transcripts_data = []

                except Exception as e:
                    logger.error(f"Error fetching transcript for call {call_id}: {e}")
                    continue

        # Process remaining transcripts
        if transcripts_data:
            await self.snowflake_loader.load_call_transcripts(transcripts_data)

        return len(call_ids)


# CLI Interface
async def main():
    """Main CLI entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Gong Data Ingestion for Snowflake")
    parser.add_argument(
        "--sync-mode",
        choices=["full", "incremental", "backfill"],
        default="incremental",
        help="Sync mode for data ingestion",

    async def load_batch_calls(self, calls: List[Dict[str, Any]]):
        """Load multiple calls in batch"""
        if not calls:
            return

        logger.info(f"Loading {len(calls)} calls in batch...")

        # Prepare batch operations
        operations = []
        for call in calls:
            operations.append(BatchOperation(
                query="""
                INSERT INTO RAW_estuary.GONG_CALLS (
                    CALL_ID, TITLE, SCHEDULED_START, DURATION,
                    PARTICIPANTS, TRANSCRIPT_URL, CREATED_AT
                ) VALUES (%(call_id)s, %(title)s, %(scheduled_start)s,
                         %(duration)s, %(participants)s, %(transcript_url)s,
                         %(created_at)s)
                """,
                params={
                    "call_id": call.get("id"),
                    "title": call.get("title"),
                    "scheduled_start": call.get("scheduledStart"),
                    "duration": call.get("duration"),
                    "participants": json.dumps(call.get("participants", [])),
                    "transcript_url": call.get("transcriptUrl"),
                    "created_at": datetime.utcnow()
                },
                operation_type="insert"
            ))

        # Execute batch
        db_manager = OptimizedDatabaseManager()
        await db_manager.initialize()

        try:
            result = await db_manager.execute_batch(
                ConnectionType.SNOWFLAKE,
                operations,
                transaction=True
            )

            if result["success"]:
                logger.info(f"✅ Batch insert successful: {result['affected_rows']} rows")
            else:
                logger.error(f"Batch insert failed: {result['errors']}")

        finally:
            await db_manager.close()

    )
    parser.add_argument(
        "--from-date", type=str, help="Start date (YYYY-MM-DD) for data ingestion"
    )
    parser.add_argument(
        "--to-date", type=str, help="End date (YYYY-MM-DD) for data ingestion"
    )
    parser.add_argument(
        "--no-transcripts", action="store_true", help="Skip transcript ingestion"
    )

    args = parser.parse_args()

    # Parse dates
    from_date = None
    to_date = None

    if args.from_date:
        from_date = datetime.fromisoformat(args.from_date)
    if args.to_date:
        to_date = datetime.fromisoformat(args.to_date)

    # Run ingestion
    orchestrator = GongDataIngestionOrchestrator(sync_mode=SyncMode(args.sync_mode))

    results = await orchestrator.run_ingestion(
        from_date=from_date,
        to_date=to_date,
        include_transcripts=not args.no_transcripts,
    )

    print(json.dumps(results, indent=2, default=str))


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
