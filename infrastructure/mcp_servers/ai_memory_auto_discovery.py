"""
Enhanced AI Memory Auto-Discovery System
Intelligent context detection, pattern recognition, and automated memory storage
"""
'\nFile Decomposition Plan (auto-generated by Phase 3)\nCurrent size: 1106 lines\n\nRecommended decomposition:\n- ai_memory_auto_discovery_core.py - Core functionality\n- ai_memory_auto_discovery_utils.py - Utility functions\n- ai_memory_auto_discovery_models.py - Data models\n- ai_memory_auto_discovery_handlers.py - Request handlers\n\nTODO: Implement file decomposition\n'
import asyncio
import json
import logging
import re
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

from core.config_manager import get_config_value
from infrastructure.mcp_servers.enhanced_ai_memory_mcp_server import (
    EnhancedAiMemoryMCPServer,
    MemoryCategory,
)
from infrastructure.services.llm_router import TaskType, llm_router

logger = logging.getLogger(__name__)

class ContextType(Enum):
    """Types of development context"""
    ARCHITECTURE_DECISION = 'architecture_decision'
    BUG_ANALYSIS = 'bug_analysis'
    CODE_PATTERN = 'code_pattern'
    PERFORMANCE_INSIGHT = 'performance_insight'
    SECURITY_CONSIDERATION = 'security_consideration'
    WORKFLOW_OPTIMIZATION = 'workflow_optimization'
    BUSINESS_LOGIC = 'business_logic'
    INTEGRATION_PATTERN = 'integration_pattern'
    ERROR_RESOLUTION = 'error_resolution'
    DEPLOYMENT_STRATEGY = 'deployment_strategy'

class ConfidenceLevel(Enum):
    """Confidence levels for auto-detected patterns"""
    LOW = 0.3
    MEDIUM = 0.6
    HIGH = 0.8
    VERY_HIGH = 0.9

@dataclass
class ContextPattern:
    """Pattern for detecting specific types of development context"""
    context_type: ContextType
    keywords: list[str]
    phrases: list[str]
    code_indicators: list[str]
    importance_weight: float
    confidence_threshold: float = 0.6

@dataclass
class DetectedContext:
    """Detected development context with metadata"""
    context_type: ContextType
    confidence: float
    key_points: list[str]
    suggested_category: str
    suggested_tags: list[str]
    importance_score: float
    reasoning: str
    extracted_code: str | None = None
    related_files: list[str] = field(default_factory=list)
    dependencies: list[str] = field(default_factory=list)

class IntelligentContextDetector:
    """
    Advanced context detection system that identifies development patterns,
    architectural decisions, and important insights from conversations
    """

    def __init__(self):
        self.patterns = self._initialize_patterns()
        self.initialized = False

    async def initialize(self):
        """Initialize the context detector"""
        if self.initialized:
            return
        try:
            logger.info('âœ… Intelligent Context Detector initialized')
            self.initialized = True
        except Exception as e:
            logger.exception(f'Failed to initialize Intelligent Context Detector: {e}')
            self.initialized = True

    def _initialize_patterns(self) -> list[ContextPattern]:
        """Initialize detection patterns for different context types"""
        return [ContextPattern(context_type=ContextType.ARCHITECTURE_DECISION, keywords=['architecture', 'design', 'structure', 'pattern', 'approach', 'strategy'], phrases=['we decided to', 'architectural decision', 'design choice', 'we chose', 'the approach is', "pattern we're using", 'architectural pattern', 'design decision', "we're implementing", 'the structure will be'], code_indicators=['class', 'interface', 'abstract', '@dataclass', 'def __init__'], importance_weight=0.9, confidence_threshold=0.7), ContextPattern(context_type=ContextType.BUG_ANALYSIS, keywords=['bug', 'error', 'issue', 'problem', 'fix', 'solution', 'debug'], phrases=['the bug was', 'error occurs', 'issue is', 'problem with', 'fixed by', 'solution is', 'root cause', 'debugging showed', 'the fix is', 'resolved by', 'error happens when'], code_indicators=['try:', 'except:', 'raise', 'assert', 'if __debug__', 'logging'], importance_weight=0.8, confidence_threshold=0.6), ContextPattern(context_type=ContextType.CODE_PATTERN, keywords=['pattern', 'implementation', 'method', 'function', 'class', 'algorithm'], phrases=['code pattern', 'implementation approach', 'we implement', 'the method is', 'algorithm we use', 'coding standard', 'best practice', 'pattern for', 'way to implement'], code_indicators=['def ', 'class ', 'async def', '@decorator', 'lambda', 'yield'], importance_weight=0.7, confidence_threshold=0.6), ContextPattern(context_type=ContextType.PERFORMANCE_INSIGHT, keywords=['performance', 'optimization', 'speed', 'memory', 'efficiency', 'bottleneck'], phrases=['performance improvement', 'optimization technique', 'bottleneck is', 'memory usage', 'speed up', 'efficient way', 'performance issue', 'optimize by', 'faster approach', 'reduce latency'], code_indicators=['async', 'await', 'cache', 'pool', 'batch', 'concurrent'], importance_weight=0.8, confidence_threshold=0.7), ContextPattern(context_type=ContextType.SECURITY_CONSIDERATION, keywords=['security', 'authentication', 'authorization', 'encryption', 'vulnerability'], phrases=['security consideration', 'authentication method', 'secure way', 'vulnerability in', 'encryption approach', 'security risk', 'authorization pattern', 'secure implementation', 'security best practice'], code_indicators=['auth', 'encrypt', 'hash', 'token', 'secret', 'validate'], importance_weight=0.9, confidence_threshold=0.8), ContextPattern(context_type=ContextType.INTEGRATION_PATTERN, keywords=['integration', 'api', 'service', 'connector', 'client', 'interface'], phrases=['integration with', 'api client', 'service connection', 'external service', 'integration pattern', 'connector for', 'interface to', 'api wrapper', 'service integration', 'external api', 'third party'], code_indicators=['requests', 'aiohttp', 'client', 'api', 'endpoint', 'async def'], importance_weight=0.8, confidence_threshold=0.6), ContextPattern(context_type=ContextType.BUSINESS_LOGIC, keywords=['business', 'logic', 'rule', 'process', 'workflow', 'requirement'], phrases=['business logic', 'business rule', 'process flow', 'workflow step', 'business requirement', 'logic for', 'business process', 'rule is', 'requirement is', 'business case'], code_indicators=['if', 'elif', 'else', 'match', 'case', 'validate'], importance_weight=0.8, confidence_threshold=0.7), ContextPattern(context_type=ContextType.ERROR_RESOLUTION, keywords=['error', 'exception', 'failure', 'resolve', 'handle', 'recovery'], phrases=['error handling', 'exception occurs', 'failure mode', 'error recovery', 'handle the error', 'exception handling', 'error case', 'failure scenario', 'error resolution', 'exception strategy'], code_indicators=['try:', 'except', 'finally:', 'raise', 'Exception', 'Error'], importance_weight=0.8, confidence_threshold=0.6), ContextPattern(context_type=ContextType.DEPLOYMENT_STRATEGY, keywords=['deployment', 'deploy', 'production', 'staging', 'environment', 'release'], phrases=['deployment strategy', 'deploy to', 'production deployment', 'staging environment', 'release process', 'deployment approach', 'environment setup', 'deploy with', 'production ready'], code_indicators=['docker', 'kubernetes', 'config', 'env', 'production', 'staging'], importance_weight=0.8, confidence_threshold=0.7)]

    async def detect_context(self, conversation: str, file_context: dict[str, Any] | None=None) -> list[DetectedContext]:
        """
        Detect development context from conversation with advanced AI analysis

        Args:
            conversation: The conversation text to analyze
            file_context: Optional context about files being discussed

        Returns:
            List of detected contexts with confidence scores
        """
        if not self.initialized:
            await self.initialize()
        detected_contexts = []
        pattern_contexts = self._detect_with_patterns(conversation, file_context)
        detected_contexts.extend(pattern_contexts)
        
        # Always try AI detection using llm_router
        ai_contexts = await self._detect_with_ai(conversation, file_context)
        detected_contexts.extend(ai_contexts)
        
        merged_contexts = self._merge_contexts(detected_contexts)
        high_confidence_contexts = [ctx for ctx in merged_contexts if ctx.confidence >= (ctx.context_type.value if hasattr(ctx.context_type, 'value') else 0.6)]
        return high_confidence_contexts

    def _detect_with_patterns(self, conversation: str, file_context: dict[str, Any] | None=None) -> list[DetectedContext]:
        """Pattern-based context detection"""
        detected = []
        conversation_lower = conversation.lower()
        for pattern in self.patterns:
            confidence = 0.0
            matched_keywords = []
            matched_phrases = []
            matched_code = []
            for keyword in pattern.keywords:
                if keyword.lower() in conversation_lower:
                    confidence += 0.1
                    matched_keywords.append(keyword)
            for phrase in pattern.phrases:
                if phrase.lower() in conversation_lower:
                    confidence += 0.2
                    matched_phrases.append(phrase)
            for indicator in pattern.code_indicators:
                if indicator in conversation:
                    confidence += 0.15
                    matched_code.append(indicator)
            if file_context:
                if file_context.get('file_type') in ['py', 'js', 'ts', 'java', 'cpp']:
                    confidence += 0.1
                if file_context.get('is_new_file'):
                    confidence += 0.05
            confidence *= pattern.importance_weight
            if confidence >= pattern.confidence_threshold:
                key_points = []
                key_points.extend([f'Keyword: {kw}' for kw in matched_keywords[:3]])
                key_points.extend([f'Phrase: {phrase}' for phrase in matched_phrases[:2]])
                key_points.extend([f'Code: {code}' for code in matched_code[:2]])
                tags = [pattern.context_type.value]
                tags.extend(matched_keywords[:3])
                if file_context and file_context.get('file_name'):
                    tags.append(f"file_{file_context['file_name'].replace('.', '_')}")
                detected.append(DetectedContext(context_type=pattern.context_type, confidence=min(confidence, 1.0), key_points=key_points, suggested_category=self._map_context_to_category(pattern.context_type), suggested_tags=tags, importance_score=pattern.importance_weight, reasoning=f'Pattern-based detection: {len(matched_keywords)} keywords, {len(matched_phrases)} phrases, {len(matched_code)} code indicators', extracted_code=self._extract_code_blocks(conversation), related_files=[file_context.get('file_name')] if file_context and file_context.get('file_name') else []))
        return detected

    async def _detect_with_ai(self, conversation: str, file_context: dict[str, Any] | None=None) -> list[DetectedContext]:
        """AI-enhanced context detection using OpenAI"""
        try:
            context_info = ''
            if file_context:
                context_info = f'\nFile context: {json.dumps(file_context, indent=2)}'
            prompt = f'\n            Analyze this development conversation and identify important contexts that should be remembered for future AI assistance.\n\n            Conversation:\n            {conversation[:4000]}  # Limit to avoid token limits\n            {context_info}\n\n            Identify and extract:\n            1. Architectural decisions and reasoning\n            2. Bug fixes and their root causes\n            3. Code patterns and best practices\n            4. Performance optimizations\n            5. Security considerations\n            6. Integration approaches\n            7. Business logic rules\n            8. Error handling strategies\n            9. Deployment strategies\n            10. Important insights or lessons learned\n\n            For each identified context, provide:\n            - Type of context\n            - Confidence level (0.0-1.0)\n            - Key points (bullet list)\n            - Suggested tags\n            - Importance score (0.0-1.0)\n            - Brief reasoning\n\n            Return as JSON array with this structure:\n            [{{\n                "context_type": "architecture_decision|bug_analysis|code_pattern|performance_insight|security_consideration|integration_pattern|business_logic|error_resolution|deployment_strategy",\n                "confidence": 0.8,\n                "key_points": ["point1", "point2"],\n                "suggested_tags": ["tag1", "tag2"],\n                "importance_score": 0.7,\n                "reasoning": "explanation"\n            }}]\n            '
            response = await llm_router.complete(prompt=prompt, task=TaskType.CODE_GENERATION, model='gpt-4o-mini', temperature=0.3, max_tokens=1500)
            ai_analysis = response.choices[0].message.content
            try:
                contexts_data = json.loads(ai_analysis)
                ai_contexts = []
                for ctx_data in contexts_data:
                    context_type = self._parse_context_type(ctx_data.get('context_type', ''))
                    if context_type:
                        ai_contexts.append(DetectedContext(context_type=context_type, confidence=float(ctx_data.get('confidence', 0.5)), key_points=ctx_data.get('key_points', []), suggested_category=self._map_context_to_category(context_type), suggested_tags=ctx_data.get('suggested_tags', []), importance_score=float(ctx_data.get('importance_score', 0.5)), reasoning=f'AI analysis: {ctx_data.get('reasoning', 'AI-detected pattern')}', extracted_code=self._extract_code_blocks(conversation), related_files=[file_context.get('file_name')] if file_context and file_context.get('file_name') else []))
                return ai_contexts
            except json.JSONDecodeError:
                logger.warning('Failed to parse AI context analysis response')
                return []
        except Exception as e:
            logger.exception(f'AI context detection failed: {e}')
            return []

    def _parse_context_type(self, type_str: str) -> ContextType | None:
        """Parse context type string to enum"""
        type_mapping = {'architecture_decision': ContextType.ARCHITECTURE_DECISION, 'bug_analysis': ContextType.BUG_ANALYSIS, 'code_pattern': ContextType.CODE_PATTERN, 'performance_insight': ContextType.PERFORMANCE_INSIGHT, 'security_consideration': ContextType.SECURITY_CONSIDERATION, 'integration_pattern': ContextType.INTEGRATION_PATTERN, 'business_logic': ContextType.BUSINESS_LOGIC, 'error_resolution': ContextType.ERROR_RESOLUTION, 'deployment_strategy': ContextType.DEPLOYMENT_STRATEGY, 'workflow_optimization': ContextType.WORKFLOW_OPTIMIZATION}
        return type_mapping.get(type_str.lower())

    def _map_context_to_category(self, context_type: ContextType) -> str:
        """Map context type to memory category"""
        mapping = {ContextType.ARCHITECTURE_DECISION: MemoryCategory.ARCHITECTURE, ContextType.BUG_ANALYSIS: MemoryCategory.BUG_SOLUTION, ContextType.CODE_PATTERN: MemoryCategory.AI_CODING_PATTERN, ContextType.PERFORMANCE_INSIGHT: MemoryCategory.PERFORMANCE_TIP, ContextType.SECURITY_CONSIDERATION: MemoryCategory.SECURITY_PATTERN, ContextType.INTEGRATION_PATTERN: MemoryCategory.AI_CODING_PATTERN, ContextType.BUSINESS_LOGIC: MemoryCategory.CODE_DECISION, ContextType.ERROR_RESOLUTION: MemoryCategory.BUG_SOLUTION, ContextType.DEPLOYMENT_STRATEGY: MemoryCategory.WORKFLOW, ContextType.WORKFLOW_OPTIMIZATION: MemoryCategory.WORKFLOW}
        return mapping.get(context_type, MemoryCategory.CODE_DECISION)

    def _extract_code_blocks(self, text: str) -> str | None:
        """Extract code blocks from text"""
        code_patterns = ['```[\\w]*\\n(.*?)\\n```', '`([^`\\n]+)`']
        code_blocks = []
        for pattern in code_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            code_blocks.extend(matches)
        if code_blocks:
            return '\n\n'.join(code_blocks[:3])
        return None

    def _merge_contexts(self, contexts: list[DetectedContext]) -> list[DetectedContext]:
        """Merge similar contexts and remove duplicates"""
        if not contexts:
            return []
        grouped = {}
        for ctx in contexts:
            key = ctx.context_type
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(ctx)
        merged = []
        for context_type, ctx_list in grouped.items():
            if len(ctx_list) == 1:
                merged.append(ctx_list[0])
            else:
                best_ctx = max(ctx_list, key=lambda x: x.confidence)
                all_key_points = []
                all_tags = set()
                all_files = set()
                for ctx in ctx_list:
                    all_key_points.extend(ctx.key_points)
                    all_tags.update(ctx.suggested_tags)
                    all_files.update(ctx.related_files)
                merged_ctx = DetectedContext(context_type=context_type, confidence=max(ctx.confidence for ctx in ctx_list), key_points=list(set(all_key_points))[:10], suggested_category=best_ctx.suggested_category, suggested_tags=list(all_tags)[:8], importance_score=max(ctx.importance_score for ctx in ctx_list), reasoning=f'Merged from {len(ctx_list)} detections: ' + '; '.join(ctx.reasoning for ctx in ctx_list)[:200], extracted_code=best_ctx.extracted_code, related_files=list(all_files))
                merged.append(merged_ctx)
        return merged

class AutoDiscoveryOrchestrator:
    """
    Orchestrates the automatic discovery and storage of development context
    """

    def __init__(self):
        self.context_detector = IntelligentContextDetector()
        self.ai_memory = None
        self.initialized = False
        self.discovery_stats = {'total_conversations_analyzed': 0, 'contexts_detected': 0, 'memories_stored': 0, 'last_analysis': None}

    async def initialize(self):
        """Initialize the auto-discovery system"""
        if self.initialized:
            return
        await self.context_detector.initialize()
        self.ai_memory = EnhancedAiMemoryMCPServer()
        await self.ai_memory.initialize()
        self.initialized = True
        logger.info('âœ… Auto-Discovery Orchestrator initialized')

    async def analyze_and_store_conversation(self, conversation: str, participants: list[str] | None=None, file_context: dict[str, Any] | None=None, auto_store: bool=True) -> dict[str, Any]:
        """
        Analyze conversation for important context and automatically store memories

        Args:
            conversation: The conversation text
            participants: List of participants
            file_context: Context about files being discussed
            auto_store: Whether to automatically store detected contexts

        Returns:
            Analysis results and storage status
        """
        if not self.initialized:
            await self.initialize()
        try:
            detected_contexts = await self.context_detector.detect_context(conversation, file_context)
            self.discovery_stats['total_conversations_analyzed'] += 1
            self.discovery_stats['contexts_detected'] += len(detected_contexts)
            self.discovery_stats['last_analysis'] = datetime.now().isoformat()
            if not detected_contexts:
                return {'status': 'no_context_detected', 'message': 'No significant development context detected', 'conversation_length': len(conversation), 'stats': self.discovery_stats}
            stored_memories = []
            if auto_store:
                for context in detected_contexts:
                    if context.confidence >= 0.7:
                        try:
                            memory_content = self._create_memory_content(conversation, context, participants, file_context)
                            storage_result = await self.ai_memory.store_memory(content=memory_content, category=context.suggested_category, tags=context.suggested_tags, importance_score=context.importance_score, auto_detected=True)
                            stored_memories.append({'context_type': context.context_type.value, 'confidence': context.confidence, 'storage_result': storage_result, 'memory_id': storage_result.get('id')})
                            self.discovery_stats['memories_stored'] += 1
                        except Exception as e:
                            logger.exception(f'Failed to store context memory: {e}')
                            stored_memories.append({'context_type': context.context_type.value, 'confidence': context.confidence, 'storage_result': {'status': 'error', 'error': str(e)}, 'memory_id': None})
            return {'status': 'analysis_complete', 'detected_contexts': len(detected_contexts), 'high_confidence_contexts': len([ctx for ctx in detected_contexts if ctx.confidence >= 0.7]), 'stored_memories': len(stored_memories), 'contexts': [{'type': ctx.context_type.value, 'confidence': ctx.confidence, 'key_points': ctx.key_points, 'importance': ctx.importance_score, 'reasoning': ctx.reasoning} for ctx in detected_contexts], 'storage_results': stored_memories, 'stats': self.discovery_stats}
        except Exception as e:
            logger.exception(f'Auto-discovery analysis failed: {e}')
            return {'status': 'error', 'error': str(e), 'stats': self.discovery_stats}

    def _create_memory_content(self, conversation: str, context: DetectedContext, participants: list[str] | None=None, file_context: dict[str, Any] | None=None) -> str:
        """Create comprehensive memory content from detected context"""
        content_parts = [f'# {context.context_type.value.replace('_', ' ').title()}', f'**Confidence:** {context.confidence:.2f}', f'**Importance:** {context.importance_score:.2f}', f'**Detected:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}']
        if participants:
            content_parts.append(f'**Participants:** {', '.join(participants)}')
        if file_context:
            content_parts.append(f'**File Context:** {file_context.get('file_name', 'Unknown')}')
        if context.key_points:
            content_parts.append('\n## Key Points:')
            for point in context.key_points:
                content_parts.append(f'- {point}')
        if context.extracted_code:
            content_parts.append('\n## Code Examples:')
            content_parts.append(f'```\n{context.extracted_code}\n```')
        if context.related_files:
            content_parts.append('\n## Related Files:')
            for file in context.related_files:
                content_parts.append(f'- {file}')
        content_parts.append('\n## Context:')
        content_parts.append(context.reasoning)
        conversation_excerpt = conversation[:1000] + '...' if len(conversation) > 1000 else conversation
        content_parts.append('\n## Conversation Excerpt:')
        content_parts.append(f'```\n{conversation_excerpt}\n```')
        return '\n'.join(content_parts)

    async def get_discovery_stats(self) -> dict[str, Any]:
        """Get auto-discovery statistics"""
        return {**self.discovery_stats, 'initialized': self.initialized, 'detector_available': self.context_detector.initialized, 'ai_memory_available': self.ai_memory.initialized if self.ai_memory else False}

    async def smart_recall(self, query: str, context_hint: str | None=None, file_context: dict[str, Any] | None=None, limit: int=5) -> list[dict[str, Any]]:
        """
        Smart recall that considers current context and file awareness

        Args:
            query: Search query
            context_hint: Hint about the type of context being sought
            file_context: Current file context for relevance
            limit: Maximum results

        Returns:
            Contextually relevant memories
        """
        if not self.initialized:
            await self.initialize()
        try:
            enhanced_query = query
            if context_hint:
                enhanced_query += f' {context_hint}'
            if file_context and file_context.get('file_name'):
                file_name = file_context['file_name']
                file_ext = file_name.split('.')[-1] if '.' in file_name else ''
                enhanced_query += f' {file_name} {file_ext}'
            memories = await self.ai_memory.recall_memory(query=enhanced_query, limit=limit * 2)
            contextual_memories = []
            for memory in memories:
                relevance_score = memory.get('relevance_score', 0.5)
                if file_context and file_context.get('file_name'):
                    file_name = file_context['file_name']
                    memory_content = memory.get('content', '').lower()
                    if file_name.lower() in memory_content:
                        relevance_score += 0.2
                    if '.' in file_name:
                        file_ext = file_name.split('.')[-1]
                        if file_ext in memory_content:
                            relevance_score += 0.1
                if memory.get('auto_detected'):
                    relevance_score += 0.1
                contextual_memories.append({**memory, 'contextual_relevance': min(relevance_score, 1.0)})
            contextual_memories.sort(key=lambda x: x['contextual_relevance'], reverse=True)
            return contextual_memories[:limit]
        except Exception as e:
            logger.exception(f'Smart recall failed: {e}')
            return []
auto_discovery = AutoDiscoveryOrchestrator()

async def analyze_conversation_auto(conversation: str, participants: list[str] | None=None, file_context: dict[str, Any] | None=None, auto_store: bool=True) -> dict[str, Any]:
    """
    Convenience function for automatic conversation analysis

    Args:
        conversation: Conversation text to analyze
        participants: List of participants
        file_context: File context information
        auto_store: Whether to automatically store detected contexts

    Returns:
        Analysis and storage results
    """
    return await auto_discovery.analyze_and_store_conversation(conversation=conversation, participants=participants, file_context=file_context, auto_store=auto_store)

async def smart_memory_recall(query: str, context_hint: str | None=None, file_context: dict[str, Any] | None=None, limit: int=5) -> list[dict[str, Any]]:
    """
    Convenience function for smart memory recall

    Args:
        query: Search query
        context_hint: Context hint for better results
        file_context: Current file context
        limit: Maximum results

    Returns:
        Contextually relevant memories
    """
    return await auto_discovery.smart_recall(query=query, context_hint=context_hint, file_context=file_context, limit=limit)
if __name__ == '__main__':

    async def test_auto_discovery():
        """Test the auto-discovery system"""
        test_conversation = "\n        We decided to use Snowflake Cortex for our vector embeddings because it provides\n        native integration with our data warehouse. The architecture decision was based on:\n\n        1. Performance - native vector operations are faster\n        2. Cost - reduces external dependencies on Pinecone for business data\n        3. Security - data stays within our enterprise data warehouse\n\n        Here's the implementation pattern we're using:\n\n        ```python\n        async def store_embedding_in_business_table(self, table_name: str, record_id: str):\n            embedding = await cortex.embed_text(text_content)\n            await self.update_record(table_name, record_id, embedding)\n        ```\n\n        This approach solved the SQL injection vulnerability we had before by using\n        parameterized queries and whitelist validation.\n        "
        file_context = {'file_name': 'snowflake_cortex_service.py', 'file_type': 'py', 'is_new_file': False}
        await analyze_conversation_auto(conversation=test_conversation, participants=['developer', 'architect'], file_context=file_context, auto_store=True)
        await smart_memory_recall(query='vector embedding architecture decision', context_hint='architecture', file_context=file_context, limit=3)
    asyncio.run(test_auto_discovery())
