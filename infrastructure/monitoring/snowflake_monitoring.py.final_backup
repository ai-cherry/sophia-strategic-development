"""
Comprehensive Snowflake monitoring and optimization system
Provides real-time insights and automated optimization recommendations
"""

from backend.services.unified_memory_service_v3 import UnifiedMemoryServiceV3
import asyncio
import json
import logging
import time
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

import schedule
from prometheus_client import Counter, Gauge, Histogram, start_http_server

from core.infra.cortex_gateway import get_gateway
from infrastructure.adapters.enhanced_snowflake_adapter import get_adapter

logger = logging.getLogger(__name__)

# Prometheus metrics
# REMOVED: Snowflake dependency Histogram(
    "snowflake_query_duration_seconds",
    "Time spent on Snowflake queries",
    ["function", "warehouse"],
)
# REMOVED: Snowflake dependency Counter(
    "snowflake_queries_total",
    "Total number of Snowflake queries",
    ["function", "status"],
)
# REMOVED: Snowflake dependency Gauge(
    "snowflake_warehouse_utilization", "Warehouse utilization percentage", ["warehouse"]
)
# REMOVED: Snowflake dependency Counter(
    "snowflake_credits_total",
    "Total Snowflake credits consumed",
    ["warehouse", "function"],
)
# REMOVED: Snowflake dependency Gauge(
    "snowflake_daily_credits_remaining", "Remaining daily credit allowance"
)
# REMOVED: Snowflake dependency Gauge(
    "snowflake_cost_savings_potential", "Potential cost savings in dollars"
)
# REMOVED: Snowflake dependency Counter(
    "snowflake_cache_hits_total", "Number of cache hits", ["cache_type"]
)
# REMOVED: Snowflake dependency Counter(
    "snowflake_errors_total",
    "Total number of Snowflake errors",
    ["error_type", "warehouse"],
)


class AlertSeverity(str, Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class Alert:
    severity: AlertSeverity
    title: str
    description: str
    timestamp: datetime
    warehouse: str | None = None
    recommended_action: str | None = None
    metric_value: float | None = None


class SnowflakeMonitor:
    """Comprehensive Snowflake monitoring system"""

    def __init__(self):
        self.gateway = get_gateway()
        self.adapter = get_adapter()
        self.alerts = []
        self.optimization_history = []
        self.cost_baseline = None
        self._initialized = False

    async def initialize(self):
        """Initialize monitoring system"""
        if self._initialized:
            return

        await self.gateway.initialize()
        await self.adapter.initialize()

        # Create monitoring tables if they don't exist
        await self._create_monitoring_tables()

        self._initialized = True
        logger.info("✅ SnowflakeMonitor initialized")

    async def _create_monitoring_tables(self):
        """Create monitoring tables in Snowflake"""
        create_tables_sql = """
        -- Create monitoring schema if not exists
        CREATE SCHEMA IF NOT EXISTS SOPHIA_AI_UNIFIED.MONITORING;

        -- Query performance tracking
        CREATE TABLE IF NOT EXISTS SOPHIA_AI_UNIFIED.MONITORING.QUERY_PERFORMANCE (
            timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
            query_id VARCHAR,
            function_name VARCHAR,
            warehouse_name VARCHAR,
            execution_time_ms NUMBER,
            credits_used FLOAT,
            rows_returned NUMBER,
            cache_hit BOOLEAN,
            error_message VARCHAR,
            user_name VARCHAR,
            session_id VARCHAR
        );

        -- Credit usage tracking
        CREATE TABLE IF NOT EXISTS SOPHIA_AI_UNIFIED.MONITORING.CREDIT_USAGE (
            date DATE,
            warehouse_name VARCHAR,
            function_type VARCHAR,
            credits_used FLOAT,
            query_count NUMBER,
            avg_execution_time_ms NUMBER,
            PRIMARY KEY (date, warehouse_name, function_type)
        );

        -- Optimization recommendations
        CREATE TABLE IF NOT EXISTS SOPHIA_AI_UNIFIED.MONITORING.OPTIMIZATION_LOG (
            timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
            warehouse_name VARCHAR,
            recommendation_type VARCHAR,
            current_state VARCHAR,
            recommended_action VARCHAR,
            potential_savings_credits FLOAT,
            implemented BOOLEAN DEFAULT FALSE,
            implementation_date TIMESTAMP_NTZ
        );

        -- Alert history
        CREATE TABLE IF NOT EXISTS SOPHIA_AI_UNIFIED.MONITORING.ALERT_HISTORY (
            timestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
            severity VARCHAR,
            title VARCHAR,
            description VARCHAR,
            warehouse_name VARCHAR,
            metric_value FLOAT,
            recommended_action VARCHAR,
            acknowledged BOOLEAN DEFAULT FALSE,
            acknowledged_by VARCHAR,
            acknowledged_at TIMESTAMP_NTZ
        );
        """

        try:
            await self.gateway.execute_sql(create_tables_sql)
            logger.info("✅ Monitoring tables created/verified")
        except Exception as e:
            logger.error(f"Failed to create monitoring tables: {e}")

    async def collect_warehouse_metrics(self) -> dict[str, Any]:
        """Collect comprehensive warehouse metrics"""

        # Get warehouse utilization (for new system, will be minimal)
        utilization_query = """
        SELECT
            WAREHOUSE_NAME,
            COALESCE(AVG(AVG_RUNNING), 0) as avg_utilization,
            COALESCE(AVG(AVG_QUEUED_LOAD), 0) as avg_queue_load,
            COALESCE(SUM(CREDITS_USED), 0) as total_credits,
            COUNT(*) as query_count,
            MAX(START_TIME) as last_activity
        # REMOVED: Snowflake dependency
        WHERE START_TIME >= DATEADD(hour, -24, CURRENT_TIMESTAMP())
        GROUP BY WAREHOUSE_NAME
        ORDER BY total_credits DESC;
        """

        try:
            utilization_data = await self.adapter.execute_query(
                utilization_query, "analytics", use_cache=False
            )
        except Exception as e:
            logger.warning(f"No warehouse history yet: {e}")
            utilization_data = []

        # Get query performance metrics
        performance_query = """
        SELECT
            WAREHOUSE_NAME,
            COUNT(*) as query_count,
            AVG(TOTAL_ELAPSED_TIME) as avg_query_time,
            AVG(COMPILATION_TIME) as avg_compilation_time,
            AVG(EXECUTION_TIME) as avg_execution_time,
            SUM(CASE WHEN ERROR_CODE IS NOT NULL THEN 1 ELSE 0 END) as error_count
        # REMOVED: Snowflake dependency
        WHERE START_TIME >= DATEADD(hour, -24, CURRENT_TIMESTAMP())
            AND WAREHOUSE_NAME IS NOT NULL
        GROUP BY WAREHOUSE_NAME;
        """

        try:
            performance_data = await self.adapter.execute_query(
                performance_query, "analytics", use_cache=False
            )
        except Exception as e:
            logger.warning(f"No query history yet: {e}")
            performance_data = []

        # Update Prometheus metrics
        for warehouse in utilization_data:
            snowflake_warehouse_utilization.labels(
                warehouse=warehouse["WAREHOUSE_NAME"]
            ).set(warehouse["AVG_UTILIZATION"] or 0)

        # Log to monitoring table
        if utilization_data or performance_data:
            await self._log_metrics_to_snowflake(utilization_data, performance_data)

        return {
            "utilization": utilization_data,
            "performance": performance_data,
            "timestamp": datetime.now().isoformat(),
        }

    async def _log_metrics_to_snowflake(
        self, utilization_data: list[dict], performance_data: list[dict]
    ):
        """Log metrics to Snowflake monitoring tables"""

        # Prepare credit usage data
        if utilization_data:
            credit_values = []
            for warehouse in utilization_data:
                credit_values.append(
                    (
                        datetime.now().date(),
                        warehouse["WAREHOUSE_NAME"],
                        "general",  # Will be enhanced with actual function types
                        warehouse["TOTAL_CREDITS"] or 0,
                        warehouse["QUERY_COUNT"] or 0,
                        0,  # Will be updated with actual execution time
                    )
                )

            # Insert credit usage
            insert_credit_sql = """
            MERGE INTO SOPHIA_AI_UNIFIED.MONITORING.CREDIT_USAGE t
            USING (SELECT ? as date, ? as warehouse_name, ? as function_type,
                         ? as credits_used, ? as query_count, ? as avg_execution_time_ms) s
            ON t.date = s.date AND t.warehouse_name = s.warehouse_name
               AND t.function_type = s.function_type
            WHEN MATCHED THEN UPDATE SET
                credits_used = s.credits_used,
                query_count = s.query_count,
                avg_execution_time_ms = s.avg_execution_time_ms
            WHEN NOT MATCHED THEN INSERT
                (date, warehouse_name, function_type, credits_used, query_count, avg_execution_time_ms)
                VALUES (s.date, s.warehouse_name, s.function_type, s.credits_used,
                       s.query_count, s.avg_execution_time_ms);
            """

            for values in credit_values:
                try:
                    await self.gateway.execute_sql(insert_credit_sql)
                except Exception as e:
                    logger.error(f"Failed to log credit usage: {e}")

    async def analyze_cost_optimization(self) -> dict[str, Any]:
        """Analyze cost optimization opportunities"""

        # For new system, create baseline recommendations
        optimization_recommendations = []

        # Check if warehouses exist
        warehouse_check = """
        SHOW WAREHOUSES;
        """

        try:
            warehouses = await self.adapter.execute_query(
                warehouse_check, use_cache=False
            )

            for warehouse in warehouses:
                wh_name = warehouse.get("name", "")
                wh_size = warehouse.get("size", "")
                wh_state = warehouse.get("state", "")

                # Basic recommendations for new system
                if wh_state == "SUSPENDED":
                    continue

                if wh_size in ["XLARGE", "XXLARGE", "XXXLARGE", "X4LARGE"]:
                    optimization_recommendations.append(
                        {
                            "warehouse": wh_name,
                            "current_size": wh_size,
                            "recommendation": "Consider starting with MEDIUM size",
                            "potential_savings": 50,  # Estimated percentage
                            "action": "RESIZE",
                        }
                    )

                # Log recommendation
                await self._log_optimization_recommendation(
                    {
                        "warehouse_name": wh_name,
                        "recommendation_type": "INITIAL_SIZING",
                        "current_state": wh_size,
                        "recommended_action": "Start with MEDIUM, scale as needed",
                        "potential_savings_credits": 10,  # Estimated daily
                    }
                )

        except Exception as e:
            logger.warning(f"No warehouses configured yet: {e}")

        # Update Prometheus metric
        total_potential_savings = sum(
            rec.get("potential_savings", 0) for rec in optimization_recommendations
        )
        snowflake_cost_optimization.set(total_potential_savings)

        return {
            "recommendations": optimization_recommendations,
            "total_potential_savings_percentage": total_potential_savings,
            "timestamp": datetime.now().isoformat(),
        }

    async def _log_optimization_recommendation(self, recommendation: dict[str, Any]):
        """Log optimization recommendation to Snowflake"""

        insert_sql = """
        INSERT INTO SOPHIA_AI_UNIFIED.MONITORING.OPTIMIZATION_LOG
        (warehouse_name, recommendation_type, current_state,
         recommended_action, potential_savings_credits)
        VALUES (?, ?, ?, ?, ?);
        """

        try:
            await self.gateway.execute_sql(insert_sql)
        except Exception as e:
            logger.error(f"Failed to log optimization: {e}")

    async def check_performance_alerts(self) -> list[Alert]:
        """Check for performance-related alerts"""

        alerts = []

        # For new system, check basic configuration alerts
        try:
            # Check if monitoring is set up
            health = await self.gateway.health_check()

            if health.get("status") == "healthy":
                alerts.append(
                    Alert(
                        severity=AlertSeverity.INFO,
                        title="Snowflake Monitoring Active",
                        description="Monitoring system is operational and collecting metrics",
                        timestamp=datetime.now(),
                    )
                )

            # Check credit usage from adapter
            credit_summary = await self.adapter.get_credit_usage_summary()

            if credit_summary["usage_percentage"] > 80:
                alerts.append(
                    Alert(
                        severity=AlertSeverity.WARNING,
                        title="High Credit Usage",
                        description=f"Daily credit usage at {credit_summary['usage_percentage']:.1f}%",
                        timestamp=datetime.now(),
                        recommended_action="Review query patterns and warehouse sizing",
                        metric_value=credit_summary["usage_percentage"],
                    )
                )

            # Log alerts
            for alert in alerts:
                await self._log_alert(alert)

        except Exception as e:
            logger.error(f"Alert check failed: {e}")
            alerts.append(
                Alert(
                    severity=AlertSeverity.WARNING,
                    title="Monitoring Error",
                    description=f"Failed to check alerts: {e!s}",
                    timestamp=datetime.now(),
                )
            )

        return alerts

    async def _log_alert(self, alert: Alert):
        """Log alert to Snowflake"""

        insert_sql = """
        INSERT INTO SOPHIA_AI_UNIFIED.MONITORING.ALERT_HISTORY
        (severity, title, description, warehouse_name,
         metric_value, recommended_action)
        VALUES (?, ?, ?, ?, ?, ?);
        """

        try:
            await self.gateway.execute_sql(insert_sql)
        except Exception as e:
            logger.error(f"Failed to log alert: {e}")

    async def generate_monitoring_dashboard_data(self) -> dict[str, Any]:
        """Generate data for monitoring dashboard"""

        # Collect current metrics
        warehouse_metrics = await self.collect_warehouse_metrics()
        cost_analysis = await self.analyze_cost_optimization()
        alerts = await self.check_performance_alerts()
        credit_summary = await self.adapter.get_credit_usage_summary()

        # Format for dashboard
        dashboard_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_warehouses": len(warehouse_metrics.get("utilization", [])),
                "active_queries": sum(
                    w.get("QUERY_COUNT", 0)
                    for w in warehouse_metrics.get("performance", [])
                ),
                "daily_credits_used": credit_summary.get("credits_used", 0),
                "daily_credits_limit": credit_summary.get("daily_limit", 100),
                "credit_usage_percentage": credit_summary.get("usage_percentage", 0),
                "active_alerts": len(
                    [a for a in alerts if a.severity != AlertSeverity.INFO]
                ),
                "potential_savings": cost_analysis.get(
                    "total_potential_savings_percentage", 0
                ),
            },
            "warehouses": warehouse_metrics.get("utilization", []),
            "performance": warehouse_metrics.get("performance", []),
            "optimization": cost_analysis.get("recommendations", []),
            "alerts": [
                {
                    "severity": alert.severity.value,
                    "title": alert.title,
                    "description": alert.description,
                    "timestamp": alert.timestamp.isoformat(),
                    "warehouse": alert.warehouse,
                    "action": alert.recommended_action,
                }
                for alert in alerts
            ],
            "credit_breakdown": credit_summary.get("top_warehouses", []),
        }

        return dashboard_data

    async def run_monitoring_cycle(self):
        """Run complete monitoring cycle"""

        logger.info("🔍 Starting Snowflake monitoring cycle")

        try:
            await self.initialize()

            # Collect metrics
            metrics = await self.collect_warehouse_metrics()

            # Analyze costs
            cost_analysis = await self.analyze_cost_optimization()

            # Check for alerts
            new_alerts = await self.check_performance_alerts()
            self.alerts.extend(new_alerts)

            # Update Prometheus metrics
            # REMOVED: Snowflake dependency"success").inc()

            # Generate dashboard data
            dashboard_data = await self.generate_monitoring_dashboard_data()

            # Save dashboard data for API access
            with open("reports/snowflake_monitoring_latest.json", "w") as f:
                json.dump(dashboard_data, f, indent=2, default=str)

            # Log summary
            logger.info("✅ Monitoring cycle completed:")
            logger.info(
                f"   - Warehouses monitored: {len(metrics.get('utilization', []))}"
            )
            logger.info(
                f"   - Credits used today: {dashboard_data['summary']['daily_credits_used']:.2f}"
            )
            logger.info(f"   - New alerts: {len(new_alerts)}")
            logger.info(
                f"   - Potential savings: {cost_analysis.get('total_potential_savings_percentage', 0)}%"
            )

            return dashboard_data

        except Exception as e:
            logger.error(f"❌ Monitoring cycle failed: {e}")
            # REMOVED: Snowflake dependency"error").inc()
            snowflake_errors.labels(
                error_type="monitoring_cycle", warehouse="system"
            ).inc()
            return None


# Global monitor instance
# REMOVED: Snowflake dependency SnowflakeMonitor()


async def scheduled_monitoring():
    """Scheduled monitoring function"""
    await snowflake_monitor.run_monitoring_cycle()


def start_monitoring_service(port: int = 8003):
    """Start monitoring service with Prometheus metrics"""

    # Start Prometheus metrics server
    start_http_server(port)
    logger.info(f"📊 Prometheus metrics server started on port {port}")
    logger.info("📊 Metrics available at http://localhost:{port}/metrics")

    # Schedule monitoring cycles
    schedule.every(15).minutes.do(lambda: asyncio.create_task(scheduled_monitoring()))
    schedule.every().hour.do(lambda: asyncio.create_task(scheduled_monitoring()))

    # Run initial monitoring cycle
    asyncio.create_task(scheduled_monitoring())

    logger.info("⏰ Monitoring scheduler started")

    # Run scheduler
    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    # For testing, run one monitoring cycle
    import asyncio

    asyncio.run(snowflake_monitor.run_monitoring_cycle())
