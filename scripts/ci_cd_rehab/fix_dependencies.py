#!/usr/bin/env python3
"""
Dependency Resolution Script for CI/CD Pipeline Rehabilitation
Fixes missing packages, pins versions, and validates imports
"""

import logging
import re
import subprocess
import sys
from pathlib import Path

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# Known problematic packages to replace
PACKAGE_REPLACEMENTS = {
    "anthropic-mcp-python-sdk": "anthropic>=0.25.0,<1.0",
    "anthropic_mcp_python_sdk": "anthropic>=0.25.0,<1.0",
    "anthropic-mcp-sdk": "anthropic>=0.25.0,<1.0",
}

# Core packages to pin with major version constraints
CORE_PACKAGES = {
    "fastapi": ">=0.110,<1.0",
    "anthropic": ">=0.25.0,<1.0",
    "openai": ">=1.13.0,<2.0",
    "snowflake-connector-python": ">=3.6.0,<4.0",
    "pulumi": ">=3.100.0,<4.0",
    "uvicorn[standard]": ">=0.29.0,<1.0",
    "pydantic": ">=2.5.0,<3.0",
    "httpx": ">=0.25.0,<1.0",
    "redis": ">=5.0.0,<6.0",
    "prometheus-client": ">=0.19.0,<1.0",
    "structlog": ">=24.0.0,<25.0",
    "pytest": ">=7.4.0,<8.0",
    "pytest-asyncio": ">=0.21.0,<1.0",
    "ruff": ">=0.1.0,<1.0",
    "black": ">=23.0.0,<24.0",
    "mypy": ">=1.8.0,<2.0",
}


class DependencyResolver:
    """Resolve and fix dependency issues across the project"""

    def __init__(self):
        self.project_root = Path.cwd()
        self.requirements_files: list[Path] = []
        self.import_issues: list[tuple[Path, str, str]] = []
        self.fixed_files: set[Path] = set()

    def find_requirements_files(self) -> list[Path]:
        """Find all requirements files in the project"""
        patterns = ["requirements*.txt", "pyproject.toml", "setup.py", "Pipfile"]
        found_files = []

        for pattern in patterns:
            found_files.extend(self.project_root.rglob(pattern))

        # Filter out unwanted paths
        filtered = []
        for file in found_files:
            path_str = str(file)
            if any(
                skip in path_str
                for skip in [
                    "__pycache__",
                    ".git",
                    "venv",
                    ".venv",
                    "node_modules",
                    "build",
                    "dist",
                    ".techdebt",
                ]
            ):
                continue
            filtered.append(file)

        self.requirements_files = sorted(filtered)
        logger.info(f"Found {len(self.requirements_files)} requirements files")
        return self.requirements_files

    def fix_missing_package(self, file_path: Path):
        """Fix missing anthropic-mcp-python-sdk package references"""
        try:
            content = file_path.read_text()
            original_content = content

            # Replace problematic packages
            for old_pkg, new_pkg in PACKAGE_REPLACEMENTS.items():
                # Match various patterns
                patterns = [
                    f"{old_pkg}[>=<~!]*[0-9.]*",  # With version
                    f"{old_pkg}",  # Without version
                ]

                for pattern in patterns:
                    content = re.sub(pattern, new_pkg, content, flags=re.IGNORECASE)

            if content != original_content:
                file_path.write_text(content)
                self.fixed_files.add(file_path)
                logger.info(f"Fixed package references in {file_path}")

        except Exception as e:
            logger.exception(f"Error fixing {file_path}: {e}")

    def create_consolidated_requirements(self):
        """Create consolidated requirements.in file"""
        requirements_in = self.project_root / "requirements.in"

        content = """# Core dependencies for Sophia AI
# Auto-generated by CI/CD rehabilitation script

# Web framework
fastapi>=0.110,<1.0
uvicorn[standard]>=0.29.0,<1.0
pydantic>=2.5.0,<3.0
pydantic-settings>=2.0.0,<3.0

# AI/ML providers
anthropic>=0.25.0,<1.0
openai>=1.13.0,<2.0

# Database and data processing
snowflake-connector-python>=3.6.0,<4.0
redis>=5.0.0,<6.0
sqlalchemy>=2.0.0,<3.0

# Infrastructure
pulumi>=3.100.0,<4.0
docker>=6.1.0,<7.0

# HTTP and networking
httpx>=0.25.0,<1.0
aiohttp>=3.9.0,<4.0
websockets>=12.0,<13.0

# Monitoring and logging
prometheus-client>=0.19.0,<1.0
structlog>=24.0.0,<25.0

# Testing
pytest>=7.4.0,<8.0
pytest-asyncio>=0.21.0,<1.0
pytest-cov>=4.1.0,<5.0
httpx-mock>=0.25.0,<1.0

# Code quality
ruff>=0.1.0,<1.0
black>=23.0.0,<24.0
mypy>=1.8.0,<2.0
pre-commit>=3.5.0,<4.0

# Utilities
python-dotenv>=1.0.0,<2.0
pyyaml>=6.0.0,<7.0
click>=8.1.0,<9.0
rich>=13.7.0,<14.0
"""

        requirements_in.write_text(content)
        logger.info(f"Created consolidated {requirements_in}")
        return requirements_in

    def compile_with_uv(self):
        """Compile requirements using uv"""
        try:
            # Install uv if not present
            subprocess.run(["pip", "install", "uv"], check=True, capture_output=True)

            # Compile requirements
            logger.info("Compiling requirements with uv...")
            result = subprocess.run(
                [
                    "uv",
                    "pip",
                    "compile",
                    "requirements.in",
                    "--python-version",
                    "3.12",
                    "--output-file",
                    "requirements.txt",
                ],
                capture_output=True,
                text=True,
                check=False,
            )

            if result.returncode == 0:
                logger.info("Successfully compiled requirements.txt")

                # Also create uv.lock
                subprocess.run(["uv", "lock"], check=False)
                logger.info("Created uv.lock file")
            else:
                logger.error(f"UV compilation failed: {result.stderr}")

        except subprocess.CalledProcessError as e:
            logger.exception(f"Failed to run uv: {e}")

    def scan_imports(self) -> list[tuple[Path, str, str]]:
        """Scan Python files for problematic imports"""
        python_files = list(self.project_root.rglob("*.py"))

        # Skip certain directories
        python_files = [
            f
            for f in python_files
            if not any(
                skip in str(f)
                for skip in [
                    "__pycache__",
                    ".git",
                    "venv",
                    ".venv",
                    "node_modules",
                    "build",
                    "dist",
                    ".techdebt",
                ]
            )
        ]

        problematic_imports = []

        for file_path in python_files:
            try:
                content = file_path.read_text()

                # Check for old MCP SDK imports
                old_imports = [
                    r"from\s+anthropic_mcp_python_sdk\s+import",
                    r"import\s+anthropic_mcp_python_sdk",
                    r"from\s+anthropic\.mcp\s+import",
                ]

                for pattern in old_imports:
                    matches = re.finditer(pattern, content, re.MULTILINE)
                    for match in matches:
                        line_num = content[: match.start()].count("\n") + 1
                        problematic_imports.append(
                            (file_path, f"Line {line_num}", match.group())
                        )

            except Exception as e:
                logger.warning(f"Error scanning {file_path}: {e}")

        self.import_issues = problematic_imports
        return problematic_imports

    def fix_imports(self):
        """Fix problematic imports in Python files"""
        for file_path, _line_info, _import_stmt in self.import_issues:
            try:
                content = file_path.read_text()
                original_content = content

                # Replace old MCP imports with shim
                replacements = {
                    "from anthropic_mcp_python_sdk import mcp_tool": "from backend.mcp.shim import mcp_tool",
                    "from anthropic_mcp_python_sdk import": "from backend.mcp.shim import",
                    "import anthropic_mcp_python_sdk": "import backend.mcp.shim as mcp",
                }

                for old, new in replacements.items():
                    content = content.replace(old, new)

                if content != original_content:
                    file_path.write_text(content)
                    self.fixed_files.add(file_path)
                    logger.info(f"Fixed imports in {file_path}")

            except Exception as e:
                logger.exception(f"Error fixing imports in {file_path}: {e}")

    def create_validation_test(self):
        """Create test to validate all dependencies are importable"""
        test_file = self.project_root / "tests" / "test_dependencies.py"
        test_file.parent.mkdir(parents=True, exist_ok=True)

        content = '''"""
Dependency validation tests
Auto-generated by CI/CD rehabilitation script
"""

import json
import importlib
import pathlib
import pytest
from typing import List

def get_installed_packages() -> List[str]:
    """Get list of installed packages from requirements.txt"""
    req_file = pathlib.Path("requirements.txt")
    if not req_file.exists():
        return []

    packages = []
    for line in req_file.read_text().splitlines():
        line = line.strip()
        if line and not line.startswith("#"):
            # Extract package name (before any version specifier)
            pkg_name = line.split("==")[0].split(">=")[0].split("<")[0].split("[")[0]
            packages.append(pkg_name.strip())

    return packages


@pytest.mark.parametrize("package", get_installed_packages())
def test_package_importable(package):
    """Test that each package in requirements is importable"""
    # Convert package name to module name
    module_name = package.replace("-", "_")

    # Special cases
    special_cases = {
        "uvicorn": "uvicorn",
        "pydantic_settings": "pydantic_settings",
        "prometheus_client": "prometheus_client",
        "python_dotenv": "dotenv",
    }

    if package in special_cases:
        module_name = special_cases[package]

    # Skip packages that are tools, not importable modules
    skip_packages = ["black", "ruff", "mypy", "pre-commit", "pytest-cov"]
    if package in skip_packages:
        pytest.skip(f"{package} is a tool, not an importable module")

    try:
        importlib.import_module(module_name)
    except ImportError as e:
        pytest.fail(f"Failed to import {module_name} (from package {package}): {e}")


def test_no_missing_mcp_sdk():
    """Ensure no files reference the missing MCP SDK"""
    import subprocess

    result = subprocess.run(
        ["grep", "-r", "anthropic_mcp_python_sdk", ".", "--include=*.py"],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        pytest.fail(f"Found references to missing SDK:\\n{result.stdout}")


def test_uv_lock_exists():
    """Ensure uv.lock file exists"""
    assert pathlib.Path("uv.lock").exists(), "uv.lock file is missing"


def test_requirements_compiled():
    """Ensure requirements.txt is compiled from requirements.in"""
    req_in = pathlib.Path("requirements.in")
    req_txt = pathlib.Path("requirements.txt")

    assert req_in.exists(), "requirements.in is missing"
    assert req_txt.exists(), "requirements.txt is missing"

    # Check that requirements.txt is newer than requirements.in
    assert req_txt.stat().st_mtime >= req_in.stat().st_mtime, \\
        "requirements.txt is older than requirements.in - needs recompilation"
'''

        test_file.write_text(content)
        logger.info(f"Created validation test: {test_file}")

    def generate_report(self) -> dict[str, any]:
        """Generate summary report of fixes"""
        report = {
            "requirements_files_found": len(self.requirements_files),
            "requirements_files_fixed": len(
                [f for f in self.fixed_files if f in self.requirements_files]
            ),
            "import_issues_found": len(self.import_issues),
            "python_files_fixed": len(
                [f for f in self.fixed_files if f.suffix == ".py"]
            ),
            "files_modified": [str(f) for f in sorted(self.fixed_files)],
        }

        return report

    def run(self):
        """Run the complete dependency resolution process"""
        logger.info("Starting dependency resolution...")

        # Step 1: Find all requirements files
        self.find_requirements_files()

        # Step 2: Fix missing packages
        for req_file in self.requirements_files:
            self.fix_missing_package(req_file)

        # Step 3: Create consolidated requirements.in
        self.create_consolidated_requirements()

        # Step 4: Scan for import issues
        self.scan_imports()
        logger.info(f"Found {len(self.import_issues)} import issues")

        # Step 5: Fix imports
        if self.import_issues:
            self.fix_imports()

        # Step 6: Compile with UV
        self.compile_with_uv()

        # Step 7: Create validation tests
        self.create_validation_test()

        # Step 8: Generate report
        report = self.generate_report()

        logger.info("\n" + "=" * 60)
        logger.info("DEPENDENCY RESOLUTION COMPLETE")
        logger.info("=" * 60)
        logger.info(f"Requirements files fixed: {report['requirements_files_fixed']}")
        logger.info(f"Python files fixed: {report['python_files_fixed']}")
        logger.info(f"Total files modified: {len(report['files_modified'])}")

        # Save report
        report_file = (
            self.project_root / ".techdebt" / "dependency_resolution_report.json"
        )
        report_file.parent.mkdir(exist_ok=True)

        import json

        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)

        logger.info(f"\nDetailed report saved to: {report_file}")

        return report


def main():
    """Main entry point"""
    resolver = DependencyResolver()
    report = resolver.run()

    # Run Black and Ruff
    logger.info("\nRunning code formatters...")
    subprocess.run(["ruff", "check", "--fix-only", "."], check=False)
    subprocess.run(["black", "."], check=False)

    sys.exit(0 if len(report["files_modified"]) > 0 else 1)


if __name__ == "__main__":
    main()
