#!/usr/bin/env python3
"""
Development Metrics Generator
Generate comprehensive development metrics and analytics
"""

"""
File Decomposition Plan (auto-generated by Phase 3)
Current size: 769 lines

Recommended decomposition:
- generate_dev_metrics_core.py - Core functionality
- generate_dev_metrics_utils.py - Utility functions
- generate_dev_metrics_models.py - Data models
- generate_dev_metrics_handlers.py - Request handlers

TODO: Implement file decomposition
"""

import json
import logging
import statistics
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

import git

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DevelopmentMetricsGenerator:
    """Generate development metrics and analytics"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.repo = self._get_git_repo()

    def _get_git_repo(self) -> git.Repo:
        """Get Git repository"""
        try:
            return git.Repo(self.project_root)
        except git.InvalidGitRepositoryError:
            raise ValueError(f"Not a git repository: {self.project_root}")

    def get_commit_metrics(self, days: int = 30) -> dict[str, Any]:
        """Get commit-related metrics"""
        logger.info(f"📊 Analyzing commit metrics for last {days} days")

        since_date = datetime.now() - timedelta(days=days)
        commits = list(self.repo.iter_commits(since=since_date))

        metrics = {
            "total_commits": len(commits),
            "commits_per_day": len(commits) / days if days > 0 else 0,
            "authors": {},
            "commit_times": [],
            "commit_messages": [],
            "files_changed": 0,
            "lines_added": 0,
            "lines_deleted": 0,
            "commit_frequency": {},
        }

        for commit in commits:
            # Author metrics
            author = commit.author.name
            if author not in metrics["authors"]:
                metrics["authors"][author] = 0
            metrics["authors"][author] += 1

            # Commit time analysis
            commit_hour = commit.committed_datetime.hour
            metrics["commit_times"].append(commit_hour)

            # Commit message analysis
            message = commit.message.strip()
            metrics["commit_messages"].append(message)

            # File change analysis
            try:
                stats = commit.stats
                metrics["files_changed"] += stats.total["files"]
                metrics["lines_added"] += stats.total["insertions"]
                metrics["lines_deleted"] += stats.total["deletions"]
            except Exception:
                pass  # Skip if stats unavailable

            # Daily frequency
            commit_date = commit.committed_datetime.date().isoformat()
            if commit_date not in metrics["commit_frequency"]:
                metrics["commit_frequency"][commit_date] = 0
            metrics["commit_frequency"][commit_date] += 1

        # Calculate additional metrics
        if metrics["commit_times"]:
            metrics["avg_commit_hour"] = statistics.mean(metrics["commit_times"])
            metrics["most_active_hours"] = self._get_most_frequent(
                metrics["commit_times"]
            )

        metrics["most_active_author"] = (
            max(metrics["authors"].items(), key=lambda x: x[1])[0]
            if metrics["authors"]
            else None
        )

        return metrics

    def get_code_quality_metrics(self) -> dict[str, Any]:
        """Get code quality metrics"""
        logger.info("🎯 Analyzing code quality metrics")

        metrics = {
            "total_files": 0,
            "python_files": 0,
            "javascript_files": 0,
            "lines_of_code": 0,
            "docstring_coverage": 0,
            "type_hint_coverage": 0,
            "test_coverage": 0,
            "complexity_score": 0,
            "file_sizes": [],
            "largest_files": [],
        }

        # Analyze Python files
        python_files = list(self.project_root.rglob("*.py"))
        js_files = list(self.project_root.rglob("*.js")) + list(
            self.project_root.rglob("*.jsx")
        )

        metrics["python_files"] = len(python_files)
        metrics["javascript_files"] = len(js_files)
        metrics["total_files"] = len(python_files) + len(js_files)

        # Analyze Python code quality
        docstring_count = 0
        type_hint_count = 0
        total_functions = 0

        for py_file in python_files:
            try:
                content = py_file.read_text(encoding="utf-8")
                lines = content.split("\n")
                metrics["lines_of_code"] += len(lines)
                metrics["file_sizes"].append(len(lines))

                # Simple docstring detection
                if '"""' in content or "'''" in content:
                    docstring_count += content.count('"""') + content.count("'''")

                # Simple type hint detection
                if ": " in content and "->" in content:
                    type_hint_count += content.count("->") + content.count(": ")

                # Function count
                total_functions += content.count("def ")

                # Track largest files
                if len(lines) > 100:
                    metrics["largest_files"].append(
                        {
                            "file": str(py_file.relative_to(self.project_root)),
                            "lines": len(lines),
                        }
                    )

            except Exception as e:
                logger.warning(f"Could not analyze {py_file}: {e}")

        # Calculate coverage percentages
        if total_functions > 0:
            metrics["docstring_coverage"] = (docstring_count / total_functions) * 100
            metrics["type_hint_coverage"] = (type_hint_count / total_functions) * 100

        # Sort largest files
        metrics["largest_files"].sort(key=lambda x: x["lines"], reverse=True)
        metrics["largest_files"] = metrics["largest_files"][:10]  # Top 10

        # Calculate average file size
        if metrics["file_sizes"]:
            metrics["avg_file_size"] = statistics.mean(metrics["file_sizes"])
            metrics["median_file_size"] = statistics.median(metrics["file_sizes"])

        return metrics

    def get_testing_metrics(self) -> dict[str, Any]:
        """Get testing-related metrics"""
        logger.info("🧪 Analyzing testing metrics")

        metrics = {
            "test_files": 0,
            "test_functions": 0,
            "test_coverage": 0,
            "test_frameworks": [],
            "test_directories": [],
        }

        # Find test files
        test_patterns = ["test_*.py", "*_test.py", "tests.py"]
        test_files = []

        for pattern in test_patterns:
            test_files.extend(self.project_root.rglob(pattern))

        metrics["test_files"] = len(test_files)

        # Analyze test content
        total_test_functions = 0
        frameworks = set()

        for test_file in test_files:
            try:
                content = test_file.read_text(encoding="utf-8")

                # Count test functions
                test_functions = content.count("def test_")
                total_test_functions += test_functions

                # Detect testing frameworks
                if "import pytest" in content or "from pytest" in content:
                    frameworks.add("pytest")
                if "import unittest" in content or "from unittest" in content:
                    frameworks.add("unittest")
                if "import asyncio" in content and "async def test_" in content:
                    frameworks.add("asyncio")

                # Track test directories
                test_dir = str(test_file.parent.relative_to(self.project_root))
                if test_dir not in metrics["test_directories"]:
                    metrics["test_directories"].append(test_dir)

            except Exception as e:
                logger.warning(f"Could not analyze test file {test_file}: {e}")

        metrics["test_functions"] = total_test_functions
        metrics["test_frameworks"] = list(frameworks)

        # Try to get coverage information
        try:
            result = subprocess.run(
                ["coverage", "report", "--show-missing"],
                capture_output=True,
                text=True,
                timeout=30,
            )
            if result.returncode == 0:
                # Parse coverage report
                lines = result.stdout.split("\n")
                for line in lines:
                    if "TOTAL" in line:
                        parts = line.split()
                        if len(parts) >= 4 and "%" in parts[-1]:
                            metrics["test_coverage"] = float(parts[-1].replace("%", ""))
                            break
        except (subprocess.TimeoutExpired, FileNotFoundError):
            logger.info("Coverage tool not available")

        return metrics

    def get_dependency_metrics(self) -> dict[str, Any]:
        """Get dependency-related metrics"""
        logger.info("📦 Analyzing dependency metrics")

        metrics = {
            "python_dependencies": 0,
            "npm_dependencies": 0,
            "outdated_packages": [],
            "security_vulnerabilities": 0,
            "dependency_files": [],
        }

        # Python dependencies
        requirements_files = [
            "requirements.txt",
            "requirements-dev.txt",
            "pyproject.toml",
            "setup.py",
        ]

        for req_file in requirements_files:
            file_path = self.project_root / req_file
            if file_path.exists():
                metrics["dependency_files"].append(req_file)

                if req_file == "requirements.txt":
                    try:
                        content = file_path.read_text()
                        # Count non-empty, non-comment lines
                        deps = [
                            line.strip()
                            for line in content.split("\n")
                            if line.strip() and not line.strip().startswith("#")
                        ]
                        metrics["python_dependencies"] = len(deps)
                    except Exception:
                        pass

        # Node.js dependencies
        package_json = self.project_root / "package.json"
        if package_json.exists():
            metrics["dependency_files"].append("package.json")
            try:
                with open(package_json) as f:
                    package_data = json.load(f)

                deps = package_data.get("dependencies", {})
                dev_deps = package_data.get("devDependencies", {})
                metrics["npm_dependencies"] = len(deps) + len(dev_deps)
            except Exception:
                pass

        # Check for outdated packages (Python)
        try:
            result = subprocess.run(
                [sys.executable, "-m", "pip", "list", "--outdated", "--format=json"],
                capture_output=True,
                text=True,
                timeout=30,
            )
            if result.returncode == 0:
                outdated = json.loads(result.stdout)
                metrics["outdated_packages"] = [pkg["name"] for pkg in outdated]
        except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError):
            pass

        # Security audit (if available)
        try:
            result = subprocess.run(
                ["safety", "check", "--json"],
                capture_output=True,
                text=True,
                timeout=30,
            )
            if result.returncode == 0:
                safety_data = json.loads(result.stdout)
                metrics["security_vulnerabilities"] = len(safety_data)
        except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError):
            pass

        return metrics

    def get_performance_metrics(self) -> dict[str, Any]:
        """Get performance-related metrics"""
        logger.info("⚡ Analyzing performance metrics")

        metrics = {
            "repository_size": 0,
            "largest_files": [],
            "file_count_by_type": {},
            "git_object_count": 0,
            "branch_count": 0,
            "tag_count": 0,
        }

        # Repository size
        try:
            result = subprocess.run(
                ["du", "-sh", str(self.project_root)], capture_output=True, text=True
            )
            if result.returncode == 0:
                size_str = result.stdout.split()[0]
                metrics["repository_size"] = size_str
        except (subprocess.CalledProcessError, FileNotFoundError):
            pass

        # File analysis
        file_types = {}
        all_files = list(self.project_root.rglob("*"))

        for file_path in all_files:
            if file_path.is_file():
                suffix = file_path.suffix.lower()
                if suffix not in file_types:
                    file_types[suffix] = 0
                file_types[suffix] += 1

                # Track large files
                try:
                    size = file_path.stat().st_size
                    if size > 100000:  # Files larger than 100KB
                        metrics["largest_files"].append(
                            {
                                "file": str(file_path.relative_to(self.project_root)),
                                "size": size,
                            }
                        )
                except OSError:
                    pass

        metrics["file_count_by_type"] = dict(
            sorted(file_types.items(), key=lambda x: x[1], reverse=True)
        )

        # Sort largest files
        metrics["largest_files"].sort(key=lambda x: x["size"], reverse=True)
        metrics["largest_files"] = metrics["largest_files"][:10]  # Top 10

        # Git metrics
        try:
            metrics["branch_count"] = len(list(self.repo.branches))
            metrics["tag_count"] = len(list(self.repo.tags))
        except Exception:
            pass

        return metrics

    def get_mcp_integration_metrics(self) -> dict[str, Any]:
        """Get MCP integration metrics"""
        logger.info("🔌 Analyzing MCP integration metrics")

        metrics = {
            "mcp_servers_configured": 0,
            "mcp_config_valid": False,
            "server_types": [],
            "auto_triggers_enabled": False,
            "workflow_automation": False,
        }

        # Check MCP configuration
        mcp_config_file = self.project_root / "cursor_mcp_config.json"
        if mcp_config_file.exists():
            try:
                with open(mcp_config_file) as f:
                    mcp_config = json.load(f)

                servers = mcp_config.get("mcpServers", {})
                metrics["mcp_servers_configured"] = len(servers)
                metrics["server_types"] = list(servers.keys())
                metrics["mcp_config_valid"] = True

                # Check for auto-triggers
                for server_config in servers.values():
                    if server_config.get("auto_triggers"):
                        metrics["auto_triggers_enabled"] = True
                        break

                # Check for workflow automation
                if mcp_config.get("workflow_automation"):
                    metrics["workflow_automation"] = True

            except Exception as e:
                logger.warning(f"Could not parse MCP config: {e}")

        return metrics

    def get_github_integration_metrics(self) -> dict[str, Any]:
        """Get GitHub integration metrics"""
        logger.info("🔗 Analyzing GitHub integration metrics")

        metrics = {
            "workflows_configured": 0,
            "workflow_files": [],
            "github_actions_enabled": False,
            "pr_templates": False,
            "issue_templates": False,
            "branch_protection": False,
        }

        # Check GitHub workflows
        workflows_dir = self.project_root / ".github" / "workflows"
        if workflows_dir.exists():
            workflow_files = list(workflows_dir.glob("*.yml")) + list(
                workflows_dir.glob("*.yaml")
            )
            metrics["workflows_configured"] = len(workflow_files)
            metrics["workflow_files"] = [f.name for f in workflow_files]
            metrics["github_actions_enabled"] = len(workflow_files) > 0

        # Check templates
        github_dir = self.project_root / ".github"
        if github_dir.exists():
            if (github_dir / "PULL_REQUEST_TEMPLATE.md").exists():
                metrics["pr_templates"] = True

            issue_templates_dir = github_dir / "ISSUE_TEMPLATE"
            if issue_templates_dir.exists() and list(issue_templates_dir.glob("*.md")):
                metrics["issue_templates"] = True

        return metrics

    def _get_most_frequent(self, items: list) -> list:
        """Get most frequent items from a list"""
        if not items:
            return []

        frequency = {}
        for item in items:
            frequency[item] = frequency.get(item, 0) + 1

        max_freq = max(frequency.values())
        return [item for item, freq in frequency.items() if freq == max_freq]

    def generate_comprehensive_metrics(self) -> dict[str, Any]:
        """Generate comprehensive development metrics"""
        logger.info("📊 Generating comprehensive development metrics")

        metrics = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(self.project_root),
            "git_branch": (
                self.repo.active_branch.name if self.repo.active_branch else "unknown"
            ),
            "commit_metrics": self.get_commit_metrics(),
            "code_quality_metrics": self.get_code_quality_metrics(),
            "testing_metrics": self.get_testing_metrics(),
            "dependency_metrics": self.get_dependency_metrics(),
            "performance_metrics": self.get_performance_metrics(),
            "mcp_integration_metrics": self.get_mcp_integration_metrics(),
            "github_integration_metrics": self.get_github_integration_metrics(),
        }

        # Calculate overall scores
        metrics["overall_scores"] = self._calculate_overall_scores(metrics)

        return metrics

    def _calculate_overall_scores(self, metrics: dict[str, Any]) -> dict[str, Any]:
        """Calculate overall development scores"""
        scores = {
            "development_velocity": 0,
            "code_quality": 0,
            "testing_maturity": 0,
            "integration_completeness": 0,
            "overall_health": 0,
        }

        # Development velocity (based on commits and activity)
        commit_metrics = metrics["commit_metrics"]
        if commit_metrics["commits_per_day"] >= 1:
            scores["development_velocity"] = min(
                100, commit_metrics["commits_per_day"] * 20
            )

        # Code quality (based on various factors)
        quality_metrics = metrics["code_quality_metrics"]
        quality_score = 0
        if quality_metrics["docstring_coverage"] > 50:
            quality_score += 30
        if quality_metrics["type_hint_coverage"] > 50:
            quality_score += 30
        if quality_metrics["avg_file_size"] < 200:  # Reasonable file size
            quality_score += 20
        if len(quality_metrics["largest_files"]) < 5:  # Not too many large files
            quality_score += 20
        scores["code_quality"] = quality_score

        # Testing maturity
        testing_metrics = metrics["testing_metrics"]
        testing_score = 0
        if testing_metrics["test_files"] > 0:
            testing_score += 40
        if testing_metrics["test_coverage"] > 80:
            testing_score += 40
        elif testing_metrics["test_coverage"] > 50:
            testing_score += 20
        if len(testing_metrics["test_frameworks"]) > 0:
            testing_score += 20
        scores["testing_maturity"] = testing_score

        # Integration completeness
        mcp_metrics = metrics["mcp_integration_metrics"]
        github_metrics = metrics["github_integration_metrics"]
        integration_score = 0

        if mcp_metrics["mcp_servers_configured"] > 0:
            integration_score += 30
        if mcp_metrics["auto_triggers_enabled"]:
            integration_score += 20
        if github_metrics["workflows_configured"] > 0:
            integration_score += 30
        if github_metrics["pr_templates"] and github_metrics["issue_templates"]:
            integration_score += 20

        scores["integration_completeness"] = integration_score

        # Overall health (average of all scores)
        all_scores = [
            scores["development_velocity"],
            scores["code_quality"],
            scores["testing_maturity"],
            scores["integration_completeness"],
        ]
        scores["overall_health"] = sum(all_scores) / len(all_scores)

        return scores

    def generate_metrics_report(self, metrics: dict[str, Any]) -> str:
        """Generate comprehensive metrics report"""
        report = []
        report.append("# 📊 Development Metrics Report")
        report.append("")
        report.append(f"**Generated**: {metrics['timestamp']}")
        report.append(f"**Project**: {metrics['project_root']}")
        report.append(f"**Branch**: {metrics['git_branch']}")
        report.append("")

        # Overall Scores
        scores = metrics["overall_scores"]
        report.append("## 🎯 Overall Health Scores")
        report.append(f"- **Overall Health**: {scores['overall_health']:.1f}/100")
        report.append(
            f"- **Development Velocity**: {scores['development_velocity']:.1f}/100"
        )
        report.append(f"- **Code Quality**: {scores['code_quality']:.1f}/100")
        report.append(f"- **Testing Maturity**: {scores['testing_maturity']:.1f}/100")
        report.append(
            f"- **Integration Completeness**: {scores['integration_completeness']:.1f}/100"
        )
        report.append("")

        # Commit Metrics
        commit_metrics = metrics["commit_metrics"]
        report.append("## 📝 Commit Activity")
        report.append(
            f"- **Total Commits (30 days)**: {commit_metrics['total_commits']}"
        )
        report.append(f"- **Commits per Day**: {commit_metrics['commits_per_day']:.1f}")
        report.append(
            f"- **Most Active Author**: {commit_metrics['most_active_author']}"
        )
        report.append(f"- **Lines Added**: {commit_metrics['lines_added']:,}")
        report.append(f"- **Lines Deleted**: {commit_metrics['lines_deleted']:,}")
        report.append("")

        # Code Quality
        quality_metrics = metrics["code_quality_metrics"]
        report.append("## 🎯 Code Quality")
        report.append(f"- **Total Files**: {quality_metrics['total_files']}")
        report.append(f"- **Lines of Code**: {quality_metrics['lines_of_code']:,}")
        report.append(
            f"- **Docstring Coverage**: {quality_metrics['docstring_coverage']:.1f}%"
        )
        report.append(
            f"- **Type Hint Coverage**: {quality_metrics['type_hint_coverage']:.1f}%"
        )
        report.append(
            f"- **Average File Size**: {quality_metrics.get('avg_file_size', 0):.0f} lines"
        )
        report.append("")

        # Testing
        testing_metrics = metrics["testing_metrics"]
        report.append("## 🧪 Testing")
        report.append(f"- **Test Files**: {testing_metrics['test_files']}")
        report.append(f"- **Test Functions**: {testing_metrics['test_functions']}")
        report.append(f"- **Test Coverage**: {testing_metrics['test_coverage']:.1f}%")
        report.append(
            f"- **Test Frameworks**: {', '.join(testing_metrics['test_frameworks'])}"
        )
        report.append("")

        # Dependencies
        dep_metrics = metrics["dependency_metrics"]
        report.append("## 📦 Dependencies")
        report.append(
            f"- **Python Dependencies**: {dep_metrics['python_dependencies']}"
        )
        report.append(f"- **NPM Dependencies**: {dep_metrics['npm_dependencies']}")
        report.append(
            f"- **Outdated Packages**: {len(dep_metrics['outdated_packages'])}"
        )
        report.append(
            f"- **Security Vulnerabilities**: {dep_metrics['security_vulnerabilities']}"
        )
        report.append("")

        # MCP Integration
        mcp_metrics = metrics["mcp_integration_metrics"]
        report.append("## 🔌 MCP Integration")
        report.append(
            f"- **Servers Configured**: {mcp_metrics['mcp_servers_configured']}"
        )
        report.append(f"- **Server Types**: {', '.join(mcp_metrics['server_types'])}")
        report.append(
            f"- **Auto-triggers**: {'✅' if mcp_metrics['auto_triggers_enabled'] else '❌'}"
        )
        report.append(
            f"- **Workflow Automation**: {'✅' if mcp_metrics['workflow_automation'] else '❌'}"
        )
        report.append("")

        # GitHub Integration
        github_metrics = metrics["github_integration_metrics"]
        report.append("## 🔗 GitHub Integration")
        report.append(f"- **Workflows**: {github_metrics['workflows_configured']}")
        report.append(
            f"- **GitHub Actions**: {'✅' if github_metrics['github_actions_enabled'] else '❌'}"
        )
        report.append(
            f"- **PR Templates**: {'✅' if github_metrics['pr_templates'] else '❌'}"
        )
        report.append(
            f"- **Issue Templates**: {'✅' if github_metrics['issue_templates'] else '❌'}"
        )
        report.append("")

        # Recommendations
        report.append("## 🎯 Recommendations")

        if scores["code_quality"] < 70:
            report.append(
                "- 📝 Improve code quality by adding docstrings and type hints"
            )

        if scores["testing_maturity"] < 70:
            report.append("- 🧪 Increase test coverage and add more test files")

        if scores["development_velocity"] < 50:
            report.append("- 🚀 Increase development activity and commit frequency")

        if scores["integration_completeness"] < 70:
            report.append(
                "- 🔌 Complete MCP server setup and GitHub workflow configuration"
            )

        if dep_metrics["security_vulnerabilities"] > 0:
            report.append("- 🔒 Address security vulnerabilities in dependencies")

        if len(dep_metrics["outdated_packages"]) > 5:
            report.append("- 📦 Update outdated packages")

        return "\n".join(report)


def main():
    """Main metrics generation function"""
    import argparse

    parser = argparse.ArgumentParser(description="Generate Development Metrics")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    parser.add_argument("--output", help="Output file for metrics")
    parser.add_argument(
        "--format",
        choices=["json", "markdown"],
        default="markdown",
        help="Output format",
    )
    parser.add_argument(
        "--days", type=int, default=30, help="Days to analyze for commit metrics"
    )

    args = parser.parse_args()

    try:
        generator = DevelopmentMetricsGenerator(args.project_root)

        # Generate comprehensive metrics
        metrics = generator.generate_comprehensive_metrics()

        if args.format == "json":
            output = json.dumps(metrics, indent=2, default=str)
        else:
            output = generator.generate_metrics_report(metrics)

        if args.output:
            with open(args.output, "w") as f:
                f.write(output)
            logger.info(f"📊 Metrics saved to {args.output}")
        else:
            pass

        # Print summary
        metrics["overall_scores"]

        return 0

    except Exception as e:
        logger.error(f"❌ Metrics generation failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
