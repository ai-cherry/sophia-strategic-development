#!/usr/bin/env python3
"""
Validate GH200 GPU Deployment on Lambda Labs
Checks actual deployment against PR specifications
"""

import json
import os
import subprocess
import sys
from datetime import datetime
from typing import Any, Dict, List, Optional

import requests


class GH200DeploymentValidator:
    def __init__(self):
        self.deployment_report = {
            "master_ip": "192.222.50.155",
            "worker1_ip": "192.222.51.100",
            "worker2_ip": "TBD",  # Still booting per report
            "gpu_type": "GH200",
            "gpu_memory_per_instance": "96GB",
            "total_gpu_memory": "288GB",
            "cost_per_hour": 1.49,
            "monthly_cost": 3217,
            "region": "us-east-3",
        }

        self.pr136_specs = {
            "gpu_type": "H200",
            "gpu_memory_per_instance": "141GB",
            "memory_pools": {
                "active_models": "60GB",
                "inference_cache": "40GB",
                "vector_cache": "30GB",
                "buffer": "11GB",
            },
        }

        self.validation_results = {
            "discrepancies": [],
            "recommendations": [],
            "files_to_update": [],
            "memory_adjustments": {},
        }

    def validate_gpu_specifications(self):
        """Validate GPU specifications between PR and deployment"""
        print("üîç Validating GPU Specifications...")

        # Check GPU type mismatch
        if self.pr136_specs["gpu_type"] != self.deployment_report["gpu_type"]:
            self.validation_results["discrepancies"].append(
                {
                    "type": "GPU Model",
                    "pr136": self.pr136_specs["gpu_type"],
                    "actual": self.deployment_report["gpu_type"],
                    "impact": "All H200 references need to be updated to GH200",
                }
            )

        # Check memory mismatch
        pr_memory = self.pr136_specs["gpu_memory_per_instance"]
        actual_memory = self.deployment_report["gpu_memory_per_instance"]

        if pr_memory != actual_memory:
            self.validation_results["discrepancies"].append(
                {
                    "type": "GPU Memory",
                    "pr136": pr_memory,
                    "actual": actual_memory,
                    "impact": f"Memory reduced by {int(141-96)/141*100:.1f}%",
                }
            )

    def calculate_memory_pool_adjustments(self):
        """Calculate adjusted memory pools for GH200 (96GB)"""
        print("üìä Calculating Memory Pool Adjustments...")

        # Original H200 allocations (141GB total)
        h200_pools = self.pr136_specs["memory_pools"]
        h200_total = 141  # GB

        # Calculate proportional adjustments for GH200 (96GB)
        gh200_total = 96  # GB
        scaling_factor = gh200_total / h200_total

        gh200_pools = {}
        for pool_name, size_str in h200_pools.items():
            size_gb = int(size_str.replace("GB", ""))
            adjusted_size = int(size_gb * scaling_factor)
            gh200_pools[pool_name] = f"{adjusted_size}GB"

        # Ensure total equals 96GB (adjust buffer if needed)
        total_allocated = sum(int(v.replace("GB", "")) for v in gh200_pools.values())
        if total_allocated != gh200_total:
            buffer_adjustment = (
                gh200_total
                - total_allocated
                + int(gh200_pools["buffer"].replace("GB", ""))
            )
            gh200_pools["buffer"] = f"{buffer_adjustment}GB"

        self.validation_results["memory_adjustments"] = {
            "original_h200": h200_pools,
            "adjusted_gh200": gh200_pools,
            "scaling_factor": f"{scaling_factor:.2f}",
        }

    def identify_files_to_update(self):
        """Identify all files that need H200 ‚Üí GH200 updates"""
        print("üìÅ Identifying Files to Update...")

        files_to_check = [
            ("Dockerfile.h200", "Dockerfile.gh200"),
            ("requirements-h200.txt", "requirements-gh200.txt"),
            ("infrastructure/enhanced_lambda_labs_provisioner.py", None),
            ("backend/core/enhanced_memory_architecture.py", None),
            (
                "infrastructure/pulumi/enhanced-h200-stack.ts",
                "infrastructure/pulumi/enhanced-gh200-stack.ts",
            ),
            ("infrastructure/ENHANCED_LAMBDA_LABS_SETUP_GUIDE.md", None),
            ("ENHANCED_INFRASTRUCTURE_IMPLEMENTATION_REPORT.md", None),
            ("docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md", None),
        ]

        for current_name, new_name in files_to_check:
            update_info = {
                "current": current_name,
                "action": "rename" if new_name else "update",
                "new_name": new_name,
                "changes": [
                    "Replace 'H200' with 'GH200'",
                    "Replace 'h200' with 'gh200'",
                    "Replace '141GB' with '96GB'",
                    "Update memory pool allocations",
                ],
            }
            self.validation_results["files_to_update"].append(update_info)

    def generate_update_script(self):
        """Generate script to update all H200 references to GH200"""
        print("üîß Generating Update Script...")

        script_content = '''#!/usr/bin/env python3
"""
Update all H200 references to GH200 to match actual deployment
Generated by GH200 Deployment Validator
"""

import os
import re
import shutil
from pathlib import Path

def update_h200_to_gh200():
    """Update all H200 references to GH200"""

    # Files to rename
    renames = [
        ("Dockerfile.h200", "Dockerfile.gh200"),
        ("requirements-h200.txt", "requirements-gh200.txt"),
        ("infrastructure/pulumi/enhanced-h200-stack.ts", "infrastructure/pulumi/enhanced-gh200-stack.ts")
    ]

    # Perform renames
    for old_name, new_name in renames:
        if os.path.exists(old_name):
            shutil.move(old_name, new_name)
            print(f"‚úÖ Renamed {old_name} ‚Üí {new_name}")

    # Files to update content
    files_to_update = [
        "Dockerfile.gh200",
        "requirements-gh200.txt",
        "infrastructure/enhanced_lambda_labs_provisioner.py",
        "backend/core/enhanced_memory_architecture.py",
        "infrastructure/pulumi/enhanced-gh200-stack.ts",
        "infrastructure/ENHANCED_LAMBDA_LABS_SETUP_GUIDE.md",
        "ENHANCED_INFRASTRUCTURE_IMPLEMENTATION_REPORT.md",
        "docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md"
    ]

    # Replacement patterns
    replacements = [
        (r'H200', 'GH200'),
        (r'h200', 'gh200'),
        (r'141GB', '96GB'),
        (r'141\\s*GB', '96 GB'),
        (r'gpu_1x_h200', 'gpu_1x_gh200'),
        (r'active_models:\\s*str\\s*=\\s*"60GB"', 'active_models: str = "40GB"'),
        (r'inference_cache:\\s*str\\s*=\\s*"40GB"', 'inference_cache: str = "30GB"'),
        (r'vector_cache:\\s*str\\s*=\\s*"30GB"', 'vector_cache: str = "20GB"'),
        (r'buffer:\\s*str\\s*=\\s*"11GB"', 'buffer: str = "6GB"'),
    ]

    # Apply replacements
    for file_path in files_to_update:
        if os.path.exists(file_path):
            print(f"üìù Updating {file_path}...")

            with open(file_path, 'r') as f:
                content = f.read()

            original_content = content
            for pattern, replacement in replacements:
                content = re.sub(pattern, replacement, content)

            if content != original_content:
                with open(file_path, 'w') as f:
                    f.write(content)
                print(f"‚úÖ Updated {file_path}")
            else:
                print(f"‚ÑπÔ∏è No changes needed in {file_path}")

    print("\\n‚úÖ All H200 ‚Üí GH200 updates completed!")

if __name__ == "__main__":
    update_h200_to_gh200()
'''

        with open("scripts/update_h200_to_gh200.py", "w") as f:
            f.write(script_content)

        os.chmod("scripts/update_h200_to_gh200.py", 0o755)
        print("‚úÖ Created update script: scripts/update_h200_to_gh200.py")

    def generate_recommendations(self):
        """Generate actionable recommendations"""
        print("üí° Generating Recommendations...")

        self.validation_results["recommendations"] = [
            {
                "priority": "HIGH",
                "action": "Merge PR #137",
                "reason": "Contains critical fixes that are still valid",
            },
            {
                "priority": "HIGH",
                "action": "Run update_h200_to_gh200.py script",
                "reason": "Align codebase with actual GH200 deployment",
            },
            {
                "priority": "MEDIUM",
                "action": "Create PR #139 with GH200 updates",
                "reason": "Document the GPU model change officially",
            },
            {
                "priority": "MEDIUM",
                "action": "Update monitoring dashboards",
                "reason": "Adjust GPU memory thresholds from 141GB to 96GB",
            },
            {
                "priority": "LOW",
                "action": "Update cost projections",
                "reason": "GH200 is 40% cheaper than expected",
            },
        ]

    def generate_report(self):
        """Generate comprehensive validation report"""
        print("\n" + "=" * 60)
        print("üìä GH200 DEPLOYMENT VALIDATION REPORT")
        print("=" * 60)

        # Discrepancies
        print("\nüö® DISCREPANCIES FOUND:")
        for disc in self.validation_results["discrepancies"]:
            print(f"\n  ‚Ä¢ {disc['type']}:")
            print(f"    - PR #136: {disc['pr136']}")
            print(f"    - Actual: {disc['actual']}")
            print(f"    - Impact: {disc['impact']}")

        # Memory Adjustments
        print("\nüíæ MEMORY POOL ADJUSTMENTS:")
        if self.validation_results["memory_adjustments"]:
            adj = self.validation_results["memory_adjustments"]
            print(f"\n  Scaling Factor: {adj['scaling_factor']}")
            print("\n  Original H200 Allocation (141GB):")
            for pool, size in adj["original_h200"].items():
                print(f"    - {pool}: {size}")
            print("\n  Adjusted GH200 Allocation (96GB):")
            for pool, size in adj["adjusted_gh200"].items():
                print(f"    - {pool}: {size}")

        # Files to Update
        print("\nüìÅ FILES TO UPDATE:")
        for file_info in self.validation_results["files_to_update"]:
            action = "Rename to" if file_info["action"] == "rename" else "Update"
            print(f"\n  ‚Ä¢ {file_info['current']}")
            if file_info["new_name"]:
                print(f"    - {action}: {file_info['new_name']}")
            print("    - Changes needed:")
            for change in file_info["changes"]:
                print(f"      - {change}")

        # Recommendations
        print("\nüí° RECOMMENDATIONS:")
        for rec in self.validation_results["recommendations"]:
            emoji = (
                "üî¥"
                if rec["priority"] == "HIGH"
                else "üü°"
                if rec["priority"] == "MEDIUM"
                else "üü¢"
            )
            print(f"\n  {emoji} [{rec['priority']}] {rec['action']}")
            print(f"     Reason: {rec['reason']}")

        print("\n" + "=" * 60)
        print("‚úÖ Validation Complete!")
        print("=" * 60)

    def save_validation_results(self):
        """Save validation results to JSON file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"gh200_validation_results_{timestamp}.json"

        with open(filename, "w") as f:
            json.dump(
                {
                    "timestamp": timestamp,
                    "deployment_report": self.deployment_report,
                    "pr136_specs": self.pr136_specs,
                    "validation_results": self.validation_results,
                },
                f,
                indent=2,
            )

        print(f"\nüìÑ Results saved to: {filename}")

    def run_validation(self):
        """Run complete validation process"""
        print("üöÄ Starting GH200 Deployment Validation...\n")

        self.validate_gpu_specifications()
        self.calculate_memory_pool_adjustments()
        self.identify_files_to_update()
        self.generate_update_script()
        self.generate_recommendations()
        self.generate_report()
        self.save_validation_results()


if __name__ == "__main__":
    validator = GH200DeploymentValidator()
    validator.run_validation()
