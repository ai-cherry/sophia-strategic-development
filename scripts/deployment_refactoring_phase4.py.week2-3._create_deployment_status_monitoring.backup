#!/usr/bin/env python3
"""
Deployment Refactoring Phase 4: Monitoring & Observability
Implements comprehensive health monitoring, performance analytics, and alerting
"""

import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Any

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeploymentRefactoringPhase4:
    """Phase 4: Monitoring and observability implementation"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.files_created = []
        self.errors = []

    async def execute_phase4(self) -> dict[str, Any]:
        """Execute Phase 4 refactoring"""
        logger.info("üöÄ Starting Phase 4: Monitoring & Observability")

        results = {
            "phase": "Phase 4 - Monitoring & Observability",
            "start_time": datetime.now().isoformat(),
            "tasks_completed": [],
            "files_created": [],
            "errors": [],
            "success": False,
        }

        try:
            # Task 1: Create comprehensive health monitoring
            await self._create_comprehensive_health_monitoring()
            results["tasks_completed"].append("comprehensive_health_monitoring")

            # Task 2: Create performance analytics
            await self._create_performance_analytics()
            results["tasks_completed"].append("performance_analytics")

            # Task 3: Create alerting system
            await self._create_alerting_system()
            results["tasks_completed"].append("alerting_system")

            # Task 4: Create deployment status monitoring
            await self._create_deployment_status_monitoring()
            results["tasks_completed"].append("deployment_status_monitoring")

            results["files_created"] = self.files_created
            results["errors"] = self.errors
            results["success"] = len(self.errors) == 0
            results["end_time"] = datetime.now().isoformat()

            logger.info(
                f"‚úÖ Phase 4 completed! Created {len(self.files_created)} files"
            )

        except Exception as e:
            error_msg = f"Phase 4 execution failed: {e}"
            logger.error(error_msg)
            results["errors"].append(error_msg)
            results["success"] = False

        return results

    async def _create_comprehensive_health_monitoring(self):
        """Create comprehensive health monitoring system"""
        logger.info("üè• Creating comprehensive health monitoring...")

        monitoring_code = '''#!/usr/bin/env python3
"""
Comprehensive Health Monitoring System for Sophia AI
Real-time system health tracking with predictive alerting
"""

import asyncio
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class HealthStatus(Enum):
    """System health status levels"""
    HEALTHY = "healthy"
    WARNING = "warning"
    CRITICAL = "critical"
    UNKNOWN = "unknown"


@dataclass
class HealthMetric:
    """Individual health metric"""
    name: str
    value: float
    threshold_warning: float
    threshold_critical: float
    unit: str
    timestamp: datetime

    @property
    def status(self) -> HealthStatus:
        if self.value >= self.threshold_critical:
            return HealthStatus.CRITICAL
        elif self.value >= self.threshold_warning:
            return HealthStatus.WARNING
        else:
            return HealthStatus.HEALTHY


class ComprehensiveHealthMonitor:
    """Production-grade health monitoring with predictive capabilities"""

    def __init__(self):
        self.metrics_history: Dict[str, List[HealthMetric]] = {}
        self.alert_callbacks = []
        self.monitoring_enabled = True

    async def start_monitoring(self):
        """Start continuous health monitoring"""
        logger.info("üöÄ Starting comprehensive health monitoring")

        while self.monitoring_enabled:
            try:
                # Collect all health metrics
                health_data = await self.collect_all_metrics()

                # Store metrics history
                await self._store_metrics_history(health_data)

                # Check for alerts
                await self._check_alerts(health_data)

                # Wait before next check
                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                logger.error(f"Health monitoring error: {e}")
                await asyncio.sleep(60)  # Longer wait on error

    async def collect_all_metrics(self) -> Dict[str, HealthMetric]:
        """Collect comprehensive system health metrics"""

        metrics = {}

        # System metrics
        system_metrics = await self._collect_system_metrics()
        metrics.update(system_metrics)

        # Service metrics
        service_metrics = await self._collect_service_metrics()
        metrics.update(service_metrics)

        # Database metrics
        db_metrics = await self._collect_database_metrics()
        metrics.update(db_metrics)

        # Performance metrics
        perf_metrics = await self._collect_performance_metrics()
        metrics.update(perf_metrics)

        return metrics

    async def _collect_system_metrics(self) -> Dict[str, HealthMetric]:
        """Collect system-level health metrics"""
        now = datetime.now()

        return {
            "cpu_usage": HealthMetric(
                name="CPU Usage",
                value=45.2,  # Simulated
                threshold_warning=70.0,
                threshold_critical=90.0,
                unit="%",
                timestamp=now
            ),
            "memory_usage": HealthMetric(
                name="Memory Usage",
                value=62.8,  # Simulated
                threshold_warning=80.0,
                threshold_critical=95.0,
                unit="%",
                timestamp=now
            ),
            "disk_usage": HealthMetric(
                name="Disk Usage",
                value=35.5,  # Simulated
                threshold_warning=80.0,
                threshold_critical=95.0,
                unit="%",
                timestamp=now
            )
        }

    async def _collect_service_metrics(self) -> Dict[str, HealthMetric]:
        """Collect service health metrics"""
        now = datetime.now()

        return {
            "api_response_time": HealthMetric(
                name="API Response Time",
                value=125.5,  # Simulated
                threshold_warning=200.0,
                threshold_critical=500.0,
                unit="ms",
                timestamp=now
            ),
            "error_rate": HealthMetric(
                name="Error Rate",
                value=0.2,  # Simulated
                threshold_warning=1.0,
                threshold_critical=5.0,
                unit="%",
                timestamp=now
            )
        }

    async def _collect_database_metrics(self) -> Dict[str, HealthMetric]:
        """Collect database health metrics"""
        now = datetime.now()

        return {
            "db_connections": HealthMetric(
                name="Database Connections",
                value=15,  # Simulated
                threshold_warning=80,
                threshold_critical=95,
                unit="count",
                timestamp=now
            ),
            "db_query_time": HealthMetric(
                name="Database Query Time",
                value=45.2,  # Simulated
                threshold_warning=100.0,
                threshold_critical=500.0,
                unit="ms",
                timestamp=now
            )
        }

    async def _collect_performance_metrics(self) -> Dict[str, HealthMetric]:
        """Collect performance metrics"""
        now = datetime.now()

        return {
            "cache_hit_rate": HealthMetric(
                name="Cache Hit Rate",
                value=85.5,  # Simulated
                threshold_warning=70.0,  # Lower is warning
                threshold_critical=50.0,  # Lower is critical
                unit="%",
                timestamp=now
            )
        }

    async def _store_metrics_history(self, metrics: Dict[str, HealthMetric]):
        """Store metrics for trend analysis"""
        for metric_name, metric in metrics.items():
            if metric_name not in self.metrics_history:
                self.metrics_history[metric_name] = []

            self.metrics_history[metric_name].append(metric)

            # Keep only last 24 hours of data
            cutoff_time = datetime.now() - timedelta(hours=24)
            self.metrics_history[metric_name] = [
                m for m in self.metrics_history[metric_name]
                if m.timestamp > cutoff_time
            ]

    async def _check_alerts(self, metrics: Dict[str, HealthMetric]):
        """Check for alert conditions"""
        critical_metrics = []
        warning_metrics = []

        for metric_name, metric in metrics.items():
            if metric.status == HealthStatus.CRITICAL:
                critical_metrics.append(metric)
            elif metric.status == HealthStatus.WARNING:
                warning_metrics.append(metric)

        # Send alerts
        if critical_metrics:
            await self._send_critical_alert(critical_metrics)

        if warning_metrics:
            await self._send_warning_alert(warning_metrics)

    async def _send_critical_alert(self, metrics: List[HealthMetric]):
        """Send critical alert"""
        logger.critical(f"üö® CRITICAL ALERT: {len(metrics)} metrics in critical state")
        for metric in metrics:
            logger.critical(f"   ‚Ä¢ {metric.name}: {metric.value}{metric.unit}")

    async def _send_warning_alert(self, metrics: List[HealthMetric]):
        """Send warning alert"""
        logger.warning(f"‚ö†Ô∏è WARNING: {len(metrics)} metrics in warning state")
        for metric in metrics:
            logger.warning(f"   ‚Ä¢ {metric.name}: {metric.value}{metric.unit}")

    def get_health_summary(self) -> Dict[str, Any]:
        """Get overall health summary"""
        if not self.metrics_history:
            return {"status": "unknown", "message": "No metrics available"}

        # Get latest metrics
        latest_metrics = {}
        for metric_name, history in self.metrics_history.items():
            if history:
                latest_metrics[metric_name] = history[-1]

        # Determine overall status
        critical_count = sum(1 for m in latest_metrics.values() if m.status == HealthStatus.CRITICAL)
        warning_count = sum(1 for m in latest_metrics.values() if m.status == HealthStatus.WARNING)

        if critical_count > 0:
            overall_status = HealthStatus.CRITICAL
        elif warning_count > 0:
            overall_status = HealthStatus.WARNING
        else:
            overall_status = HealthStatus.HEALTHY

        return {
            "overall_status": overall_status.value,
            "critical_metrics": critical_count,
            "warning_metrics": warning_count,
            "total_metrics": len(latest_metrics),
            "last_updated": datetime.now().isoformat(),
            "metrics": {
                name: {
                    "value": metric.value,
                    "unit": metric.unit,
                    "status": metric.status.value
                }
                for name, metric in latest_metrics.items()
            }
        }


# Global instance
comprehensive_health_monitor = ComprehensiveHealthMonitor()
'''

        # Write comprehensive health monitoring
        health_file = (
            self.project_root
            / "backend"
            / "monitoring"
            / "comprehensive_health_monitor.py"
        )
        health_file.parent.mkdir(exist_ok=True)
        health_file.write_text(monitoring_code)
        self.files_created.append(str(health_file))

        logger.info("‚úÖ Comprehensive health monitoring created")

    async def _create_performance_analytics(self):
        """Create performance analytics system"""
        logger.info("üìà Creating performance analytics...")

        analytics_code = '''#!/usr/bin/env python3
"""
Performance Analytics System for Sophia AI
Advanced performance tracking with trend analysis and optimization recommendations
"""

import asyncio
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetric:
    """Performance metric with trend data"""
    name: str
    current_value: float
    previous_value: float
    target_value: float
    unit: str
    timestamp: datetime

    @property
    def trend_percentage(self) -> float:
        if self.previous_value == 0:
            return 0.0
        return ((self.current_value - self.previous_value) / self.previous_value) * 100

    @property
    def target_achievement(self) -> float:
        if self.target_value == 0:
            return 100.0
        return (self.current_value / self.target_value) * 100


class PerformanceAnalytics:
    """Advanced performance analytics with optimization insights"""

    def __init__(self):
        self.performance_history = {}
        self.sla_targets = {
            "api_response_time_ms": 200,
            "database_query_time_ms": 100,
            "cache_hit_rate_percent": 85,
            "system_uptime_percent": 99.9,
            "error_rate_percent": 0.1
        }

    async def collect_performance_data(self) -> Dict[str, PerformanceMetric]:
        """Collect comprehensive performance metrics"""

        current_data = {
            "api_response_time_ms": 125.5,
            "database_query_time_ms": 45.2,
            "cache_hit_rate_percent": 87.3,
            "system_uptime_percent": 99.95,
            "error_rate_percent": 0.05
        }

        metrics = {}
        now = datetime.now()

        for metric_name, current_value in current_data.items():
            # Get previous value from history
            previous_value = self._get_previous_value(metric_name)
            target_value = self.sla_targets.get(metric_name, 0)

            metric = PerformanceMetric(
                name=metric_name,
                current_value=current_value,
                previous_value=previous_value,
                target_value=target_value,
                unit=self._get_metric_unit(metric_name),
                timestamp=now
            )

            metrics[metric_name] = metric

            # Store in history
            self._store_metric_history(metric_name, current_value, now)

        return metrics

    def _get_previous_value(self, metric_name: str) -> float:
        """Get previous value for trend calculation"""
        if metric_name not in self.performance_history:
            return 0.0

        history = self.performance_history[metric_name]
        if len(history) < 2:
            return 0.0

        return history[-2]["value"]

    def _get_metric_unit(self, metric_name: str) -> str:
        """Get unit for metric"""
        if "time" in metric_name or "_ms" in metric_name:
            return "ms"
        elif "percent" in metric_name or "rate" in metric_name:
            return "%"
        else:
            return "count"

    def _store_metric_history(self, metric_name: str, value: float, timestamp: datetime):
        """Store metric in history"""
        if metric_name not in self.performance_history:
            self.performance_history[metric_name] = []

        self.performance_history[metric_name].append({
            "value": value,
            "timestamp": timestamp
        })

        # Keep only last 7 days
        cutoff_time = timestamp - timedelta(days=7)
        self.performance_history[metric_name] = [
            entry for entry in self.performance_history[metric_name]
            if entry["timestamp"] > cutoff_time
        ]

    async def generate_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report"""

        metrics = await self.collect_performance_data()

        # Calculate overall performance score
        performance_score = self._calculate_performance_score(metrics)

        # Generate optimization recommendations
        recommendations = self._generate_optimization_recommendations(metrics)

        # Identify performance trends
        trends = self._analyze_performance_trends(metrics)

        return {
            "timestamp": datetime.now().isoformat(),
            "overall_performance_score": performance_score,
            "sla_compliance": self._check_sla_compliance(metrics),
            "performance_metrics": {
                name: {
                    "current_value": metric.current_value,
                    "target_value": metric.target_value,
                    "trend_percentage": metric.trend_percentage,
                    "target_achievement": metric.target_achievement,
                    "unit": metric.unit
                }
                for name, metric in metrics.items()
            },
            "trends": trends,
            "optimization_recommendations": recommendations,
            "next_review": (datetime.now() + timedelta(hours=24)).isoformat()
        }

    def _calculate_performance_score(self, metrics: Dict[str, PerformanceMetric]) -> float:
        """Calculate overall performance score (0-100)"""
        total_score = 0
        metric_count = 0

        for metric in metrics.values():
            # Calculate individual metric score
            if metric.target_value > 0:
                if "error_rate" in metric.name.lower():
                    # Lower is better for error rates
                    score = max(0, 100 - (metric.current_value / metric.target_value) * 100)
                elif "response_time" in metric.name.lower() or "query_time" in metric.name.lower():
                    # Lower is better for response times
                    score = max(0, 100 - ((metric.current_value - metric.target_value) / metric.target_value) * 100)
                else:
                    # Higher is better for most metrics
                    score = min(100, (metric.current_value / metric.target_value) * 100)

                total_score += score
                metric_count += 1

        return total_score / metric_count if metric_count > 0 else 0

    def _check_sla_compliance(self, metrics: Dict[str, PerformanceMetric]) -> Dict[str, bool]:
        """Check SLA compliance for all metrics"""
        compliance = {}

        for name, metric in metrics.items():
            if "error_rate" in name.lower():
                compliance[name] = metric.current_value <= metric.target_value
            elif "response_time" in name.lower() or "query_time" in name.lower():
                compliance[name] = metric.current_value <= metric.target_value
            else:
                compliance[name] = metric.current_value >= metric.target_value

        return compliance

    def _analyze_performance_trends(self, metrics: Dict[str, PerformanceMetric]) -> Dict[str, str]:
        """Analyze performance trends"""
        trends = {}

        for name, metric in metrics.items():
            if abs(metric.trend_percentage) < 1:
                trends[name] = "stable"
            elif metric.trend_percentage > 5:
                trends[name] = "improving" if "error_rate" not in name.lower() and "time" not in name.lower() else "degrading"
            elif metric.trend_percentage < -5:
                trends[name] = "degrading" if "error_rate" not in name.lower() and "time" not in name.lower() else "improving"
            else:
                trends[name] = "slight_change"

        return trends

    def _generate_optimization_recommendations(self, metrics: Dict[str, PerformanceMetric]) -> List[str]:
        """Generate optimization recommendations"""
        recommendations = []

        for name, metric in metrics.items():
            if metric.target_achievement < 80:  # Below 80% of target
                if "response_time" in name.lower():
                    recommendations.append(f"Optimize {name}: Consider implementing caching or query optimization")
                elif "cache_hit_rate" in name.lower():
                    recommendations.append(f"Improve {name}: Review cache strategy and TTL settings")
                elif "error_rate" in name.lower():
                    recommendations.append(f"Reduce {name}: Investigate and fix recurring error patterns")
                elif "uptime" in name.lower():
                    recommendations.append(f"Improve {name}: Review infrastructure reliability and monitoring")

        if not recommendations:
            recommendations.append("All metrics are performing well. Continue monitoring for optimization opportunities.")

        return recommendations


# Global instance
performance_analytics = PerformanceAnalytics()
'''

        # Write performance analytics
        analytics_file = (
            self.project_root / "backend" / "monitoring" / "performance_analytics.py"
        )
        analytics_file.write_text(analytics_code)
        self.files_created.append(str(analytics_file))

        logger.info("‚úÖ Performance analytics created")

    def _iteration_1(self):
        """Extracted iteration logic"""
                if (alert.source == source and
                    alert.status == AlertStatus.ACTIVE and
                    self._calculate_similarity(alert.title, title) > 0.8):
                    return alert
            return None


    def _iteration_2(self):
        """Extracted iteration logic"""
                if alert.status != AlertStatus.ACTIVE:
                    continue

                escalation_rule = self.escalation_rules.get(alert.severity)
                if not escalation_rule:
                    continue


    def _iteration_3(self):
        """Extracted iteration logic"""
                severity_counts[severity.value] = sum(
                    1 for alert in self.active_alerts.values()
                    if alert.severity == severity and alert.status == AlertStatus.ACTIVE
                )


    def _iteration_4(self):
        """Extracted iteration logic"""
                source_counts[alert.source] = source_counts.get(alert.source, 0) + 1

            top_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)[:5]

            return {
                "timestamp": datetime.now().isoformat(),
                "active_alerts": {
                    "total": len(self.active_alerts),
                    "by_severity": severity_counts,
                    "unacknowledged": sum(
                        1 for alert in self.active_alerts.values()
                        if alert.status == AlertStatus.ACTIVE
                    )
                },
                "alert_trends": {

    def _iteration_5(self):
        """Extracted iteration logic"""
            )

            return total_time / len(resolved_alerts)

        def _calculate_escalation_rate(self) -> float:
            """Calculate escalation rate percentage"""
            recent_alerts = [
                alert for alert in self.alert_history + list(self.active_alerts.values())
                if alert.timestamp > datetime.now() - timedelta(days=7)
            ]


    def _conditional_6(self):
        """Extracted conditional logic"""
                return 0.0

            escalated_alerts = sum(1 for alert in recent_alerts if alert.escalation_count > 0)
            return (escalated_alerts / len(recent_alerts)) * 100


    def _error_handling_7(self):
        """Extracted error_handling logic"""
                await intelligent_alerting_system.check_escalations()
                await asyncio.sleep(300)  # Check every 5 minutes
            except Exception as e:
                logger.error(f"Escalation check error: {e}")
                await asyncio.sleep(60)


    async def _create_alerting_system(self):
        """Create intelligent alerting system"""
        logger.info("üö® Creating alerting system...")

        alerting_code = '''#!/usr/bin/env python3
"""
Intelligent Alerting System for Sophia AI
Smart alerting with escalation, noise reduction, and context-aware notifications
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


class AlertStatus(Enum):
    """Alert status"""
    ACTIVE = "active"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"


@dataclass
class Alert:
    """Alert definition"""
    id: str
    title: str
    description: str
    severity: AlertSeverity
    source: str
    timestamp: datetime
    status: AlertStatus = AlertStatus.ACTIVE
    acknowledged_by: Optional[str] = None
    acknowledged_at: Optional[datetime] = None
    resolved_at: Optional[datetime] = None
    escalation_count: int = 0

    @property
    def age_minutes(self) -> int:
        return int((datetime.now() - self.timestamp).total_seconds() / 60)


class IntelligentAlertingSystem:
    """Production-grade alerting with intelligent features"""

    def __init__(self):
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        self.escalation_rules = {
            AlertSeverity.CRITICAL: {"escalate_after_minutes": 15, "max_escalations": 3},
            AlertSeverity.WARNING: {"escalate_after_minutes": 60, "max_escalations": 2},
            AlertSeverity.INFO: {"escalate_after_minutes": 240, "max_escalations": 1}
        }
        self.noise_reduction_enabled = True

    async def create_alert(self, title: str, description: str, severity: AlertSeverity,
                          source: str, context: Dict[str, Any] = None) -> Alert:
        """Create new alert with intelligent deduplication"""

        # Check for duplicate alerts (noise reduction)
        if self.noise_reduction_enabled:
            existing_alert = self._find_similar_alert(title, source)
            if existing_alert:
                logger.info(f"Suppressing duplicate alert: {title}")
                return existing_alert

        # Create new alert
        alert_id = f"{source}_{int(datetime.now().timestamp())}"
        alert = Alert(
            id=alert_id,
            title=title,
            description=description,
            severity=severity,
            source=source,
            timestamp=datetime.now()
        )

        self.active_alerts[alert_id] = alert

        # Send immediate notification for critical/emergency alerts
        if severity in [AlertSeverity.CRITICAL, AlertSeverity.EMERGENCY]:
            await self._send_immediate_notification(alert)

        logger.info(f"Created {severity.value} alert: {title}")
        return alert

    def _find_similar_alert(self, title: str, source: str) -> Optional[Alert]:
        """Find similar active alert for deduplication"""
        self._iteration_1()
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate text similarity (simplified)"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 or not words2:
            return 0.0

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        return len(intersection) / len(union)

    async def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
        """Acknowledge an alert"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.status = AlertStatus.ACKNOWLEDGED
            alert.acknowledged_by = acknowledged_by
            alert.acknowledged_at = datetime.now()

            logger.info(f"Alert acknowledged by {acknowledged_by}: {alert.title}")
            return True

        return False

    async def resolve_alert(self, alert_id: str, resolved_by: str = None) -> bool:
        """Resolve an alert"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.status = AlertStatus.RESOLVED
            alert.resolved_at = datetime.now()

            # Move to history
            self.alert_history.append(alert)
            del self.active_alerts[alert_id]

            logger.info(f"Alert resolved: {alert.title}")
            return True

        return False

    async def check_escalations(self):
        """Check for alerts that need escalation"""
        current_time = datetime.now()

        self._iteration_2()
            # Check if escalation is needed
            minutes_since_created = (current_time - alert.timestamp).total_seconds() / 60
            escalate_after = escalation_rule["escalate_after_minutes"]
            max_escalations = escalation_rule["max_escalations"]

            if (minutes_since_created >= escalate_after and
                alert.escalation_count < max_escalations):

                await self._escalate_alert(alert)

    async def _escalate_alert(self, alert: Alert):
        """Escalate an alert"""
        alert.escalation_count += 1

        logger.warning(f"Escalating alert (#{alert.escalation_count}): {alert.title}")

        # Send escalation notification
        await self._send_escalation_notification(alert)

    async def _send_immediate_notification(self, alert: Alert):
        """Send immediate notification for critical alerts"""
        logger.critical(f"üö® IMMEDIATE ALERT: {alert.title}")
        logger.critical(f"   Severity: {alert.severity.value}")
        logger.critical(f"   Source: {alert.source}")
        logger.critical(f"   Description: {alert.description}")

        # In production, this would send to Slack, email, PagerDuty, etc.

    async def _send_escalation_notification(self, alert: Alert):
        """Send escalation notification"""
        logger.error(f"‚¨ÜÔ∏è ESCALATION #{alert.escalation_count}: {alert.title}")
        logger.error(f"   Alert has been active for {alert.age_minutes} minutes")
        logger.error(f"   Severity: {alert.severity.value}")

        # In production, this would escalate to higher-level contacts

    async def generate_alert_summary(self) -> Dict[str, Any]:
        """Generate comprehensive alert summary"""

        # Count alerts by severity
        severity_counts = {}
        self._iteration_3()
        # Calculate alert trends
        recent_alerts = [
            alert for alert in self.alert_history
            if alert.timestamp > datetime.now() - timedelta(hours=24)
        ]

        # Top alert sources
        source_counts = {}
        self._iteration_4()
                "last_24h": len(recent_alerts),
                "avg_resolution_time_minutes": self._calculate_avg_resolution_time(),
                "escalation_rate": self._calculate_escalation_rate()
            },
            "top_alert_sources": top_sources,
            "system_health": {
                "alerting_system_status": "operational",
                "noise_reduction_enabled": self.noise_reduction_enabled,
                "total_escalations_today": sum(
                    alert.escalation_count for alert in self.active_alerts.values()
                )
            }
        }

    def _calculate_avg_resolution_time(self) -> float:
        """Calculate average alert resolution time"""
        resolved_alerts = [
            alert for alert in self.alert_history
            if alert.resolved_at and alert.timestamp > datetime.now() - timedelta(days=7)
        ]

        if not resolved_alerts:
            return 0.0

        total_time = sum(
            (alert.resolved_at - alert.timestamp).total_seconds() / 60
            self._iteration_5()
        self._conditional_6()

# Global instance
intelligent_alerting_system = IntelligentAlertingSystem()

# Background escalation check task
async def escalation_check_task():
    """Background task for checking escalations"""
        self._error_handling_7()
# Start escalation check task
asyncio.create_task(escalation_check_task())
'''

        # Write alerting system
        alerting_file = (
            self.project_root
            / "backend"
            / "monitoring"
            / "intelligent_alerting_system.py"
        )
        alerting_file.write_text(alerting_code)
        self.files_created.append(str(alerting_file))

        logger.info("‚úÖ Intelligent alerting system created")

    async def _create_deployment_status_monitoring(self):
        """Create deployment status monitoring"""
        logger.info("üöÄ Creating deployment status monitoring...")

        status_code = '''#!/usr/bin/env python3
"""
Deployment Status Monitoring for Sophia AI
Real-time deployment health and rollback capabilities
"""

import asyncio
import logging
from typing import Dict, Any, List
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class DeploymentStatus(Enum):
    """Deployment status enumeration"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"
    ROLLING_BACK = "rolling_back"
    ROLLED_BACK = "rolled_back"


@dataclass
class DeploymentMetrics:
    """Deployment performance metrics"""
    deployment_id: str
    start_time: datetime
    end_time: datetime = None
    success_rate: float = 0.0
    response_time_ms: float = 0.0
    error_count: int = 0
    rollback_triggered: bool = False


class DeploymentStatusMonitor:
    """Monitor deployment health and trigger rollbacks if needed"""

    def __init__(self):
        self.current_deployment: Dict[str, Any] = {}
        self.deployment_history: List[DeploymentMetrics] = []
        self.health_thresholds = {
            "min_success_rate": 95.0,
            "max_response_time_ms": 500.0,
            "max_error_rate": 5.0
        }
        self.monitoring_enabled = False

    async def start_deployment_monitoring(self, deployment_id: str, version: str):
        """Start monitoring a new deployment"""
        logger.info(f"üöÄ Starting deployment monitoring: {deployment_id}")

        self.current_deployment = {
            "id": deployment_id,
            "version": version,
            "status": DeploymentStatus.IN_PROGRESS,
            "start_time": datetime.now(),
            "metrics": DeploymentMetrics(
                deployment_id=deployment_id,
                start_time=datetime.now()
            )
        }

        self.monitoring_enabled = True

        # Start monitoring loop
        asyncio.create_task(self._monitor_deployment_health())

    async def _monitor_deployment_health(self):
        """Monitor deployment health continuously"""

        while self.monitoring_enabled and self.current_deployment:
            try:
                # Collect current metrics
                metrics = await self._collect_deployment_metrics()

                # Check health thresholds
                health_check = await self._check_deployment_health(metrics)

                if not health_check["healthy"]:
                    logger.error(f"üö® Deployment health check failed: {health_check['issues']}")

                    # Trigger rollback if critical issues
                    if health_check["critical"]:
                        await self._trigger_rollback(health_check["issues"])
                        break

                # Wait before next check
                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                logger.error(f"Deployment monitoring error: {e}")
                await asyncio.sleep(60)

    async def _collect_deployment_metrics(self) -> Dict[str, float]:
        """Collect current deployment metrics"""

        # Simulate metric collection
        # In production, this would query actual monitoring systems

        return {
            "success_rate": 98.5,  # Simulated
            "response_time_ms": 145.2,  # Simulated
            "error_rate": 0.8,  # Simulated
            "cpu_usage": 65.3,  # Simulated
            "memory_usage": 72.1,  # Simulated
            "active_connections": 1250  # Simulated
        }

    async def _check_deployment_health(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """Check if deployment meets health thresholds"""

        issues = []
        critical = False

        # Check success rate
        if metrics["success_rate"] < self.health_thresholds["min_success_rate"]:
            issues.append(f"Success rate {metrics['success_rate']}% below threshold {self.health_thresholds['min_success_rate']}%")
            critical = True

        # Check response time
        if metrics["response_time_ms"] > self.health_thresholds["max_response_time_ms"]:
            issues.append(f"Response time {metrics['response_time_ms']}ms above threshold {self.health_thresholds['max_response_time_ms']}ms")

        # Check error rate
        if metrics["error_rate"] > self.health_thresholds["max_error_rate"]:
            issues.append(f"Error rate {metrics['error_rate']}% above threshold {self.health_thresholds['max_error_rate']}%")
            if metrics["error_rate"] > 10.0:  # Critical threshold
                critical = True

        return {
            "healthy": len(issues) == 0,
            "critical": critical,
            "issues": issues,
            "metrics": metrics
        }

    async def _trigger_rollback(self, issues: List[str]):
        """Trigger deployment rollback"""
        if not self.current_deployment:
            return

        deployment_id = self.current_deployment["id"]
        logger.critical(f"üîÑ TRIGGERING ROLLBACK for deployment {deployment_id}")
        logger.critical(f"   Issues: {', '.join(issues)}")

        # Update deployment status
        self.current_deployment["status"] = DeploymentStatus.ROLLING_BACK
        self.current_deployment["metrics"].rollback_triggered = True

        try:
            # Execute rollback procedure
            await self._execute_rollback()

            self.current_deployment["status"] = DeploymentStatus.ROLLED_BACK
            logger.info(f"‚úÖ Rollback completed for deployment {deployment_id}")

        except Exception as e:
            logger.error(f"‚ùå Rollback failed for deployment {deployment_id}: {e}")
            self.current_deployment["status"] = DeploymentStatus.FAILED

        finally:
            # Stop monitoring
            await self._finish_deployment_monitoring()

    async def _execute_rollback(self):
        """Execute the actual rollback procedure"""
        logger.info("üîÑ Executing rollback procedure...")

        # Simulate rollback steps
        rollback_steps = [
            "Stopping new traffic routing",
            "Reverting to previous version",
            "Restarting services",
            "Validating rollback health",
            "Restoring traffic routing"
        ]

        for step in rollback_steps:
            logger.info(f"   ‚Ä¢ {step}...")
            await asyncio.sleep(2)  # Simulate step execution time

        logger.info("‚úÖ Rollback procedure completed")

    async def mark_deployment_success(self):
        """Mark current deployment as successful"""
        if not self.current_deployment:
            return

        deployment_id = self.current_deployment["id"]
        logger.info(f"‚úÖ Deployment successful: {deployment_id}")

        self.current_deployment["status"] = DeploymentStatus.SUCCESS
        await self._finish_deployment_monitoring()

    async def _finish_deployment_monitoring(self):
        """Finish deployment monitoring and store metrics"""
        if not self.current_deployment:
            return

        # Update final metrics
        metrics = self.current_deployment["metrics"]
        metrics.end_time = datetime.now()

        # Store in history
        self.deployment_history.append(metrics)

        # Keep only last 30 deployments
        self.deployment_history = self.deployment_history[-30:]

        # Stop monitoring
        self.monitoring_enabled = False

        logger.info(f"üìä Deployment monitoring finished: {self.current_deployment['id']}")
        self.current_deployment = {}

    def get_deployment_status(self) -> Dict[str, Any]:
        """Get current deployment status"""

        if not self.current_deployment:
            return {
                "status": "no_active_deployment",
                "message": "No deployment currently being monitored"
            }

        deployment = self.current_deployment
        runtime_minutes = (datetime.now() - deployment["start_time"]).total_seconds() / 60

        return {
            "deployment_id": deployment["id"],
            "version": deployment["version"],
            "status": deployment["status"].value,
            "runtime_minutes": runtime_minutes,
            "start_time": deployment["start_time"].isoformat(),
            "monitoring_enabled": self.monitoring_enabled,
            "health_thresholds": self.health_thresholds
        }

    def get_deployment_history(self) -> Dict[str, Any]:
        """Get deployment history and statistics"""

        if not self.deployment_history:
            return {
                "total_deployments": 0,
                "success_rate": 0,
                "average_duration_minutes": 0,
                "rollback_rate": 0
            }

        total = len(self.deployment_history)
        successful = sum(1 for d in self.deployment_history if not d.rollback_triggered)
        rollbacks = sum(1 for d in self.deployment_history if d.rollback_triggered)

        # Calculate average duration
        completed_deployments = [d for d in self.deployment_history if d.end_time]
        avg_duration = 0
        if completed_deployments:
            total_duration = sum(
                (d.end_time - d.start_time).total_seconds() / 60
                for d in completed_deployments
            )
            avg_duration = total_duration / len(completed_deployments)

        return {
            "total_deployments": total,
            "success_rate": (successful / total) * 100 if total > 0 else 0,
            "rollback_rate": (rollbacks / total) * 100 if total > 0 else 0,
            "average_duration_minutes": avg_duration,
            "last_deployment": self.deployment_history[-1].deployment_id if self.deployment_history else None,
            "recent_deployments": [
                {
                    "id": d.deployment_id,
                    "start_time": d.start_time.isoformat(),
                    "duration_minutes": (d.end_time - d.start_time).total_seconds() / 60 if d.end_time else None,
                    "rollback_triggered": d.rollback_triggered
                }
                for d in self.deployment_history[-10:]  # Last 10 deployments
            ]
        }


# Global instance
deployment_status_monitor = DeploymentStatusMonitor()
'''

        # Write deployment status monitoring
        status_file = (
            self.project_root
            / "backend"
            / "monitoring"
            / "deployment_status_monitor.py"
        )
        status_file.write_text(status_code)
        self.files_created.append(str(status_file))

        logger.info("‚úÖ Deployment status monitoring created")


async def main():
    """Main execution function"""
    refactoring = DeploymentRefactoringPhase4()
    results = await refactoring.execute_phase4()

    print("\n" + "=" * 80)
    print("üìä DEPLOYMENT REFACTORING PHASE 4 RESULTS")
    print("=" * 80)
    print(f"Phase: {results['phase']}")
    print(f"Success: {'‚úÖ YES' if results['success'] else '‚ùå NO'}")
    print(f"Tasks Completed: {len(results['tasks_completed'])}")
    print(f"Files Created: {len(results['files_created'])}")
    print(f"Errors: {len(results['errors'])}")

    if results["tasks_completed"]:
        print("\n‚úÖ Completed Tasks:")
        for task in results["tasks_completed"]:
            print(f"   ‚Ä¢ {task}")

    if results["files_created"]:
        print("\nüìÅ Files Created:")
        for file_path in results["files_created"]:
            print(f"   ‚Ä¢ {file_path}")

    if results["errors"]:
        print("\n‚ùå Errors:")
        for error in results["errors"]:
            print(f"   ‚Ä¢ {error}")

    print(f"\nStart Time: {results['start_time']}")
    print(f"End Time: {results.get('end_time', 'N/A')}")
    print("=" * 80)

    return results


if __name__ == "__main__":
    asyncio.run(main())
