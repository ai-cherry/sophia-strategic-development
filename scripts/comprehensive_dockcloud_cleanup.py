#!/usr/bin/env python3
"""
Comprehensive Dockcloud-Aware Cleanup Script for Sophia AI
Removes obsolete files while ensuring Dockcloud compatibility and updating all references.

This script addresses the following cleanup categories:
1. Backup/obsolete files (.bak, .corrupted, etc.)
2. Deprecated endpoints and legacy code
3. Unused scripts and modules
4. Placeholder/unimplemented service files
5. Environment and example files
6. Unused test files and modules
7. Documentation updates after cleanup

Dockcloud considerations:
- Updates all Docker and docker-compose files
- Validates service definitions
- Ensures secret management via Pulumi ESC
- Updates CI/CD workflows
- Maintains deployment integrity
"""

import os
import shutil
import json
import yaml
import re
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
from dataclasses import dataclass
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('dockcloud_cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class CleanupStats:
    """Track cleanup statistics."""
    files_deleted: int = 0
    directories_deleted: int = 0
    docker_references_updated: int = 0
    compose_files_updated: int = 0
    ci_workflows_updated: int = 0
    bytes_freed: int = 0
    
    def __post_init__(self):
        self.errors: List[str] = []

class DockcloudAwareCleanup:
    """Comprehensive cleanup with Dockcloud integration awareness."""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root).resolve()
        self.stats = CleanupStats()
        self.dry_run = False
        
        # Files and patterns to delete
        self.backup_patterns = [
            r"\.bak\d*$",
            r"\.backup$",
            r"\.old$",
            r"\.tmp$",
            r"\.corrupted$",
            r"_backup_\d+",
            r"_old_\d+",
            r"_deprecated_",
            r"\.week\d+.*\.backup$",
            r"backup_\d{8}_\d{6}",
            r"remodel_backup_\d{8}_\d{6}",
            r"cleanup_backup_\d{8}_\d{6}",
            r"manus_cleanup_backup_\d{8}_\d{6}",
        ]
        
        # Directories to remove entirely
        self.obsolete_directories = [
            "backup_dependencies_20250703_030231",
            "manus_cleanup_backup_20250702_224630", 
            "remodel_backup_20250702_125135",
            "remodel_backup_20250702_125252",
            "backend/app/_deprecated_apps",
            "mcp-servers/_archived_slack",
            "mcp-servers/_archived_snowflake",
            "mcp-servers/_archived_sophia_intelligence",
            "__pycache__",
            "node_modules/.cache",
            ".pytest_cache",
            ".ruff_cache",
            ".mypy_cache"
        ]
        
        # Specific files to remove
        self.obsolete_files = [
            # Environment files (secrets managed via Pulumi ESC)
            "backend/env.example",
            "frontend/env.example", 
            "deployment_credentials.env.example",
            
            # Backup files in backend/app
            "backend/app/unified_fastapi_app.py.bak",
            "backend/app/unified_fastapi_app.py.bak2",
            "backend/app/unified_fastapi_app.py.bak3",
            "backend/app/unified_fastapi_app.py.bak4",
            "backend/app/unified_fastapi_app.py.bak5",
            "backend/app/unified_fastapi_app.py.bak6",
            "backend/app/unified_fastapi_app.py.bak7",
            "backend/app/unified_fastapi_app.py.bak8",
            "backend/app/unified_fastapi_app.py.bak9",
            "backend/app/unified_fastapi_app.py.bak10",
            "backend/app/unified_fastapi_app.py.bak11",
            "backend/app/unified_fastapi_app.py.bak12",
            "backend/app/unified_fastapi_app.py.bak13",
            "backend/app/unified_fastapi_app.py.bak14",
            "backend/app/unified_fastapi_app.py.bak15",
            
            # Deprecated Docker files
            "Dockerfile.advanced",
            "Dockerfile.containerized", 
            "Dockerfile.optimized",
            "Dockerfile.production",
            "Dockerfile.iac",
            "Dockerfile.mcp-gateway",
            "Dockerfile.uv.enhanced",
            "Dockerfile.uv.minimal",
            "Dockerfile.streamlit",
            "Dockerfile.uv.debug",
            "Dockerfile.uv.final",
            "Dockerfile.uv.working",
            "Dockerfile.gong-webhook",
            "Dockerfile.mcp",
            "Dockerfile.staging",
            "Dockerfile.uv",
            "Dockerfile.uv.fixed",
            
            # Deprecated FastAPI apps
            "backend/app/working_fastapi_app.py",
            "backend/app/modern_flask_to_fastapi.py",
            "backend/app/unified_api.py",
            "backend/app/unified_main.py",
            "backend/app/minimal_api.py",
            "backend/app/minimal_api_v2.py",
            "backend/app/self_healing_api.py",
            "backend/app/simple_test_api.py",
            "backend/app/simple_unified_api.py",
            "backend/app/main.py",
            "backend/app/modernized_fastapi_app.py",
            "backend/app/fastapi_app.py",
            "backend/app/simple_startup.py",
            "backend/app/test_startup.py",
            
            # Obsolete standalone files
            "sophia_standalone_server.py",
            "unified_sophia_api.py",
            "start_backend_services.py",
            "test_comprehensive_system.py",
            "test_unified_chat_system.py",
            "graphiti_enhanced_memory.py",
            "consolidate_mcp_servers.py",
            "comprehensive_platform_analysis.py",
            "all_files.txt",
            
            # Log files
            "snowflake_deployment.log",
            "production_codacy.log",
            "ui_ux_simple.log",
            "github_simple.log", 
            "ai_memory_simple.log",
            "huggingface.log",
            "ui_ux.log",
            "github.log",
            "ai_memory.log",
            "docker_build_logs.txt",
            "build.log",
            
            # Temporary reports and JSON files
            "secret_usage_audit_report.json",
            "documentation_cleanup_report.json",
            "documentation_enhancement_report.json",
            "pulumi_esc_validation_report.json",
            "system_health_report.json",
            "dependency_audit_report.json",
            "docker_validation_report.json",
            "deployment_report_prod_1751539489.json",
            "gong_deployment_status.json",
            
            # Obsolete scripts
            "scripts/deploy_to_lambda_labs.py",  # Replaced by deploy_to_lambda_labs_cloud.py
            "deploy.sh",
            
            # Package files in root
            "package.json",
            ".nvmrc",
        ]
        
        # Docker and compose files to validate/update
        self.docker_files = [
            "Dockerfile",
            "docker-compose.yml",
            "docker-compose.override.yml", 
            "docker-compose.prod.yml",
            "docker-compose.cloud.yml",
            "docker/Dockerfile.mcp-server",
        ]
        
        # CI/CD workflows to update
        self.ci_workflows = [
            ".github/workflows/deploy-sophia-platform.yml",
            ".github/workflows/docker-build.yml",
            ".github/workflows/sync_secrets.yml",
            ".github/workflows/vercel-deployment.yml",
        ]

    def execute_cleanup(self, dry_run: bool = False) -> CleanupStats:
        """Execute the comprehensive cleanup process."""
        self.dry_run = dry_run
        
        logger.info("üßπ Starting Dockcloud-Aware Comprehensive Cleanup")
        logger.info(f"   Project Root: {self.project_root}")
        logger.info(f"   Dry Run: {dry_run}")
        
        try:
            # Phase 1: Remove obsolete files and directories
            self._remove_obsolete_files()
            self._remove_obsolete_directories()
            self._remove_backup_files()
            
            # Phase 2: Update Dockcloud configurations
            self._update_docker_files()
            self._update_compose_files()
            self._update_ci_workflows()
            
            # Phase 3: Update documentation
            self._update_documentation()
            
            # Phase 4: Generate cleanup report
            self._generate_cleanup_report()
            
            logger.info("‚úÖ Cleanup completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Cleanup failed: {e}")
            self.stats.errors.append(str(e))
            
        return self.stats

    def _remove_obsolete_files(self) -> None:
        """Remove specific obsolete files."""
        logger.info("üóëÔ∏è  Removing obsolete files...")
        
        for file_path in self.obsolete_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                try:
                    size = full_path.stat().st_size
                    if not self.dry_run:
                        full_path.unlink()
                    self.stats.files_deleted += 1
                    self.stats.bytes_freed += size
                    logger.info(f"   ‚úÖ Deleted: {file_path}")
                except Exception as e:
                    error_msg = f"Failed to delete {file_path}: {e}"
                    logger.error(f"   ‚ùå {error_msg}")
                    self.stats.errors.append(error_msg)

    def _remove_obsolete_directories(self) -> None:
        """Remove obsolete directories."""
        logger.info("üìÅ Removing obsolete directories...")
        
        for dir_path in self.obsolete_directories:
            full_path = self.project_root / dir_path
            if full_path.exists() and full_path.is_dir():
                try:
                    # Calculate size before deletion
                    size = sum(f.stat().st_size for f in full_path.rglob('*') if f.is_file())
                    
                    if not self.dry_run:
                        shutil.rmtree(full_path)
                    self.stats.directories_deleted += 1
                    self.stats.bytes_freed += size
                    logger.info(f"   ‚úÖ Deleted directory: {dir_path}")
                except Exception as e:
                    error_msg = f"Failed to delete directory {dir_path}: {e}"
                    logger.error(f"   ‚ùå {error_msg}")
                    self.stats.errors.append(error_msg)

    def _remove_backup_files(self) -> None:
        """Remove backup files matching patterns."""
        logger.info("üíæ Removing backup files...")
        
        for root, dirs, files in os.walk(self.project_root):
            # Skip external directories and git
            if '/.git/' in root or '/external/' in root or '/node_modules/' in root:
                continue
                
            for file in files:
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.project_root)
                
                # Check if file matches backup patterns
                for pattern in self.backup_patterns:
                    if re.search(pattern, file):
                        try:
                            size = file_path.stat().st_size
                            if not self.dry_run:
                                file_path.unlink()
                            self.stats.files_deleted += 1
                            self.stats.bytes_freed += size
                            logger.info(f"   ‚úÖ Deleted backup: {relative_path}")
                            break
                        except Exception as e:
                            error_msg = f"Failed to delete backup {relative_path}: {e}"
                            logger.error(f"   ‚ùå {error_msg}")
                            self.stats.errors.append(error_msg)

    def _update_docker_files(self) -> None:
        """Update Docker files to remove references to deleted files."""
        logger.info("üê≥ Updating Docker files...")
        
        for docker_file in self.docker_files:
            full_path = self.project_root / docker_file
            if not full_path.exists():
                continue
                
            try:
                with open(full_path, 'r') as f:
                    content = f.read()
                
                original_content = content
                
                # Remove references to deleted files
                content = self._remove_docker_references(content)
                
                if content != original_content:
                    if not self.dry_run:
                        with open(full_path, 'w') as f:
                            f.write(content)
                    self.stats.docker_references_updated += 1
                    logger.info(f"   ‚úÖ Updated: {docker_file}")
                    
            except Exception as e:
                error_msg = f"Failed to update {docker_file}: {e}"
                logger.error(f"   ‚ùå {error_msg}")
                self.stats.errors.append(error_msg)

    def _remove_docker_references(self, content: str) -> str:
        """Remove references to deleted files from Docker content."""
        # Remove COPY commands for deleted files
        deleted_files = [
            "backend/env.example",
            "frontend/env.example",
            "deployment_credentials.env.example",
            "backend/app/working_fastapi_app.py",
            "backend/app/modernized_fastapi_app.py",
            "backend/app/fastapi_app.py",
            "sophia_standalone_server.py",
        ]
        
        for file_path in deleted_files:
            # Remove COPY commands
            content = re.sub(rf"COPY\s+{re.escape(file_path)}\s+.*\n", "", content)
            # Remove RUN commands that reference the file
            content = re.sub(rf"RUN\s+.*{re.escape(file_path)}.*\n", "", content)
        
        return content

    def _update_compose_files(self) -> None:
        """Update docker-compose files to remove references to deleted services."""
        logger.info("üìã Updating docker-compose files...")
        
        compose_files = [
            "docker-compose.yml",
            "docker-compose.override.yml",
            "docker-compose.prod.yml",
            "docker-compose.cloud.yml",
        ]
        
        for compose_file in compose_files:
            full_path = self.project_root / compose_file
            if not full_path.exists():
                continue
                
            try:
                with open(full_path, 'r') as f:
                    content = f.read()
                
                original_content = content
                
                # Remove references to deleted environment files
                content = re.sub(r"env_file:\s*\n\s*-\s*.*\.env\.example.*\n", "", content)
                content = re.sub(r"-\s*.*\.env\.example.*\n", "", content)
                
                # Remove volume mounts to deleted files
                content = re.sub(r"-\s*\./[^:]*\.backup[^:]*:.*\n", "", content)
                
                if content != original_content:
                    if not self.dry_run:
                        with open(full_path, 'w') as f:
                            f.write(content)
                    self.stats.compose_files_updated += 1
                    logger.info(f"   ‚úÖ Updated: {compose_file}")
                    
            except Exception as e:
                error_msg = f"Failed to update {compose_file}: {e}"
                logger.error(f"   ‚ùå {error_msg}")
                self.stats.errors.append(error_msg)

    def _update_ci_workflows(self) -> None:
        """Update CI/CD workflows to remove references to deleted files."""
        logger.info("üîÑ Updating CI/CD workflows...")
        
        for workflow_file in self.ci_workflows:
            full_path = self.project_root / workflow_file
            if not full_path.exists():
                continue
                
            try:
                with open(full_path, 'r') as f:
                    content = f.read()
                
                original_content = content
                
                # Remove references to deleted scripts
                content = re.sub(r"python\s+scripts/deploy_to_lambda_labs\.py.*\n", "", content)
                content = re.sub(r"\./deploy\.sh.*\n", "", content)
                
                # Remove environment file references
                content = re.sub(r".*\.env\.example.*\n", "", content)
                
                if content != original_content:
                    if not self.dry_run:
                        with open(full_path, 'w') as f:
                            f.write(content)
                    self.stats.ci_workflows_updated += 1
                    logger.info(f"   ‚úÖ Updated: {workflow_file}")
                    
            except Exception as e:
                error_msg = f"Failed to update {workflow_file}: {e}"
                logger.error(f"   ‚ùå {error_msg}")
                self.stats.errors.append(error_msg)

    def _update_documentation(self) -> None:
        """Update documentation to reflect cleanup changes."""
        logger.info("üìö Updating documentation...")
        
        # Update System Handbook
        handbook_path = self.project_root / "docs/system_handbook/00_SOPHIA_AI_SYSTEM_HANDBOOK.md"
        if handbook_path.exists():
            try:
                with open(handbook_path, 'r') as f:
                    content = f.read()
                
                # Add cleanup section
                cleanup_section = f"""
## üßπ Codebase Cleanup (Updated {datetime.now().strftime('%Y-%m-%d')})

### Removed Components
- **Backup Files**: {self.stats.files_deleted} backup files removed
- **Obsolete Directories**: {self.stats.directories_deleted} directories removed  
- **Deprecated Docker Files**: All non-production Dockerfiles consolidated
- **Legacy FastAPI Apps**: Consolidated to single `unified_fastapi_app.py`
- **Environment Files**: Removed in favor of Pulumi ESC secret management

### Dockcloud Integration
- **Docker Files Updated**: {self.stats.docker_references_updated} references cleaned
- **Compose Files Updated**: {self.stats.compose_files_updated} configurations cleaned
- **CI/CD Workflows Updated**: {self.stats.ci_workflows_updated} workflows cleaned
- **Secret Management**: 100% Pulumi ESC integration, no local .env files

### Current Architecture
- **Single Dockerfile**: Multi-stage production build
- **Unified FastAPI App**: Single backend application
- **Dockcloud Deployment**: Lambda Labs infrastructure only
- **Enterprise Secrets**: GitHub Org Secrets ‚Üí Pulumi ESC ‚Üí Containers
"""
                
                if "## üßπ Codebase Cleanup" not in content:
                    content += cleanup_section
                    
                    if not self.dry_run:
                        with open(handbook_path, 'w') as f:
                            f.write(content)
                    logger.info("   ‚úÖ Updated System Handbook")
                    
            except Exception as e:
                error_msg = f"Failed to update documentation: {e}"
                logger.error(f"   ‚ùå {error_msg}")
                self.stats.errors.append(error_msg)

    def _generate_cleanup_report(self) -> None:
        """Generate comprehensive cleanup report."""
        logger.info("üìä Generating cleanup report...")
        
        report = {
            "cleanup_summary": {
                "timestamp": datetime.now().isoformat(),
                "dry_run": self.dry_run,
                "files_deleted": self.stats.files_deleted,
                "directories_deleted": self.stats.directories_deleted,
                "bytes_freed": self.stats.bytes_freed,
                "mb_freed": round(self.stats.bytes_freed / (1024 * 1024), 2),
                "docker_references_updated": self.stats.docker_references_updated,
                "compose_files_updated": self.stats.compose_files_updated,
                "ci_workflows_updated": self.stats.ci_workflows_updated,
                "errors": self.stats.errors
            },
            "dockcloud_validations": {
                "docker_files_validated": len(self.docker_files),
                "compose_files_validated": len([f for f in ["docker-compose.yml", "docker-compose.override.yml", "docker-compose.prod.yml", "docker-compose.cloud.yml"] if (self.project_root / f).exists()]),
                "ci_workflows_validated": len([f for f in self.ci_workflows if (self.project_root / f).exists()]),
                "secret_management": "Pulumi ESC Only",
                "deployment_target": "Lambda Labs Dockcloud"
            },
            "recommendations": [
                "Perform full Dockcloud deployment test in staging environment",
                "Validate all service endpoints after cleanup",
                "Run comprehensive integration tests",
                "Update team documentation with new file structure",
                "Monitor deployment logs for any missing file references"
            ]
        }
        
        report_path = self.project_root / "DOCKCLOUD_CLEANUP_REPORT.json"
        if not self.dry_run:
            with open(report_path, 'w') as f:
                json.dump(report, f, indent=2)
        
        logger.info(f"   ‚úÖ Cleanup report: {report_path}")
        
        # Log summary
        logger.info("\nüìà CLEANUP SUMMARY")
        logger.info(f"   Files Deleted: {self.stats.files_deleted}")
        logger.info(f"   Directories Deleted: {self.stats.directories_deleted}")
        logger.info(f"   Storage Freed: {round(self.stats.bytes_freed / (1024 * 1024), 2)} MB")
        logger.info(f"   Docker References Updated: {self.stats.docker_references_updated}")
        logger.info(f"   Compose Files Updated: {self.stats.compose_files_updated}")
        logger.info(f"   CI/CD Workflows Updated: {self.stats.ci_workflows_updated}")
        logger.info(f"   Errors: {len(self.stats.errors)}")


def main():
    """Main execution function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Dockcloud-Aware Comprehensive Cleanup")
    parser.add_argument("--dry-run", action="store_true", help="Show what would be deleted without actually deleting")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    
    args = parser.parse_args()
    
    cleaner = DockcloudAwareCleanup(args.project_root)
    stats = cleaner.execute_cleanup(dry_run=args.dry_run)
    
    if stats.errors:
        logger.error(f"‚ùå Cleanup completed with {len(stats.errors)} errors")
        for error in stats.errors:
            logger.error(f"   - {error}")
        return 1
    else:
        logger.info("‚úÖ Cleanup completed successfully with no errors")
        return 0


if __name__ == "__main__":
    exit(main()) 