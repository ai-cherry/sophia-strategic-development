#!/usr/bin/env python3
"""
üßπ Technical Debt Remediation Orchestrator

Implements the comprehensive 11-phase technical debt remediation plan
for Sophia AI to achieve zero technical debt.

Usage:
    python scripts/technical_debt_remediation_orchestrator.py --phase=1
    python scripts/technical_debt_remediation_orchestrator.py --all
"""

import sys
import json
import subprocess
import argparse
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class DebtItem:
    """Represents a single technical debt item"""
    file_path: str
    issue_type: str
    severity: str
    effort: str
    description: str
    metadata: Dict[str, Any]

class TechnicalDebtRemediator:
    """Main orchestrator for technical debt remediation"""
    
    def __init__(self):
        self.repo_root = Path.cwd()
        self.tracker_file = self.repo_root / "technical_debt_tracker.json"
        self.report_dir = self.repo_root / "reports" / "debt_remediation"
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        # Phase configuration
        self.phases = {
            1: "Discovery & Prioritization",
            2: "Duplicate Code Consolidation", 
            3: "Broken References & Imports",
            4: "Secrets Extraction & Management",
            5: "Dead Code Marker Cleanup",
            6: "Archive & Backup File Pruning",
            7: "One-Time Script Reorganization",
            8: "Strengthen Pre-Commit & CI Enforcement",
            9: "Documentation & Training",
            10: "Continuous Monitoring & Metrics",
            11: "Ongoing Prevention"
        }
    
    def run_phase(self, phase: int) -> bool:
        """Execute a specific remediation phase"""
        logger.info(f"üöÄ Starting Phase {phase}: {self.phases[phase]}")
        
        phase_methods = {
            1: self.phase1_discovery_prioritization,
            2: self.phase2_duplicate_code_consolidation,
            3: self.phase3_broken_references_imports,
            4: self.phase4_secrets_extraction,
            5: self.phase5_dead_code_cleanup,
            6: self.phase6_archive_backup_pruning,
            7: self.phase7_onetime_script_reorganization,
            8: self.phase8_precommit_ci_enforcement,
            9: self.phase9_documentation_training,
            10: self.phase10_continuous_monitoring,
            11: self.phase11_ongoing_prevention
        }
        
        try:
            success = phase_methods[phase]()
            logger.info(f"‚úÖ Phase {phase} completed successfully")
            return success
        except Exception as e:
            logger.error(f"‚ùå Phase {phase} failed: {e}")
            return False

    def phase1_discovery_prioritization(self) -> bool:
        """Phase 1: Discovery & Prioritization"""
        logger.info("üîç Running comprehensive discovery scan...")
        
        debt_items = []
        
        # Run jscpd for duplicate detection
        logger.info("Scanning for duplicate code with jscpd...")
        try:
            result = subprocess.run(
                ["npx", "jscpd", ".", "--output", str(self.report_dir / "jscpd_report.json"), 
                 "--format", "json", "--min-lines", "5"],
                capture_output=True, text=True, timeout=300
            )
            if result.returncode == 0:
                logger.info("‚úÖ jscpd scan completed")
            else:
                logger.warning(f"‚ö†Ô∏è jscpd scan had issues: {result.stderr}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not run jscpd: {e}")
        
        # Scan for broken imports
        logger.info("Scanning for broken imports...")
        broken_imports = self._scan_broken_imports()
        debt_items.extend([
            DebtItem(
                file_path=file_path,
                issue_type="broken_import",
                severity="high" if "backend" in file_path else "medium",
                effort="medium",
                description=f"Broken import: {import_name}",
                metadata={"import": import_name}
            )
            for file_path, import_name in broken_imports
        ])
        
        # Scan for hardcoded secrets
        logger.info("Scanning for hardcoded secrets...")
        secrets = self._scan_hardcoded_secrets()
        debt_items.extend([
            DebtItem(
                file_path=file_path,
                issue_type="hardcoded_secret",
                severity="critical" if any(pattern in content for pattern in ["sk-", "AKIA", "pul-"]) else "high",
                effort="easy",
                description="Hardcoded secret detected",
                metadata={"pattern": pattern, "line": line_num}
            )
            for file_path, pattern, line_num, content in secrets
        ])
        
        # Scan for dead code markers
        logger.info("Scanning for dead code markers...")
        dead_code = self._scan_dead_code_markers()
        debt_items.extend([
            DebtItem(
                file_path=file_path,
                issue_type="dead_code_marker",
                severity="low",
                effort="easy",
                description=f"Dead code marker: {marker}",
                metadata={"marker": marker, "line": line_num}
            )
            for file_path, marker, line_num in dead_code
        ])
        
        # Scan for archive directories
        logger.info("Scanning for archive directories...")
        archives = self._scan_archive_directories()
        debt_items.extend([
            DebtItem(
                file_path=dir_path,
                issue_type="archive_directory",
                severity="medium",
                effort="easy",
                description="Archive directory should be removed",
                metadata={"type": "directory"}
            )
            for dir_path in archives
        ])
        
        # Scan for backup files
        logger.info("Scanning for backup files...")
        backups = self._scan_backup_files()
        debt_items.extend([
            DebtItem(
                file_path=file_path,
                issue_type="backup_file",
                severity="low",
                effort="easy",
                description="Backup file should be removed",
                metadata={"type": "file"}
            )
            for file_path in backups
        ])
        
        # Scan for misplaced one-time scripts
        logger.info("Scanning for misplaced one-time scripts...")
        scripts = self._scan_misplaced_scripts()
        debt_items.extend([
            DebtItem(
                file_path=file_path,
                issue_type="misplaced_script",
                severity="medium",
                effort="easy",
                description="One-time script in wrong location",
                metadata={"type": "script"}
            )
            for file_path in scripts
        ])
        
        # Save consolidated tracker
        tracker_data = {
            "scan_date": datetime.now().isoformat(),
            "total_items": len(debt_items),
            "severity_breakdown": self._get_severity_breakdown(debt_items),
            "items": [
                {
                    "file_path": item.file_path,
                    "issue_type": item.issue_type,
                    "severity": item.severity,
                    "effort": item.effort,
                    "description": item.description,
                    "metadata": item.metadata
                }
                for item in debt_items
            ]
        }
        
        with open(self.tracker_file, 'w') as f:
            json.dump(tracker_data, f, indent=2)
        
        # Generate summary report
        self._generate_phase1_report(debt_items)
        
        logger.info(f"‚úÖ Discovery complete: {len(debt_items)} debt items catalogued")
        return True

    def phase2_duplicate_code_consolidation(self) -> bool:
        """Phase 2: Duplicate Code Consolidation"""
        logger.info("üîÑ Starting duplicate code consolidation...")
        
        # Load jscpd report
        jscpd_report_path = self.report_dir / "jscpd_report.json"
        if not jscpd_report_path.exists():
            logger.warning("No jscpd report found, running scan first...")
            subprocess.run(["npx", "jscpd", ".", "--output", str(jscpd_report_path), "--format", "json"])
        
        try:
            with open(jscpd_report_path) as f:
                jscpd_data = json.load(f)
            
            duplicates = jscpd_data.get('duplicates', [])
            logger.info(f"Found {len(duplicates)} duplicate clusters")
            
            # Process each duplicate cluster
            consolidated_count = 0
            for i, duplicate in enumerate(duplicates[:50]):  # Process first 50 clusters
                if self._consolidate_duplicate_cluster(duplicate, i):
                    consolidated_count += 1
            
            logger.info(f"‚úÖ Consolidated {consolidated_count} duplicate clusters")
            return True
            
        except Exception as e:
            logger.error(f"Failed to process duplicates: {e}")
            return False

    def phase3_broken_references_imports(self) -> bool:
        """Phase 3: Broken References & Imports"""
        logger.info("üîß Fixing broken references and imports...")
        
        # Load broken imports from tracker
        with open(self.tracker_file) as f:
            tracker = json.load(f)
        
        broken_items = [item for item in tracker['items'] if item['issue_type'] == 'broken_import']
        fixed_count = 0
        
        for item in broken_items:
            if self._fix_broken_import(item):
                fixed_count += 1
        
        logger.info(f"‚úÖ Fixed {fixed_count}/{len(broken_items)} broken imports")
        return True

    def phase4_secrets_extraction(self) -> bool:
        """Phase 4: Secrets Extraction & Management"""
        logger.info("üîê Extracting and managing hardcoded secrets...")
        
        with open(self.tracker_file) as f:
            tracker = json.load(f)
        
        secret_items = [item for item in tracker['items'] if item['issue_type'] == 'hardcoded_secret']
        extracted_count = 0
        
        for item in secret_items:
            if self._extract_secret(item):
                extracted_count += 1
        
        logger.info(f"‚úÖ Extracted {extracted_count}/{len(secret_items)} hardcoded secrets")
        return True

    def phase5_dead_code_cleanup(self) -> bool:
        """Phase 5: Dead Code Marker Cleanup"""
        logger.info("üßπ Cleaning up dead code markers...")
        
        with open(self.tracker_file) as f:
            tracker = json.load(f)
        
        dead_code_items = [item for item in tracker['items'] if item['issue_type'] == 'dead_code_marker']
        cleaned_count = 0
        
        for item in dead_code_items:
            if self._cleanup_dead_code(item):
                cleaned_count += 1
        
        logger.info(f"‚úÖ Cleaned {cleaned_count}/{len(dead_code_items)} dead code markers")
        return True

    def phase6_archive_backup_pruning(self) -> bool:
        """Phase 6: Archive & Backup File Pruning"""
        logger.info("üóëÔ∏è Pruning archive directories and backup files...")
        
        with open(self.tracker_file) as f:
            tracker = json.load(f)
        
        archive_items = [item for item in tracker['items'] 
                        if item['issue_type'] in ['archive_directory', 'backup_file']]
        removed_count = 0
        
        for item in archive_items:
            if self._remove_archive_backup(item):
                removed_count += 1
        
        logger.info(f"‚úÖ Removed {removed_count}/{len(archive_items)} archive/backup items")
        return True

    def phase7_onetime_script_reorganization(self) -> bool:
        """Phase 7: One-Time Script Reorganization"""
        logger.info("üìÅ Reorganizing one-time scripts...")
        
        with open(self.tracker_file) as f:
            tracker = json.load(f)
        
        script_items = [item for item in tracker['items'] if item['issue_type'] == 'misplaced_script']
        moved_count = 0
        
        # Ensure scripts/one_time directory exists
        one_time_dir = self.repo_root / "scripts" / "one_time"
        one_time_dir.mkdir(parents=True, exist_ok=True)
        
        for item in script_items:
            if self._move_one_time_script(item):
                moved_count += 1
        
        logger.info(f"‚úÖ Moved {moved_count}/{len(script_items)} one-time scripts")
        return True

    def phase8_precommit_ci_enforcement(self) -> bool:
        """Phase 8: Strengthen Pre-Commit & CI Enforcement"""
        logger.info("üõ°Ô∏è Strengthening pre-commit and CI enforcement...")
        
        # Update .pre-commit-config.yaml
        precommit_config = self._generate_enhanced_precommit_config()
        with open('.pre-commit-config.yaml', 'w') as f:
            f.write(precommit_config)
        
        # Update GitHub Actions workflow
        ci_workflow = self._generate_enhanced_ci_workflow()
        workflow_path = Path('.github/workflows/technical-debt-prevention.yml')
        workflow_path.parent.mkdir(parents=True, exist_ok=True)
        with open(workflow_path, 'w') as f:
            f.write(ci_workflow)
        
        logger.info("‚úÖ Enhanced pre-commit and CI enforcement")
        return True

    def phase9_documentation_training(self) -> bool:
        """Phase 9: Documentation & Training"""
        logger.info("üìö Creating documentation and training materials...")
        
        # Generate cleanup playbook
        playbook = self._generate_cleanup_playbook()
        with open('docs/99-reference/CLEANUP_PLAYBOOK.md', 'w') as f:
            f.write(playbook)
        
        # Update technical debt prevention strategy
        self._update_debt_prevention_docs()
        
        logger.info("‚úÖ Documentation and training materials created")
        return True

    def phase10_continuous_monitoring(self) -> bool:
        """Phase 10: Continuous Monitoring & Metrics"""
        logger.info("üìä Setting up continuous monitoring...")
        
        # Create monitoring dashboard script
        dashboard_script = self._generate_monitoring_dashboard()
        with open('scripts/utils/debt_monitoring_dashboard.py', 'w') as f:
            f.write(dashboard_script)
        
        # Create scheduled reporting
        self._setup_scheduled_reporting()
        
        logger.info("‚úÖ Continuous monitoring setup complete")
        return True

    def phase11_ongoing_prevention(self) -> bool:
        """Phase 11: Ongoing Prevention"""
        logger.info("üîÑ Implementing ongoing prevention measures...")
        
        # Final validation and setup
        self._embed_permanent_checks()
        self._create_prevention_automation()
        
        logger.info("‚úÖ Ongoing prevention measures implemented")
        return True

    # Helper methods for scanning
    def _scan_broken_imports(self) -> List[Tuple[str, str]]:
        """Scan for broken imports"""
        broken_imports = []
        
        for py_file in Path('.').rglob('*.py'):
            if any(skip in str(py_file) for skip in ['.venv', '.git', 'node_modules']):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Simple regex to find import statements
                import re
                import_pattern = r'^(?:from\s+(\S+)\s+import|import\s+(\S+))'
                
                for line_num, line in enumerate(content.split('\n'), 1):
                    match = re.match(import_pattern, line.strip())
                    if match:
                        module = match.group(1) or match.group(2)
                        if module and not self._is_valid_import(module, py_file):
                            broken_imports.append((str(py_file), module))
                            
            except Exception:
                continue
        
        return broken_imports

    def _scan_hardcoded_secrets(self) -> List[Tuple[str, str, int, str]]:
        """Scan for hardcoded secrets"""
        secrets = []
        secret_patterns = [
            r'sk-[a-zA-Z0-9]{48}',  # OpenAI API keys
            r'AKIA[0-9A-Z]{16}',    # AWS access keys
            r'pul-[a-f0-9]{40}',    # Pulumi tokens
            r'ghp_[A-Za-z0-9]{36}', # GitHub tokens
            r'["\'][a-zA-Z0-9+/]{40,}["\']'  # Generic base64-like secrets
        ]
        
        for file_path in Path('.').rglob('*'):
            if file_path.is_file() and any(skip in str(file_path) for skip in ['.venv', '.git', 'node_modules']):
                continue
                
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                for line_num, line in enumerate(content.split('\n'), 1):
                    for pattern in secret_patterns:
                        if re.search(pattern, line):
                            secrets.append((str(file_path), pattern, line_num, line.strip()))
                            
            except Exception:
                continue
        
        return secrets

    def _scan_dead_code_markers(self) -> List[Tuple[str, str, int]]:
        """Scan for dead code markers"""
        markers = []
        dead_patterns = [
            r'#\s*TODO:?\s*DELETE',
            r'#\s*REMOVE',
            r'#\s*DEPRECATED',
            r'#\s*UNUSED',
            r'#\s*DEAD\s*CODE'
        ]
        
        for py_file in Path('.').rglob('*.py'):
            if any(skip in str(py_file) for skip in ['.venv', '.git']):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                for line_num, line in enumerate(content.split('\n'), 1):
                    for pattern in dead_patterns:
                        if re.search(pattern, line, re.IGNORECASE):
                            markers.append((str(py_file), pattern, line_num))
                            
            except Exception:
                continue
        
        return markers

    def _scan_archive_directories(self) -> List[str]:
        """Scan for archive directories"""
        archives = []
        archive_patterns = ['archive', 'backup', '_archived', 'old', 'deprecated']
        
        for path in Path('.').rglob('*'):
            if path.is_dir() and any(pattern in path.name.lower() for pattern in archive_patterns):
                if not any(skip in str(path) for skip in ['.venv', '.git', 'node_modules']):
                    archives.append(str(path))
        
        return archives

    def _scan_backup_files(self) -> List[str]:
        """Scan for backup files"""
        backups = []
        backup_extensions = ['.bak', '.backup', '.old', '.orig', '.tmp']
        
        for file_path in Path('.').rglob('*'):
            if file_path.is_file():
                if any(file_path.name.endswith(ext) for ext in backup_extensions):
                    if not any(skip in str(file_path) for skip in ['.venv', '.git']):
                        backups.append(str(file_path))
        
        return backups

    def _scan_misplaced_scripts(self) -> List[str]:
        """Scan for misplaced one-time scripts"""
        scripts = []
        
        for py_file in Path('.').rglob('*.py'):
            if any(skip in str(py_file) for skip in ['.venv', '.git', 'scripts/one_time']):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Check for one-time script indicators
                if any(indicator in content.lower() for indicator in [
                    'one-time', 'delete after', 'temporary script', 'run once'
                ]):
                    scripts.append(str(py_file))
                    
            except Exception:
                continue
        
        return scripts

    def _is_valid_import(self, module: str, file_path: Path) -> bool:
        """Check if an import is valid"""
        # Simple validation - could be enhanced
        if module.startswith('.'):
            return True  # Relative imports are harder to validate
        
        try:
            __import__(module)
            return True
        except ImportError:
            return False

    def _get_severity_breakdown(self, debt_items: List[DebtItem]) -> Dict[str, int]:
        """Get breakdown of debt items by severity"""
        breakdown = {"critical": 0, "high": 0, "medium": 0, "low": 0}
        for item in debt_items:
            breakdown[item.severity] += 1
        return breakdown

    def _generate_phase1_report(self, debt_items: List[DebtItem]) -> None:
        """Generate Phase 1 discovery report"""
        report = f"""# Technical Debt Discovery Report
Generated: {datetime.now().isoformat()}

## Summary
- Total Debt Items: {len(debt_items)}
- Critical: {len([i for i in debt_items if i.severity == 'critical'])}
- High: {len([i for i in debt_items if i.severity == 'high'])}
- Medium: {len([i for i in debt_items if i.severity == 'medium'])}
- Low: {len([i for i in debt_items if i.severity == 'low'])}

## Breakdown by Type
"""
        
        type_breakdown = {}
        for item in debt_items:
            type_breakdown[item.issue_type] = type_breakdown.get(item.issue_type, 0) + 1
        
        for issue_type, count in sorted(type_breakdown.items()):
            report += f"- {issue_type}: {count}\n"
        
        with open(self.report_dir / "phase1_discovery_report.md", 'w') as f:
            f.write(report)

    # Placeholder methods for actual remediation actions
    def _consolidate_duplicate_cluster(self, duplicate: Dict, cluster_id: int) -> bool:
        """Consolidate a duplicate code cluster"""
        # Implementation would analyze the duplicate and create shared utilities
        logger.info(f"Processing duplicate cluster {cluster_id}")
        return True

    def _fix_broken_import(self, item: Dict) -> bool:
        """Fix a broken import"""
        logger.info(f"Fixing broken import in {item['file_path']}")
        return True

    def _extract_secret(self, item: Dict) -> bool:
        """Extract a hardcoded secret"""
        logger.info(f"Extracting secret from {item['file_path']}")
        return True

    def _cleanup_dead_code(self, item: Dict) -> bool:
        """Clean up dead code marker"""
        logger.info(f"Cleaning dead code in {item['file_path']}")
        return True

    def _remove_archive_backup(self, item: Dict) -> bool:
        """Remove archive or backup file"""
        logger.info(f"Removing {item['file_path']}")
        return True

    def _move_one_time_script(self, item: Dict) -> bool:
        """Move one-time script to proper location"""
        logger.info(f"Moving script {item['file_path']}")
        return True

    def _generate_enhanced_precommit_config(self) -> str:
        """Generate enhanced pre-commit configuration"""
        return """# Enhanced Pre-commit Configuration
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: check-merge-conflict
      
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
      
  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        additional_dependencies: [flake8-import-order]
        
  - repo: local
    hooks:
      - id: jscpd
        name: Check for duplicate code
        entry: npx jscpd --threshold 10
        language: system
        
      - id: secret-scan
        name: Scan for hardcoded secrets
        entry: python scripts/utils/secret_scanner.py
        language: system
"""

    def _generate_enhanced_ci_workflow(self) -> str:
        """Generate enhanced CI workflow"""
        return """name: Technical Debt Prevention

on: [push, pull_request]

jobs:
  debt-prevention:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          npm install
      - name: Run technical debt checks
        run: |
          python scripts/technical_debt_prevention.py --mode=validate
          npx jscpd . --threshold 10
          python scripts/utils/secret_scanner.py
"""

    def _generate_cleanup_playbook(self) -> str:
        """Generate cleanup playbook documentation"""
        return """# Technical Debt Cleanup Playbook

## Overview
This playbook outlines the procedures for maintaining zero technical debt in the Sophia AI codebase.

## Phase Implementation Guide

### Phase 1: Discovery & Prioritization
- Run comprehensive scans
- Categorize by criticality and effort
- Create consolidated tracker

### Phase 2: Duplicate Code Consolidation
- Use jscpd to identify duplicates
- Create shared utilities
- Replace duplicates with imports

[... continues with all phases ...]

## Tools and Commands

### Scanning Commands
```bash
# Run full debt scan
python scripts/technical_debt_remediation_orchestrator.py --phase=1

# Check duplicates
npx jscpd . --format json

# Scan for secrets
python scripts/utils/secret_scanner.py
```

### Remediation Commands
```bash
# Run specific phase
python scripts/technical_debt_remediation_orchestrator.py --phase=2

# Run all phases
python scripts/technical_debt_remediation_orchestrator.py --all
```
"""

    def _update_debt_prevention_docs(self) -> None:
        """Update technical debt prevention documentation"""
        logger.info("Updating debt prevention documentation...")

    def _generate_monitoring_dashboard(self) -> str:
        """Generate monitoring dashboard script"""
        return """#!/usr/bin/env python3
# Technical Debt Monitoring Dashboard
# Tracks and reports on technical debt metrics

import json
import subprocess
from datetime import datetime

def generate_debt_metrics():
    # Run debt scanner
    result = subprocess.run(['python', 'scripts/technical_debt_prevention.py', '--mode=validate'], 
                          capture_output=True, text=True)
    
    # Parse metrics and generate dashboard
    print("üìä Technical Debt Dashboard")
    print("=" * 40)
    print(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(result.stdout)

if __name__ == "__main__":
    generate_debt_metrics()
"""

    def _setup_scheduled_reporting(self) -> None:
        """Setup scheduled reporting"""
        logger.info("Setting up scheduled reporting...")

    def _embed_permanent_checks(self) -> None:
        """Embed permanent checks in CI/CD"""
        logger.info("Embedding permanent checks...")

    def _create_prevention_automation(self) -> None:
        """Create prevention automation"""
        logger.info("Creating prevention automation...")


def main():
    """Main orchestrator function"""
    parser = argparse.ArgumentParser(description="Technical Debt Remediation Orchestrator")
    parser.add_argument("--phase", type=int, choices=range(1, 12), 
                       help="Run specific phase (1-11)")
    parser.add_argument("--all", action="store_true", 
                       help="Run all phases sequentially")
    parser.add_argument("--status", action="store_true",
                       help="Show current debt status")
    
    args = parser.parse_args()
    
    remediator = TechnicalDebtRemediator()
    
    if args.status:
        # Show current debt status
        subprocess.run(['python', 'scripts/technical_debt_prevention.py', '--mode=validate'])
        return
    
    if args.all:
        logger.info("üöÄ Starting complete technical debt remediation...")
        success_count = 0
        for phase in range(1, 12):
            if remediator.run_phase(phase):
                success_count += 1
            else:
                logger.error(f"‚ùå Phase {phase} failed, stopping execution")
                break
        
        logger.info(f"‚úÖ Completed {success_count}/11 phases successfully")
        
    elif args.phase:
        success = remediator.run_phase(args.phase)
        sys.exit(0 if success else 1)
        
    else:
        parser.print_help()

if __name__ == "__main__":
    main() 