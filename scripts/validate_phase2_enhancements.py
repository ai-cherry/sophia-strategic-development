#!/usr/bin/env python3
"""
Phase 2 Enhancement Validation Script
Validates multi-hop orchestrator, n8n optimizer, trends injector, and personality engine
"""

import asyncio
import json
import time
import sys
from datetime import datetime
from typing import Dict, Any, List
import traceback

# Add backend to path
sys.path.append('backend')

try:
    from backend.services.enhanced_multi_hop_orchestrator import enhanced_orchestrator
    from backend.services.n8n_alpha_optimizer import n8n_optimizer
    from backend.services.x_trends_injector import x_trends_injector
    from backend.services.personality_engine import personality_engine
    print("‚úÖ All Phase 2 services imported successfully")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("üîß Note: Some imports may fail due to missing dependencies in validation environment")

class Phase2Validator:
    """Comprehensive Phase 2 enhancement validator"""
    
    def __init__(self):
        self.results = {
            "validation_start": datetime.utcnow().isoformat(),
            "tests_run": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "service_results": {},
            "performance_metrics": {},
            "recommendations": []
        }
    
    async def validate_enhanced_multi_hop_orchestrator(self) -> Dict[str, Any]:
        """Validate enhanced multi-hop orchestrator"""
        print("\nüß† Validating Enhanced Multi-Hop Orchestrator...")
        
        test_results = {
            "service_name": "enhanced_multi_hop_orchestrator",
            "tests": {},
            "overall_status": "unknown"
        }
        
        try:
            # Test 1: Service initialization
            test_results["tests"]["initialization"] = {
                "status": "pass" if enhanced_orchestrator else "fail",
                "details": "Service instance available"
            }
            
            # Test 2: Query complexity detection
            test_query = "What are our Q4 revenue trends and how do they compare to market performance?"
            try:
                context = enhanced_orchestrator.detect_response_context if hasattr(enhanced_orchestrator, 'detect_response_context') else None
                test_results["tests"]["complexity_detection"] = {
                    "status": "pass" if context else "skip",
                    "details": "Query complexity detection method available"
                }
            except Exception as e:
                test_results["tests"]["complexity_detection"] = {
                    "status": "fail",
                    "details": f"Error: {str(e)}"
                }
            
            # Test 3: Performance tracking
            stats = getattr(enhanced_orchestrator, 'stats', {})
            test_results["tests"]["performance_tracking"] = {
                "status": "pass" if stats else "fail",
                "details": f"Stats available: {list(stats.keys()) if stats else 'None'}"
            }
            
            # Test 4: Graph compilation
            graph = getattr(enhanced_orchestrator, 'compiled_graph', None)
            test_results["tests"]["graph_compilation"] = {
                "status": "pass" if graph else "fail",
                "details": "LangGraph compiled successfully" if graph else "Graph compilation failed"
            }
            
        except Exception as e:
            test_results["tests"]["exception"] = {
                "status": "fail",
                "details": f"Service validation failed: {str(e)}"
            }
        
        # Calculate overall status
        passed = sum(1 for test in test_results["tests"].values() if test["status"] == "pass")
        total = len([test for test in test_results["tests"].values() if test["status"] != "skip"])
        test_results["overall_status"] = "pass" if passed >= total * 0.8 else "fail"
        test_results["pass_rate"] = f"{passed}/{total}" if total > 0 else "0/0"
        
        print(f"   Multi-Hop Orchestrator: {test_results['overall_status'].upper()} ({test_results['pass_rate']})")
        return test_results
    
    async def validate_n8n_alpha_optimizer(self) -> Dict[str, Any]:
        """Validate n8n alpha optimizer"""
        print("\n‚ö° Validating N8N Alpha Optimizer...")
        
        test_results = {
            "service_name": "n8n_alpha_optimizer",
            "tests": {},
            "overall_status": "unknown"
        }
        
        try:
            # Test 1: Service initialization
            test_results["tests"]["initialization"] = {
                "status": "pass" if n8n_optimizer else "fail",
                "details": "Service instance available"
            }
            
            # Test 2: Performance cache
            cache = getattr(n8n_optimizer, 'performance_cache', {})
            test_results["tests"]["performance_cache"] = {
                "status": "pass",
                "details": f"Cache initialized: {type(cache).__name__}"
            }
            
            # Test 3: Alpha configuration
            configs = getattr(n8n_optimizer, 'alpha_configs', {})
            test_results["tests"]["alpha_configs"] = {
                "status": "pass",
                "details": f"Alpha configs initialized: {type(configs).__name__}"
            }
            
            # Test 4: Default alpha value
            default_alpha = getattr(n8n_optimizer, 'default_alpha', None)
            test_results["tests"]["default_alpha"] = {
                "status": "pass" if default_alpha == 0.5 else "fail",
                "details": f"Default alpha: {default_alpha}"
            }
            
            # Test 5: Optimization thresholds
            target_time = getattr(n8n_optimizer, 'target_response_time_ms', None)
            test_results["tests"]["optimization_thresholds"] = {
                "status": "pass" if target_time == 150 else "fail",
                "details": f"Target response time: {target_time}ms"
            }
            
        except Exception as e:
            test_results["tests"]["exception"] = {
                "status": "fail",
                "details": f"Service validation failed: {str(e)}"
            }
        
        # Calculate overall status
        passed = sum(1 for test in test_results["tests"].values() if test["status"] == "pass")
        total = len([test for test in test_results["tests"].values() if test["status"] != "skip"])
        test_results["overall_status"] = "pass" if passed >= total * 0.8 else "fail"
        test_results["pass_rate"] = f"{passed}/{total}" if total > 0 else "0/0"
        
        print(f"   N8N Alpha Optimizer: {test_results['overall_status'].upper()} ({test_results['pass_rate']})")
        return test_results
    
    async def validate_x_trends_injector(self) -> Dict[str, Any]:
        """Validate X trends injector"""
        print("\nüì± Validating X/Trends Injector...")
        
        test_results = {
            "service_name": "x_trends_injector",
            "tests": {},
            "overall_status": "unknown"
        }
        
        try:
            # Test 1: Service initialization
            test_results["tests"]["initialization"] = {
                "status": "pass" if x_trends_injector else "fail",
                "details": "Service instance available"
            }
            
            # Test 2: Trends cache
            cache = getattr(x_trends_injector, 'trends_cache', {})
            test_results["tests"]["trends_cache"] = {
                "status": "pass",
                "details": f"Cache initialized: {type(cache).__name__}"
            }
            
            # Test 3: Cache TTL
            ttl = getattr(x_trends_injector, 'cache_ttl_minutes', None)
            test_results["tests"]["cache_ttl"] = {
                "status": "pass" if ttl == 15 else "fail",
                "details": f"Cache TTL: {ttl} minutes"
            }
            
            # Test 4: Stats tracking
            stats = getattr(x_trends_injector, 'stats', {})
            expected_keys = {"trends_fetched", "cache_hits", "api_calls", "avg_relevance_score"}
            has_all_keys = all(key in stats for key in expected_keys)
            test_results["tests"]["stats_tracking"] = {
                "status": "pass" if has_all_keys else "fail",
                "details": f"Stats keys: {list(stats.keys()) if stats else 'None'}"
            }
            
            # Test 5: Mock trends functionality
            try:
                # This would normally be an async call, but we'll test the method exists
                mock_method = getattr(x_trends_injector, '_get_mock_trends', None)
                test_results["tests"]["mock_trends"] = {
                    "status": "pass" if mock_method else "fail",
                    "details": "Mock trends method available for fallback"
                }
            except Exception as e:
                test_results["tests"]["mock_trends"] = {
                    "status": "fail",
                    "details": f"Mock trends error: {str(e)}"
                }
            
        except Exception as e:
            test_results["tests"]["exception"] = {
                "status": "fail",
                "details": f"Service validation failed: {str(e)}"
            }
        
        # Calculate overall status
        passed = sum(1 for test in test_results["tests"].values() if test["status"] == "pass")
        total = len([test for test in test_results["tests"].values() if test["status"] != "skip"])
        test_results["overall_status"] = "pass" if passed >= total * 0.8 else "fail"
        test_results["pass_rate"] = f"{passed}/{total}" if total > 0 else "0/0"
        
        print(f"   X/Trends Injector: {test_results['overall_status'].upper()} ({test_results['pass_rate']})")
        return test_results
    
    async def validate_personality_engine(self) -> Dict[str, Any]:
        """Validate personality engine"""
        print("\nüé≠ Validating Personality Engine...")
        
        test_results = {
            "service_name": "personality_engine",
            "tests": {},
            "overall_status": "unknown"
        }
        
        try:
            # Test 1: Service initialization
            test_results["tests"]["initialization"] = {
                "status": "pass" if personality_engine else "fail",
                "details": "Service instance available"
            }
            
            # Test 2: Personality templates
            templates = getattr(personality_engine, 'personality_templates', {})
            expected_modes = 5  # neutral, professional, confident, snarky, ceo_savage
            test_results["tests"]["personality_templates"] = {
                "status": "pass" if len(templates) >= expected_modes else "fail",
                "details": f"Templates available: {len(templates)}/{expected_modes}"
            }
            
            # Test 3: Context patterns
            patterns = getattr(personality_engine, 'context_patterns', {})
            expected_contexts = 7  # financial, sales, market, team, customer, strategy, ops
            test_results["tests"]["context_patterns"] = {
                "status": "pass" if len(patterns) >= expected_contexts else "fail",
                "details": f"Context patterns: {len(patterns)}/{expected_contexts}"
            }
            
            # Test 4: User profiles cache
            profiles = getattr(personality_engine, 'user_profiles', {})
            test_results["tests"]["user_profiles"] = {
                "status": "pass",
                "details": f"User profiles cache: {type(profiles).__name__}"
            }
            
            # Test 5: Stats tracking
            stats = getattr(personality_engine, 'stats', {})
            expected_keys = {"responses_generated", "avg_sass_level", "personality_adjustments", "user_profiles_created"}
            has_all_keys = all(key in stats for key in expected_keys)
            test_results["tests"]["stats_tracking"] = {
                "status": "pass" if has_all_keys else "fail",
                "details": f"Stats keys: {list(stats.keys()) if stats else 'None'}"
            }
            
        except Exception as e:
            test_results["tests"]["exception"] = {
                "status": "fail",
                "details": f"Service validation failed: {str(e)}"
            }
        
        # Calculate overall status
        passed = sum(1 for test in test_results["tests"].values() if test["status"] == "pass")
        total = len([test for test in test_results["tests"].values() if test["status"] != "skip"])
        test_results["overall_status"] = "pass" if passed >= total * 0.8 else "fail"
        test_results["pass_rate"] = f"{passed}/{total}" if total > 0 else "0/0"
        
        print(f"   Personality Engine: {test_results['overall_status'].upper()} ({test_results['pass_rate']})")
        return test_results
    
    async def run_performance_benchmarks(self) -> Dict[str, Any]:
        """Run performance benchmarks for Phase 2 services"""
        print("\n‚ö° Running Performance Benchmarks...")
        
        benchmarks = {
            "test_start": time.time(),
            "service_benchmarks": {},
            "overall_performance": "unknown"
        }
        
        # Benchmark 1: Service instantiation time
        services = [
            ("enhanced_multi_hop_orchestrator", enhanced_orchestrator),
            ("n8n_alpha_optimizer", n8n_optimizer),
            ("x_trends_injector", x_trends_injector),
            ("personality_engine", personality_engine)
        ]
        
        for service_name, service_instance in services:
            start_time = time.time()
            
            # Test basic attribute access
            try:
                # Test if service has expected attributes
                has_stats = hasattr(service_instance, 'stats')
                has_init = hasattr(service_instance, '__init__')
                
                instantiation_time = (time.time() - start_time) * 1000
                
                benchmarks["service_benchmarks"][service_name] = {
                    "instantiation_time_ms": round(instantiation_time, 2),
                    "has_stats": has_stats,
                    "has_init": has_init,
                    "status": "pass" if instantiation_time < 100 else "slow"
                }
                
            except Exception as e:
                benchmarks["service_benchmarks"][service_name] = {
                    "instantiation_time_ms": -1,
                    "error": str(e),
                    "status": "fail"
                }
        
        total_time = (time.time() - benchmarks["test_start"]) * 1000
        benchmarks["total_benchmark_time_ms"] = round(total_time, 2)
        
        # Determine overall performance
        all_fast = all(
            result.get("instantiation_time_ms", 1000) < 50 
            for result in benchmarks["service_benchmarks"].values()
            if result.get("instantiation_time_ms", -1) > 0
        )
        benchmarks["overall_performance"] = "excellent" if all_fast else "good"
        
        print(f"   Performance: {benchmarks['overall_performance'].upper()} ({benchmarks['total_benchmark_time_ms']:.1f}ms total)")
        return benchmarks
    
    async def generate_validation_report(self) -> Dict[str, Any]:
        """Generate comprehensive validation report"""
        print("\nüìä Generating Validation Report...")
        
        # Run all validations
        multi_hop_results = await self.validate_enhanced_multi_hop_orchestrator()
        n8n_results = await self.validate_n8n_alpha_optimizer()
        trends_results = await self.validate_x_trends_injector()
        personality_results = await self.validate_personality_engine()
        performance_results = await self.run_performance_benchmarks()
        
        # Compile results
        self.results["service_results"] = {
            "enhanced_multi_hop_orchestrator": multi_hop_results,
            "n8n_alpha_optimizer": n8n_results,
            "x_trends_injector": trends_results,
            "personality_engine": personality_results
        }
        
        self.results["performance_metrics"] = performance_results
        
        # Calculate overall statistics
        all_services = [multi_hop_results, n8n_results, trends_results, personality_results]
        
        total_tests = sum(len(service["tests"]) for service in all_services)
        passed_tests = sum(
            len([test for test in service["tests"].values() if test["status"] == "pass"])
            for service in all_services
        )
        
        self.results["tests_run"] = total_tests
        self.results["tests_passed"] = passed_tests
        self.results["tests_failed"] = total_tests - passed_tests
        self.results["overall_pass_rate"] = f"{passed_tests}/{total_tests}"
        self.results["overall_percentage"] = round((passed_tests / total_tests) * 100, 1) if total_tests > 0 else 0
        
        # Generate recommendations
        self.results["recommendations"] = []
        
        if self.results["overall_percentage"] < 80:
            self.results["recommendations"].append("‚ö†Ô∏è Some services need attention before Phase 3")
        
        if performance_results["overall_performance"] != "excellent":
            self.results["recommendations"].append("üîß Consider performance optimization")
        
        if self.results["overall_percentage"] >= 90:
            self.results["recommendations"].append("‚úÖ Ready for Phase 3 implementation")
        elif self.results["overall_percentage"] >= 80:
            self.results["recommendations"].append("üü° Proceed to Phase 3 with monitoring")
        else:
            self.results["recommendations"].append("üî¥ Fix critical issues before Phase 3")
        
        self.results["validation_end"] = datetime.utcnow().isoformat()
        
        return self.results
    
    def print_summary(self):
        """Print validation summary"""
        print("\n" + "="*60)
        print("üîç PHASE 2 VALIDATION SUMMARY")
        print("="*60)
        
        print(f"üìä Overall Results: {self.results['overall_percentage']}% ({self.results['overall_pass_rate']})")
        print(f"‚ö° Performance: {self.results['performance_metrics']['overall_performance'].upper()}")
        
        print(f"\nüìã Service Status:")
        for service_name, service_result in self.results['service_results'].items():
            status_emoji = "‚úÖ" if service_result['overall_status'] == "pass" else "‚ùå"
            print(f"   {status_emoji} {service_name}: {service_result['overall_status'].upper()} ({service_result['pass_rate']})")
        
        print(f"\nüí° Recommendations:")
        for rec in self.results['recommendations']:
            print(f"   {rec}")
        
        print(f"\nüïí Validation completed at: {self.results['validation_end']}")
        print("="*60)


async def main():
    """Main validation function"""
    print("üöÄ Starting Phase 2 Enhancement Validation...")
    print("="*60)
    
    validator = Phase2Validator()
    
    try:
        # Run comprehensive validation
        results = await validator.generate_validation_report()
        
        # Print summary
        validator.print_summary()
        
        # Save results to file
        with open('PHASE_2_VALIDATION_REPORT.md', 'w') as f:
            f.write("# Phase 2 Enhancement Validation Report\n\n")
            f.write(f"**Generated:** {results['validation_end']}\n")
            f.write(f"**Overall Score:** {results['overall_percentage']}% ({results['overall_pass_rate']})\n\n")
            
            f.write("## Service Results\n\n")
            for service_name, service_result in results['service_results'].items():
                f.write(f"### {service_name}\n")
                f.write(f"- **Status:** {service_result['overall_status'].upper()}\n")
                f.write(f"- **Pass Rate:** {service_result['pass_rate']}\n")
                
                f.write(f"- **Tests:**\n")
                for test_name, test_result in service_result['tests'].items():
                    f.write(f"  - {test_name}: {test_result['status'].upper()} - {test_result['details']}\n")
                f.write("\n")
            
            f.write("## Performance Metrics\n\n")
            perf = results['performance_metrics']
            f.write(f"- **Overall Performance:** {perf['overall_performance'].upper()}\n")
            f.write(f"- **Total Benchmark Time:** {perf['total_benchmark_time_ms']:.1f}ms\n\n")
            
            f.write("## Recommendations\n\n")
            for rec in results['recommendations']:
                f.write(f"- {rec}\n")
        
        print(f"\nüìÑ Detailed report saved to: PHASE_2_VALIDATION_REPORT.md")
        
        # Return success/failure based on results
        if results['overall_percentage'] >= 80:
            print("‚úÖ VALIDATION PASSED - Ready for Phase 3")
            return 0
        else:
            print("‚ùå VALIDATION FAILED - Fix issues before Phase 3")
            return 1
            
    except Exception as e:
        print(f"‚ùå Validation failed with error: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 